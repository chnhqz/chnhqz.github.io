<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"/><meta name="theme-color" content="#222"/><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"/><meta name="renderer" content="webkit"/><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"/><link rel="alternate" href="/rss.xml" title="刀刀博客" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="刀刀博客" type="application/atom+xml"><link rel="alternate" type="application/json" title="刀刀博客" href="https://chnhqz.github.io/feed.json"/><link rel="preconnect" href="https://lf9-cdn-tos.bytecdntp.com"/><link rel="preconnect" href="https://at.alicdn.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.2"><link rel="modulepreload" href="/js/chunk-5NUNSAMU.js"></link><link rel="modulepreload" href="/js/chunk-JK7UX3QB.js"></link><link rel="modulepreload" href="/js/chunk-QAWHJ5B3.js"></link><link rel="modulepreload" href="/js/index.esm-6L4CBMFU.js"></link><link rel="modulepreload" href="/js/post-QRVNG3P5.js"></link><link rel="modulepreload" href="/js/quicklink-4NS4EDEP.js"></link><link rel="modulepreload" href="/js/siteInit.js"></link><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1gicit4jrvuj20zk0m8785.jpg" as="image" fetchpriority="high"><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1gicis3attqj20zk0m8k7l.jpg" as="image" fetchpriority="high"><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg" as="image" fetchpriority="high"><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1giciryrr3rj20zk0m8nhk.jpg" as="image" fetchpriority="high"><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1gicit31ffoj20zk0m8naf.jpg" as="image" fetchpriority="high"><link rel="preload" href="https://tva1.sinaimg.cn/large/6833939bly1gicitcxhpij20zk0m8hdt.jpg" as="image" fetchpriority="high"><meta name="description" content="欢迎来到刀刀的笔记空间(^_^)"/><link rel="canonical" href="https://chnhqz.github.io/2024/05/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><title>论文阅读</title><meta name="generator" content="Hexo 7.0.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">论文阅读</h1><div class="meta"><span class="item" title="创建时间：2024-05-06 10:19:12"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2024-05-06T10:19:12+08:00">2024-05-06</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>17k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>16 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">testName</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><ul><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1gicit4jrvuj20zk0m8785.jpg&quot;);"></li><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1gicis3attqj20zk0m8k7l.jpg&quot;);"></li><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg&quot;);"></li><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1giciryrr3rj20zk0m8nhk.jpg&quot;);"></li><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1gicit31ffoj20zk0m8naf.jpg&quot;);"></li><li class="item" style="background-image: url(&quot;https://tva1.sinaimg.cn/large/6833939bly1gicitcxhpij20zk0m8hdt.jpg&quot;);"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemListElement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://chnhqz.github.io/2024/05/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.jpg"/><meta itemprop="name" content="hqz"/><meta itemprop="description" content="=与其感慨路难行，不如马上出发=, 欢迎来到刀刀的笔记空间(^_^)"/></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="刀刀博客"/></span><div class="body md" itemprop="articleBody"><h3
id="selective-hourglass-mapping-for-universal-image-restoration-based-on-diffusion-model">19.Selective
Hourglass Mapping for Universal Image Restoration Based on Diffusion
Model</h3>
<p><strong>基于扩散模型的全局图像恢复的选择性沙漏映射</strong></p>
<p><strong>摘要：</strong>
"通用图像恢复是实际和潜在的计算机视觉任务，适用于现实世界的应用。该任务的主要挑战是同时处理不同的退化分布。现有方法主要利用任务特定条件（例如，提示）来引导模型分别学习不同的分布，称为多部分映射。然而，这对于通用模型学习并不适用，因为它忽略了不同任务之间的共享信息。在这项工作中，我们提出了一种基于扩散模型的先进选择性沙漏映射策略，称为DiffUIR。两个新颖的考虑使我们的DiffUIR非常规。首先，我们为模型配备了强大的条件指导，以获得扩散模型的准确生成方向（选择性）。更重要的是，DiffUIR将灵活的共享分布项（SDT）优雅而自然地整合到扩散算法中，逐渐将不同的分布映射到共享分布中。在反向过程中，结合SDT和强条件指导，DiffUIR迭代地将共享分布引导到具有高图像质量的任务特定分布（沙漏）。没有花哨的东西，仅通过修改映射策略，我们在五个图像恢复任务中取得了最先进的性能，在通用设置和零次通用化设置中有22个基准。令人惊讶的是，仅使用轻量级模型（仅0.89M），我们就能实现出色的性能。源代码和预训练模型可在
https://github.com/iSEE-Laboratory/DiffUIR 找到。"</p>
<p><strong>1.Introduction</strong></p>
<p>通用图像恢复旨在通过单一模型处理不同的图像恢复任务，在现实世界的机器人导航
[3] 和自动驾驶 [4]
等领域具有广泛的应用。通过单一模型处理不同的图像恢复任务的主要挑战是同时学习各种分布。现有的通用图像恢复方法
[24, 31, 33, 40, 57]
主要利用多编码器架构或提示大规模模型，如图1所示。按照
[63]，通过这种方式，它们将在一个模型中学习不同的分布映射，通过在满足特定条件时引导模型分别学习不同的分布（即多部分映射）。尽管强大的条件指导能够保持一定的图像质量，但它们忽视了不同任务可能共享信息的事实，这些信息有潜力补充和增强单一任务的性能。例如，在去雨数据集中，雨天和雾天通常同时发生。试图独立学习这两种退化类型可能无法解决这种情况。</p>
<p><img loading="lazy" data-src="截屏2024-05-06 10.48.50.png" alt="截屏2024-05-06 10.48.50" style="zoom:50%;" /></p>
<p>图1.
对比我们的DiffUIR与现有的通用图像恢复方法的示意图，现有方法主要设计任务特定模块来处理不同的分布，这迫使通用模型（橙色模块）同时学习不同的分布，称为多部分映射。相比之下，所提出的DiffUIR将不同的分布映射到一个共享分布中（即注意，这不是纯高斯分布），同时保持强大的条件指导。通过这种方式，DiffUIR使通用模型仅学习一个共享分布，并将共享分布引导到一个任务特定的分布，称为选择性沙漏映射。放大以获得最佳视图。</p>
<p>在这项工作中，我们旨在捕捉不同任务之间的共享信息，以实现更好的通用图像恢复学习。我们用基于条件扩散模型的新颖选择性沙漏映射策略替换了多部分映射策略，称为DiffUIR。两个新颖的设计使我们的DiffUIR非同寻常。首先，受RDDM
[28]的启发，我们将条件（即降级图像）明确融合到扩散模型的扩散算法中，并将条件与扩散目标广泛连接起来。通过这种方式，DiffUIR具备了类似于多部分映射方法的强大条件指导能力。其次，为了实现共享分布映射，我们优雅而自然地将一个名为SDT的共享分布项整合到扩散算法中，逐渐调整算法中条件的权重。通过建模这两个问题，在前向扩散过程中，DiffUIR逐渐减小条件的权重，各种分布将接近一个共享分布，使模型能够捕捉不同任务之间的共享信息。<strong>值得注意的是，我们将不同的分布映射到一个不纯的高斯分布中，留下了轻微的条件，正如[10,
27,
28]中所述，纯高斯噪声不包含任何任务信息，这不利于良好的生成质量。</strong></p>
<blockquote>
<p>这句话的意思是在反向去噪的过程中不直接使用纯高斯噪声？而是选择由分布映射到一个不纯的高斯分布中，留下轻微的条件，在这个不纯的高斯分布的基础上进行去噪？我这样理解有问题吗？</p>
</blockquote>
<p>在反向过程中，在强条件和SDT的引导下，DiffUIR将逐渐从共享分布中恢复到任务特定分布。通过仅修改映射策略，没有花里胡哨的，我们在五个图像恢复基准测试中大幅超越了所有现有的通用方法。值得注意的是，我们只使用了参数为36.26M的模型，比现有的基于大规模模型的通用方法少至少5倍，但性能更高。此外，为了满足实际应用的需求，我们提出了几个我们的DiffUIR的轻量级版本，其中微型版本DiffUIR-T仅包含0.89M个参数，但表现出色。为了进一步验证我们通用模型的能力，我们进行了已知任务和未知任务设置下的零次通用化实验，与其他通用方法相比，也实现了最先进的性能。总之，我们的主要贡献如下：</p>
<ol type="1">
<li>提出了一种新颖的选择性沙漏映射方法DiffUIR，它可以自由地将各种分布转换为一个共享分布，并使模型学习不同任务之间的共享信息。此外，配备强条件的DiffUIR将共享分布引导到具有高图像质量的任务特定分布中。</li>
<li>我们通过实证验证，我们的分布映射策略是通用图像恢复任务的更好解决方案。仅通过改变映射策略，我们甚至超越了基于大规模模型的通用图像恢复方法，而参数量仅为1/5。</li>
<li>我们的DiffUIR符合实际场景的需求。我们在零次通用化设置中超越了其他通用方法。我们的微型版本DiffUIR-T仅包含0.89M个参数，但性能出色。</li>
</ol>
<p><strong>2.Related Work</strong></p>
<p><strong>2.1. Image Restoration</strong></p>
<p>图像恢复旨在从其降级对应物中恢复出干净的图像，这是一个基础且重要的计算机视觉领域，涵盖了各种任务，如去雨、去雪、低光增强、去模糊和去雾等。现有的作品主要集中在通过独特的模型设计解决一个特定任务。虽然这些方法取得了很大的性能成功，但它们忽视了一个现实中的事实，即在实际应用中，人们更喜欢一个能够处理所有降级类型的模型。最近一些先驱研究了通用图像恢复模型并取得了一些进展。AirNet使用一个模块将不同的分布映射到由对比学习约束的一个共享分布中，这对训练来说是困难的，并且性能受到限制。IDR观察到不同的降级类型可以通过奇异值分解进行划分，并且可以通过重新制定奇异值和向量来重新计算干净的图像。Painter、ProRes和DA-CLIP旨在通过提示学习来整合大规模模型的全部潜力。尽管它们利用了大规模模型的先验知识，但由于使用了多部分映射策略，它们的性能仅有限，并且需要大量的参数。在这项工作中，我们提出了一种基于条件扩散模型的选择性沙漏映射策略，一次装备了模型具有共享分布映射和强条件引导的能力。由于这些能力，我们实现了出色的结果，而无需依赖复杂的训练流水线或大规模模型或预训练。</p>
<p><strong>2.2.Diffusion Model</strong></p>
<p>作为密集估计任务的一部分，许多研究人员将扩散模型应用于图像恢复。RainDiffusion
[62]将循环框架融入条件扩散模型中，在无监督设置中表现良好；DDNM
[58]构建了一个优雅的身份方程，自然地将条件添加到扩散模型的反向过程中，对线性图像恢复任务效果显著；RDDM
[28]将扩散方向从目标域改变为输入域，自然地将条件（即降级图像）集成到正向过程中，并在几个图像恢复基准测试中取得了令人印象深刻的性能。以上方法提出了一些关于条件约束的有趣修改，实现了强大的条件指导。然而，它们都不适合通用图像恢复学习。标准条件扩散模型的扩散终点是标准高斯噪声，没有任何特定任务的上下文信息，正如[10,
27,
28]中所述，它因为条件是以中介方式添加（即串联）而导致恢复质量不佳；RDDM
[28]将条件明确融合到扩散算法中，实现了高质量的图像。然而，多元映射问题出现了，因为不同任务的端点是可区分的，并且属于不同的分布。在这项工作中，我们解决了现有条件扩散模型的缺点，并同时实现了共享分布映射和强大的条件建模。</p>
<p><strong>3.DiffUIR</strong></p>
<p>在本节中，我们首先受RDDM启发，探索了扩散模型的适当条件机制，然后介绍了我们的选择性沙漏映射策略，该策略具备了共享分布映射和强大的条件引导能力，以实现更好的通用学习效果。</p>
<p><strong>3.1.Revist the condition mechanism of RDDM</strong></p>
<p>RDDM [28]遵循标准的T步扩散模型[14,
48]，其中包含前向过程和反向过程。在前向过程中，单步噪声可以被写成马尔可夫链：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2024-05-13 10.18.10.png" alt="截屏2024-05-13 10.18.10" style="zoom:50%;" /></p>
<p>其中 <span class="math inline">\(I_t\)</span> 是在时间步 <span
class="math inline">\(t\)</span> 的扩散结果，<span
class="math inline">\(I_{res}\)</span> 是退化图像 <span
class="math inline">\(I_{in}\)</span> 和 清晰图像 <span
class="math inline">\(I_0\)</span> 的残差 ：<span
class="math inline">\(I_{res} = I_{in} - I_0\)</span> 。<span
class="math inline">\(\alpha_t,\beta_t\)</span> 分别是 <span
class="math inline">\(I_{res}\)</span>
噪声系数和高斯噪声系数。他们将噪声目标从<span
class="math inline">\(I_0\)</span>（即之前图像恢复扩散方法[38,
62]中使用的）更改为了<span
class="math inline">\(I_{res}\)</span>，遵循了残差学习的原则[13]。通过马尔可夫链的性质和重新参数化技术[19,
20]，单步噪声分布可以扩展为任意步骤的噪声形式，如下所示：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/blog/source/_posts/论文阅读/image-20240513102646412.png" alt="image-20240513102646412" style="zoom:50%;" /></p>
<p>其中， <span
class="math inline">\(\bar{\alpha}_t=\sum^t_{i=1}\alpha_i,\bar{\beta}_t=\sqrt{\sum^t_{i=1}\beta^2_i}\)</span>
当 <span class="math inline">\(t\to T, \bar{\alpha}_T=1\)</span>
此时，公式可以被写为 <span
class="math inline">\(I_T=I_{in}+\bar{\beta}_T\epsilon\)</span> 。
这表明端点仅与退化图像和添加的噪声相关，自然地将条件添加到模型训练中。在反向过程中，RDDM使用
<span
class="math inline">\(q(I_{t-1}|I_t,I^{\theta}_0,I^{\theta}_{res})\)</span>
来模拟真实分布 <span
class="math inline">\(p_{\theta}(I_{t-1}|I_t)\)</span>
，并且它可以被写为马尔可夫链：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/blog/source/_posts/论文阅读/image-20240513103504239.png" alt="image-20240513103504239" style="zoom:50%;" /></p>
<p>在这里，θ 代表基于模型输出获得的项，0 代表根据 [48]
中的确定性隐式采样方程使用的结果。</p>
<p>作为 RDDM
的终点包含了条件信息（即，降级图像），它是扩散模型的一个很好的条件机制，我们称之为显式条件；此外，他们广泛地将条件与扩散目标进行连接，从而获得更好的图像质量，称为隐式条件。然而，RDDM
中的强条件机制并不适合用于通用训练，因为条件始终存在，这意味着他们强迫模型分别学习不同的降级分布，导致多部分映射，无法捕获不同任务之间的共享信息。</p>
<p><strong>3.2. Selective Hourglass Mapping</strong></p>
<p>我们方法的目标是同时实现强条件引导和共享分布映射。我们采用了RDDM的条件机制，并将共享分布项（SDT）整合到扩散算法中，实现了两个组件之间的协同效应。我们展示我们的变体扩散过程如下。分布逼近前向过程。在前向过程中，由于我们采用了RDDM的条件机制，一步扩散过程如下：
<span
class="math inline">\(I_t=I_{t-1}+\alpha_tI_{res}+\beta_t\epsilon_{t-1}\)</span>
。为了进一步实现共享分布映射，我们修改前向过程如下：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/blog/source/_posts/论文阅读/image-20240513104339479.png" alt="image-20240513104339479" style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(\delta_tI_{in}\)</span> 是
SDT，<span class="math inline">\(\delta\)</span> 是共享分布项系数，<span
class="math inline">\(\bar\delta_t = \sum^t_{i=1}\delta_i\)</span>
。我们设置 <span class="math inline">\(\bar\delta_t\)</span> 从 0 逐渐到
0.9 。这将逐渐减少条件的影响。当 <span class="math inline">\(t\to
T\)</span> 时，<span class="math inline">\(\bar\alpha_T = 1\)</span>
,公式可以重写为 <span class="math inline">\(I_T = (1-\bar\delta_T)I_{in}
+ \bar \beta_T\epsilon = 0.1I_{in}+\bar\beta_T\epsilon\)</span>
，这接近一个不纯的高斯分布（我们在实验中进一步验证）。请注意，我们采用渐进逼近策略来自然地适应扩散模型的扩散过程。</p>
<h3
id="structure-matters-tackling-the-semantic-discrepancy-in-diffusion-models-for-image-inpainting">20.Structure
Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image
Inpainting</h3>
<h3
id="condition-aware-neural-network-for-controlled-image-generation">21.Condition-Aware
Neural Network for Controlled Image Generation</h3>
<h3
id="accelerating-diffusion-sampling-with-optimized-time-steps">22.Accelerating
Diffusion Sampling with Optimized Time Steps</h3>
<h3
id="balancing-act-distribution-guided-debiasing-in-diffusion-models">23.Balancing
Act: Distribution-Guided Debiasing in Diffusion Models</h3>
<h3
id="diffir-efficient-diffusion-model-for-image-restoration">24.DiffIR:
Efficient Diffusion Model for Image Restoration</h3>
<h3 id="residual-denoising-diffusion-models">25.Residual Denoising
Diffusion Models</h3>
<p><strong>摘要：</strong>
我们提出了残差去噪扩散模型（RDDM），这是一种新颖的双重扩散过程，将传统的单一去噪扩散过程解耦为残差扩散和噪声扩散。这种双重扩散框架将最初仅用于图像恢复的基于去噪的扩散模型扩展为一个统一且可解释的模型，通过引入残差实现图像生成和恢复的统一。具体来说，我们的残差扩散表示从目标图像到退化输入图像的定向扩散，并明确引导图像恢复的反向生成过程，而噪声扩散表示扩散过程中的随机扰动。残差优先考虑确定性，而噪声强调多样性，使RDDM能够有效地统一具有不同确定性或多样性要求的任务，例如图像生成和恢复。我们通过系数转换证明了我们的采样过程与DDPM和DDIM的一致性，并提出了一个部分路径独立的生成过程，以更好地理解反向过程。值得注意的是，我们的RDDM使得仅通过L1损失和批量大小为1进行训练的通用UNet能够与最先进的图像恢复方法竞争。我们提供了代码和预训练模型，以鼓励对我们创新框架的进一步探索、应用和开发（https://github.com/nachifur/RDDM）。</p>
<p><strong>1.Introduction</strong></p>
<p>在现实生活中，扩散通常以涉及多个并发过程的复杂形式发生，例如多种气体的扩散或不同类型的波或场的传播。这让我们思考，基于去噪的扩散模型
[17, 51] 是否在仅关注去噪方面存在局限性。当前基于扩散的图像恢复方法 [22,
39, 48, 49, 82]
通过使用降级图像作为条件输入来将扩散模型扩展到图像恢复任务中，以隐式地指导反向生成过程，而不修改原始的去噪扩散过程
[17,
51]。然而，从噪声开始的反向过程似乎是不必要的，因为降级图像已经是已知的。正向过程对于图像恢复来说是不可解释的，因为扩散过程不包含任何关于降级图像的信息，如图1(a)所示。</p>
<p>在本文中，我们探索了一种新颖的双重扩散过程，并提出了残差去噪扩散模型（RDDM），可以解决单一去噪过程对图像恢复的不可解释性。在RDDM中，我们将先前的扩散过程解耦为残差扩散和噪声扩散。残差扩散优先考虑确定性，并表示从目标图像到条件输入图像的定向扩散，噪声扩散强调多样性，并表示扩散过程中的随机扰动。因此，我们的RDDM可以统一需要不同确定性或多样性的不同任务，例如图像生成和恢复。与基于去噪的扩散模型相比，RDDM中的残差清晰地指示了正向扩散方向，并明确地指导了图像恢复的反向生成过程，如图1(b)所示。</p>
<figure>
<img loading="lazy" data-src="截屏2024-05-06%2016.02.59.png"
alt="截屏2024-05-06 16.02.59" />
<figcaption aria-hidden="true">截屏2024-05-06 16.02.59</figcaption>
</figure>
<p>具体来说，我们重新定义了一种新的正向过程，允许残差和噪声同时扩散，在其中目标图像逐渐扩散成纯噪声图像用于图像生成，或者扩散成带有噪声的输入图像用于图像恢复。与之前的去噪扩散模型[17,
51]不同，其使用一个系数表来控制噪声和图像的混合比例，我们的RDDM使用两个独立的系数表来控制残差和噪声的扩散速度。我们发现，这种独立的扩散属性在反向生成过程中也是明显的，例如，在测试过程中在一定范围内调整系数表不会影响图像生成结果，并且首先移除残差，然后去噪（见图2），也可以生成语义一致的图像。我们的RDDM与广泛使用的去噪扩散模型兼容，即，通过转换系数表，我们的采样过程与DDPM[17]和DDIM[51]的采样过程一致。此外，我们的RDDM本身支持条件输入，使得仅使用ℓ1损失和批量大小为1进行训练的网络能够与最先进的图像恢复方法竞争。我们预见我们的模型可以促进统一且可解释的图像分布转换方法，突显了残差和噪声对于扩散模型同样重要的事实，例如，残差优先考虑确定性，而噪声强调多样性。本文的贡献总结如下：
-
我们提出了一种新颖的双重扩散框架，通过引入残差来解决单一去噪过程对于图像恢复的不可解释性。我们的残差扩散表示了从目标图像到条件输入图像的定向扩散。
-
我们引入了一个部分路径独立的生成过程，将残差和噪声解耦，突出了它们在控制方向性残差偏移（确定性）和随机扰动（多样性）中的作用。
-
我们设计了一个自动目标选择算法，用于选择对于未知新任务是否预测残差或噪声。
-
大量实验表明，我们的方法可以适用于不同的任务，例如图像生成、恢复、修补和翻译，聚焦于确定性或多样性，并涉及成对或非成对数据。</p>
<p><strong>2.Related Work</strong></p>
<p>去噪扩散模型（例如，DDPM [17]、SGM [52, 53] 和 DDIM
[51]）最初是为图像生成而开发的。基于DDPM和DDIM的后续图像恢复方法 [14,
39, 48] 将降级图像作为条件输入提供给一个去噪网络，例如DvSR [62]、SR3
[49] 和 WeatherDiffusion
[82]，这些方法通常需要较大的采样步长和批量大小。此外，在这些方法中，从噪声开始的反向过程似乎对于图像恢复任务是不必要且低效的。因此，SDEdit
[41]、ColdDiffusion [2]、InDI [11] 和 I2SB [29]
提出直接从降级图像或带有噪声的降级图像生成清晰图像。InDI [11] 和 I2SB
[29]
还提出了统一的图像生成和恢复框架，与我们提出的RDDM最相关。具体来说，InDI、I2SB
和我们的RDDM的正向扩散一致地采用了三个术语的混合（即，输入图像Iin、目标图像I0
和噪声 ϵ），超出了基于去噪的扩散模型 [17,
51]，该模型包括了两个术语的混合（即，I0 和 ϵ）。然而，InDI 和 I2SB
选择估计目标图像或其线性变换项来替换噪声估计，类似于我们RDDM的一个特例（SM-Res）。相比之下，我们引入了残差估计，同时为生成和恢复任务采用了噪声。我们的RDDM可以进一步扩展DDPM
[17]、DDIM [51]、InDI [11] 和 I2SB [29]
到独立的双重扩散过程，并为多维扩散过程铺平道路。我们强调残差和噪声同样重要，例如，残差优先考虑确定性，而噪声强调多样性。此外，我们的工作与系数表设计
[44, 48]、方差策略优化 [3, 4, 24, 44]、叠加图像分解 [12, 81]、曲线积分
[47]、随机微分方程 [53] 和图像恢复 [1, 32, 56, 70, 72, 75] 中的残差学习
[15] 相关。详细比较请参见附录 A.5。</p>
<p><strong>4.Residual Denoising Diffusion Models</strong></p>
<p>我们的目标是发展一种双重扩散过程去统一和融合图像生成和恢复。我们修改了传统DDPM中<span
class="math inline">\(I_T =
ϵ\)</span>的表示形式，将其改为在我们的RDDM中<span
class="math inline">\(I_T = I_{in} + ϵ\)</span>，其中<span
class="math inline">\(I_{in}\)</span>是一个降级图像（例如，阴影、低光或模糊图像）用于图像恢复，对于图像生成则设置为0。这种修改与广泛使用的去噪扩散模型兼容。例如，
<span class="math inline">\(I_T = 0 + ϵ\)</span>
是完全的噪声，对应生成，对于图像恢复， <span
class="math inline">\(I_T\)</span> 是一个带有噪声的降级图像（<span
class="math inline">\(I_{in} + ϵ\)</span>），如图3所示。</p>
<figure>
<img loading="lazy" data-src="截屏2024-05-06%2020.18.45.png"
alt="截屏2024-05-06 20.18.45" />
<figcaption aria-hidden="true">截屏2024-05-06 20.18.45</figcaption>
</figure>
<p>图3:提出的残差去噪扩散模型是一个统一图像生成和修复的框架，我们在RDDM中引入残差（<span
class="math inline">\(I_{res}\)</span>），重新定义正向扩散过程，使其涉及残差和噪声的同时扩散。残差
<span class="math inline">\(I_{res} = I_{in} - I_0\)</span>
扩散表示从目标图像 <span class="math inline">\(I_0\)</span>
到输入的退化图像 <span class="math inline">\(I_{in}\)</span>
的直接扩散。而噪声扩散表示扩散过成中的随机扰动。在RDDM中 <span
class="math inline">\(I_0\)</span> 逐渐扩散到 <span
class="math inline">\(I_T = I_{in} + ϵ  ϵ ∼ N (0, I)\)</span>
。在第三列，对于图像生成 <span class="math inline">\(I_{in} = 0\)</span>
此时 <span class="math inline">\(I_T\)</span>
是一个完全的噪声，并且对于图像恢复来说，<span
class="math inline">\(I_T\)</span> 是一个携带退化图像的噪声， <span
class="math inline">\(I_{in}\)</span> 是退化图像。</p>
<p>修正后的正向过程从 <span class="math inline">\(I_0\)</span> 到 <span
class="math inline">\(I_{in} + \epsilon\)</span> ,涉及将 <span
class="math inline">\(I_0\)</span> 逐渐降级为 <span
class="math inline">\(I_{in}\)</span> ，并注入噪声 <span
class="math inline">\(\epsilon\)</span>
。这自然产生了一个双重扩散过程，一个是残差扩散来建模从 <span
class="math inline">\(I_0\)</span> 到 <span
class="math inline">\(I_{in}\)</span>
的过渡，另一个是噪声扩散。例如，从无阴影图像 <span
class="math inline">\(I_0\)</span> 到带阴影图像的带噪声图像 <span
class="math inline">\(I_T\)</span>
的正向扩散过程涉及逐渐添加阴影和噪声。</p>
<p>在接下来的小节中，我们详细介绍了RDDM背后的基本理论和方法论。受到残差学习的启发，我们重新定义了第4.1节中每个正向扩散过程的步骤。对于反向过程，我们在第4.2节中提出了一个训练目标，用于预测正向过程中注入的残差和噪声。在第4.3节中，我们提出了三种采样方法，即残差预测（SM-Res）、噪声预测（SM-N）和“残差和噪声预测”（SM-Res-N）。</p>
<p><strong>4.1 Directional Residual Diffusion Process with
Perturbation</strong></p>
<p>为了建模图像质量的逐渐降低和噪声的增加，我们定义了RDDM中单个正向过程步骤如下：
<span class="math display">\[
I_t = I_{t-1} + I^t_{res} \quad \quad I^t_{res} \sim
\mathcal{N(\alpha_tI_{res},\beta^2_{t}I)}
\]</span> 其中 <span class="math inline">\(I^t_{res}\)</span> 代表从状态
<span class="math inline">\(I_{t-1}\)</span> 到状态 <span
class="math inline">\(I_t\)</span>
的方向均值偏移（残差扩散）与随机扰动（噪声扩散），其中 <span
class="math inline">\(I^t_{res}\)</span> 中的残差 <span
class="math inline">\(I_{res}\)</span> 是<span
class="math inline">\(I_{in}\)</span> 和 <span
class="math inline">\(I_0\)</span> 之间的差异（<span
class="math inline">\(I_{res} = I_{in} - I_{0}\)</span>）
，而两个独立的系数调度 <span class="math inline">\(\alpha_t\)</span> 和
<span class="math inline">\(\beta_t\)</span>
分别控制残差和噪声扩散。事实上，从 <span
class="math inline">\(I_0\)</span> 中采样 <span
class="math inline">\(I_t\)</span> 更简单。 <span
class="math display">\[
I_t = I_{t-1} + \alpha_tI_{res} + \beta_t\epsilon_{t-1} ,\\=I_{t-2} +
(\alpha_{t-1} + \alpha_t)I_{res} +
(\sqrt{\beta^2_{t-1}+\beta^2_t})\epsilon_{t-2} \\ =\cdots\\=I_0 +
\bar{\alpha_t}I_{res} + \bar{\beta_t}\epsilon, \tag{7}
\]</span> 其中，<span class="math inline">\(\epsilon_{t-1},\dots
,\epsilon \sim \mathcal{N(0,I)}\)</span> ， ${}<em>t=^t</em>{i=1}_i ,
{}_t = $ 如果 <span class="math inline">\(t= T, \bar{\alpha}_T = 1
I_T=I_{in}+\bar{\beta}_T\epsilon\)</span> 。<span
class="math inline">\(\bar{\beta}_T\)</span>
可以控制图像恢复中噪声扰动的强度（例如，<span
class="math inline">\(\bar{\beta}^2_T=0.01\)</span> 用于阴影消除）然而
<span class="math inline">\(\bar{\beta}^2_T=1\)</span>
用于图像生成，从方程6中，可以定义正向过程中的联合概率分布如下： <span
class="math display">\[
q(I_{1:T}|I_0,I_{res}):=\Pi^T_{t=1}q(I_t|I_{t-1},I_{res}), \tag{8} \\
q(I_t|I_{t-1},I_{res}):=\mathcal{N}(I_t;I_{t-1}+\alpha_tI_{res},\beta^2_tI).
\]</span> 方程7定义了边缘概率分布 <span
class="math inline">\(q(I_t|I_0,I_{res})=\mathcal{N}(I_t;I_0+\bar{\alpha}_tI_{res},\bar{\beta}^2_tI)\)</span>
事实上，我们的RDDM的正向扩散是三个项（即<span
class="math inline">\(I_0,I_{res},\epsilon\)</span> ）
的混合，扩展了广泛使用的去噪扩散模型，后者是两个项（即 <span
class="math inline">\(I_0,\epsilon\)</span> ）
的混合，类似的混合形式在几个并行的工作中也可以看到，例如 InDI, I2SB,
IRSDE, REsShift。</p>
<p><strong>4.2. Generation Process and Training Objective</strong></p>
<p>在正向过程中，残差 <span class="math inline">\(I_{res}\)</span>
和噪声 <span class="math inline">\(\epsilon\)</span> 逐渐添加到 <span
class="math inline">\(I_0\)</span> 中，然后合成为 <span
class="math inline">\(I_t\)</span> 而从 <span
class="math inline">\(I_T\)</span> 到 <span
class="math inline">\(I_0\)</span>
的逆过程及对在正向过程中注入的残差和噪声的估计。我们可以训练一个残差网络
<span class="math inline">\(I^{\theta}_{res}(I_t,t,I_{in})\)</span>
来预测 <span class="math inline">\(I_{res}\)</span> ，以及一个噪声网络
<span class="math inline">\(\epsilon_{\theta}(I_t,t,I_{in})\)</span>
来估计 <span class="math inline">\(\epsilon\)</span> 。使用公式 7
我们获得了估计的目标图像 <span class="math inline">\(I^{\theta}_{0} =
I_t - \bar{\alpha}_tI^{\theta}_{res} -
\bar{\beta}_t\epsilon_{\theta}\)</span> 。如果给定了 <span
class="math inline">\(I^{\theta}_{0}\)</span> 和 <span
class="math inline">\(I^{\theta}_{res}\)</span> ，则生成过程定义为：
<span class="math display">\[
p_{\theta} (I_{t-1}|I_t) := q_{\sigma} (I_{t-1}|I_t,
I^{\theta}_0,I^{\theta}_{res}) \tag{10}
\]</span> 其中，从<span class="math inline">\(I_t\)</span>到<span
class="math inline">\(I_{t-1}\)</span>的传递概率. <span
class="math inline">\(q_{\sigma}(I_{t-1}|I_t, I_0, I_{res})^2\)</span>
为：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2024-05-11 11.33.28.png" alt="截屏2024-05-11 11.33.28" style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(\sigma ^2_t =
\eta\beta^2_t\bar{\beta}^2_{t-1} / \bar{\beta}^2_t\)</span> , <span
class="math inline">\(\eta\)</span> 控制生成过程是随机的 <span
class="math inline">\((\eta = 1)\)</span> 还是确定性的 <span
class="math inline">\(\eta=0\)</span> 利用公式 10 和 公式 11
，可以通过以下方式从 <span class="math inline">\(I_t\)</span> 中采样
<span class="math inline">\(I_{t-1}\)</span> 。</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/blog/source/_posts/论文阅读/截屏2024-05-11 11.45.49.png" alt="截屏2024-05-11 11.45.49" style="zoom:50%;" /></p>
<p>其中， <span class="math inline">\(\epsilon_t \sim
\mathcal{N}(0,I)\)</span> 。当 <span class="math inline">\(\eta =
1\)</span> ，我们的 RDDM 具有受限制的总方差，而DDPM具有保持方差，当
<span class="math inline">\(\eta = 0\)</span> 采样过程是确定性的。</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/blog/source/_posts/论文阅读/截屏2024-05-11 11.56.46.png" alt="截屏2024-05-11 11.56.46" style="zoom:50%;" /></p>
<p>我们推导出以下简化的损失函数用于训练（附录 A.1）：</p>
<p><img loading="lazy" data-src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2024-05-11 11.57.45.png" alt="截屏2024-05-11 11.57.45" style="zoom:50%;" /></p>
<p>其中，超参数 <span
class="math inline">\(\lambda_{res},lambda_{\epsilon}\in
\{0,1\}\)</span> ，以及训练输入图像 <span
class="math inline">\(I_t\)</span> 是通过 <span
class="math inline">\(I_0, I_{res}, \epsilon\)</span>
综合得到的。也可以使用 <span class="math inline">\(I_{in}\)</span>
综合生成 <span class="math inline">\(I_t\)</span> <span
class="math display">\[
I_t = I_{in} + (\bar{\alpha}_t  - 1 )I_{res} + \bar{\beta}_t\epsilon
\]</span> <strong>5.Decoupled Dual Diffusion
Framework（解耦双重扩散框架）</strong></p>
<p>在对RDDM和DDPM进行对比分析后，我们发现DDPM确实涉及到了残差和噪音的同时扩散，这在附录A.3中的方程48等价于方程44时显而易见。我们发现可以将这两种扩散方式解耦。第5.1节介绍了解耦的前向扩散过程。在第5.2节中，我们提出了一种部分路径独立的生成过程，并将同时采样解耦为先移除残差，然后移除噪音（参见图6(d)和图17）。这种解耦的双重扩散框架揭示了DDPM生成过程中残差去除和去噪的作用。</p>
<p>该模型提出了一种残差去噪模型
（RDDM）可用于图像生成和图像修复。该文最大的特点是提出一种双扩散模型，在扩散过程中不仅包含噪声
<span class="math inline">\(\epsilon\)</span> 扩散，还包含残差信息 <span
class="math inline">\(I_{res}\)</span>
的扩散，这里的残差信息就是退化图像 <span
class="math inline">\(I_{in}\)</span> 和 <span
class="math inline">\(I_0\)</span> 之间的差值 （<span
class="math inline">\(I_{res} = I_{in} - I_0\)</span>）
。<strong>接下来我只讨论在引入条件的过程中的图像修复内容。</strong></p>
<p>首先，在传统的扩散模型中，我们的正向扩散过程是： <span
class="math display">\[
I_t = \sqrt{\bar{\alpha}_t} I_0+\sqrt{1-\bar{\alpha}_t} \epsilon \tag{1}
\]</span> 本文中的正向扩散过程： <span class="math display">\[
I_t = I_0 + \bar{\alpha}_tI_{res} + \bar{\beta}_t\epsilon   \tag{2}
\]</span> 其中，<span class="math inline">\(1-\bar{\alpha}_t =
\bar{\beta}_t\)</span>
。这样我们可以看到两个正向扩散的过程十分相近。那么作者这点在代码中的体现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机符合正态分布的噪声</span></span><br><span class="line">noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line"><span class="comment"># 残差 为 退化图像和清晰图像的差</span></span><br><span class="line">x_res = x_input - x_start</span><br><span class="line"></span><br><span class="line">x_t = self.q_sample(x_start, x_res, t, noise=noise)</span><br></pre></td></tr></table></figure>
<p><code>q_sample()</code>
是封装好的正向扩散过程，这也是与原始<code>DDPM</code>
不同的点。我们可以列出两者代码对比一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, continuous_sqrt_alpha_cumprod, noise=<span class="literal">None</span></span>):</span><br><span class="line">		noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># random gama</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        continuous_sqrt_alpha_cumprod * x_start +</span><br><span class="line">        (<span class="number">1</span> - continuous_sqrt_alpha_cumprod**<span class="number">2</span>).sqrt() * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, x_res, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x_start+extract(self.alphas_cumsum, t, x_start.shape) * x_res +</span><br><span class="line">        extract(self.betas_cumsum, t, x_start.shape) * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>可以明显看出两者之间的区别，当然不仅仅是空格缩进的区别（哈哈）</p>
<p>其次，<code>RDDM</code> 在扩散过程中需要同时训练两个模型 <span
class="math inline">\(I^{\theta}_{res}(I_t,t,I_{in})\)</span> 和 <span
class="math inline">\(\epsilon_{\theta}(I_t,t,I_{in})\)</span>
来分别预测每一步中应该存在的残差 <span
class="math inline">\(I^{\theta}_{res}\)</span> 和每一步中添加的噪声
<span class="math inline">\(\epsilon_{\theta}\)</span>
。其中，模型训练的目标函数分别为： <span class="math display">\[
L_{res}(\theta):=\mathbb{E}[\lambda_{res}\|I_{res} -
I^{\theta}_{res}(I_t,t,I_{in}) \|^2]\\
L_{\epsilon}(\theta):=\mathbb{E}[\lambda_{\epsilon}\| \epsilon -
\epsilon_{\theta}(I_t,t,I_{in}) \|^2] \tag{3}
\]</span> 这一部分在代码中体现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_out = self.model(x_in,</span><br><span class="line">                       [self.alphas_cumsum[t]*self.num_timesteps,</span><br><span class="line">                       self.betas_cumsum[t]*self.num_timesteps],</span><br><span class="line">                       x_self_cond)</span><br><span class="line"><span class="comment"># 这里 x_in 是 torch.cat((x_t, x_input, x_input_condition), dim=1) </span></span><br><span class="line"><span class="comment"># 后续是时间步</span></span><br><span class="line"><span class="comment"># x_self_cond 是与自注意力机制有关，目前不太清楚</span></span><br><span class="line"><span class="comment"># 这里的 model_out 是两个张量</span></span><br><span class="line"><span class="comment"># pred_res = model_out[0]</span></span><br><span class="line"><span class="comment"># pred_noise = model_out[1]</span></span><br><span class="line"><span class="comment"># 这里作者是通过创建两个 unet 模型来实现</span></span><br></pre></td></tr></table></figure>
<p>紧接着是通过模型训练的目标函数进行损失计算，达到训练模型的目的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_u = self.q_posterior_from_res_noise(pred_res, pred_noise, x, t)</span><br><span class="line">u_gt = self.q_posterior_from_res_noise(x_res, noise, x, t)</span><br><span class="line">loss = <span class="number">10000</span>*self.loss_fn(x_u, u_gt, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里主要是拆解一下 <code>self.q_posterior_from_res_noise()</code>
这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_posterior_from_res_noise</span>(<span class="params">self, x_res, noise, x_t, t</span>):</span><br><span class="line">    <span class="keyword">return</span> (x_t-extract(self.alphas, t, x_t.shape) * x_res -</span><br><span class="line">           (extract(self.betas2, t, x_t.shape)/extract(self.betas_cumsum, t, x_t.shape)) * noise)</span><br></pre></td></tr></table></figure>
<p>这里先引入一下原始的 <code>DDPM</code> 是如何计算损失的。 <span
class="math display">\[
L(\theta):=\mathbb{E}_{I_0\sim q(I_0),\epsilon\sim
\mathcal{N}(0,I)}[\|\epsilon - \epsilon_{\theta}(I_t,t)  \|^2] \tag{4}
\]</span> 可以看出，原始<code>DDPM</code>
的损失计算十分简单，只需要计算模型预测出来的噪声和原始的噪声的差值即可。但是在
<code>RDDM</code>
中，引入了一个新的需要计算的损失，即残差之间的损失。<code>DDPM</code>
的损失计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_recon = self.denoise_fn(torch.cat([x_in[<span class="string">&#x27;Input&#x27;</span>], x_noisy], dim=<span class="number">1</span>), continuous_sqrt_alpha_cumprod)</span><br><span class="line">loss = self.loss_func(noise, x_recon)</span><br></pre></td></tr></table></figure>
<p>我们从代码比较中可以看出，<code>RDDM</code>
并没有采用将两者损失分别计算然后相加的方法进行计算，而是通过函数
<code>q_posterior_from_res_noise</code>
计算出一个值，之后计算两个值之间的差值来进行损失计算和模型训练。通过代码分析，<code>q_posterior_from_res_noise</code>
计算的是 <code>x_t - x_res - noise</code>
，即应该是当前时间步下添加了残差和噪声的图像 <code>x_t</code>
去除模型预测的残差和噪声的值。（⚠️这里其实我也不太确定，没有找的依据）</p>
<p>至此，正向扩散过程和模型训练过程结束。</p>
<p>后续，我们就可以利用训练好的网络预测得到的 <span
class="math inline">\(I^{\theta}_{res}\)</span> 和噪声 <span
class="math inline">\(\epsilon_{\theta}\)</span> 从 <span
class="math inline">\(I_t\)</span> 中 恢复 <span
class="math inline">\(I_{t-1}\)</span> 。 <span class="math display">\[
I_{t-1} = I_{t} - (\bar{\alpha}_t - \bar{\alpha}_{t-1})I^{\theta}_{res}
-
(\bar{\beta}_{t}-\sqrt{\bar{\beta}^2_{t-1}-\sigma^2_t})\epsilon_{\theta}
+ \sigma_t\epsilon_t,where\quad \epsilon_t\sim \mathcal{N}(0,I) \\
\sigma^2_t = \eta \beta^2_t \bar{\beta}^2_{t-1} / \bar{\beta}^2_{t}
\tag{5}
\]</span></p>
<blockquote>
<p>​ <code>RDDM</code> 其实是在正向扩散过程中加入了 <span
class="math inline">\(I_{in} - I_0 = I_{res}\)</span>
残差，意味着，原本正向扩散过程中只有 <span
class="math inline">\(I_0\)</span> 变成了 有 <span
class="math inline">\(I_0,I_{res}\)</span> 。也就是说网络从只学习 <span
class="math inline">\(I_0\)</span> 分布变成了现在学习 <span
class="math inline">\(I_0,I_res\)</span>
分布。在这之后分别训练两个模型，一个专注于预测 <span
class="math inline">\(I_{res}\)</span> 另一个专注于预测 <span
class="math inline">\(\epsilon\)</span>
。作者指出，残差扩散代表了从退化图像 <span
class="math inline">\(I_{in}\)</span> 到 <span
class="math inline">\(I_0\)</span>
的过程。代表了确定性。噪声扩散代表了从杂环无序的噪声 <span
class="math inline">\(\epsilon\)</span> 到 <span
class="math inline">\(I_0\)</span> 的过程，代表了多样性。两者统一。</p>
</blockquote>
</div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2024-05-13 14:15:49" itemprop="dateModified" datetime="2024-05-13T14:15:49+08:00">2024-05-13</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="hqz 微信支付"/><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="hqz 支付宝"/><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>hqz<i class="ic i-at"><em>@</em></i>刀刀博客</li><li class="link"><strong>本文链接：</strong><a href="https://chnhqz.github.io/2024/05/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="论文阅读">https://chnhqz.github.io/2024/05/06/论文阅读/</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/05/05/%E9%9F%B3%E8%A7%86%E9%A2%91/" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giciryrr3rj20zk0m8nhk.jpg" title="音视频"><span class="type">上一篇</span><h3>音视频</h3></a></div><div class="item right"><a href="/2024/05/06/%E5%8A%9B%E6%89%A3/" rel="next" itemprop="url" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicit31ffoj20zk0m8naf.jpg" title="力扣"><span class="type">下一篇</span><h3>力扣</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#selective-hourglass-mapping-for-universal-image-restoration-based-on-diffusion-model"><span class="toc-number">1.</span> <span class="toc-text">19.Selective
Hourglass Mapping for Universal Image Restoration Based on Diffusion
Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#structure-matters-tackling-the-semantic-discrepancy-in-diffusion-models-for-image-inpainting"><span class="toc-number">2.</span> <span class="toc-text">20.Structure
Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image
Inpainting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#condition-aware-neural-network-for-controlled-image-generation"><span class="toc-number">3.</span> <span class="toc-text">21.Condition-Aware
Neural Network for Controlled Image Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accelerating-diffusion-sampling-with-optimized-time-steps"><span class="toc-number">4.</span> <span class="toc-text">22.Accelerating
Diffusion Sampling with Optimized Time Steps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#balancing-act-distribution-guided-debiasing-in-diffusion-models"><span class="toc-number">5.</span> <span class="toc-text">23.Balancing
Act: Distribution-Guided Debiasing in Diffusion Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#diffir-efficient-diffusion-model-for-image-restoration"><span class="toc-number">6.</span> <span class="toc-text">24.DiffIR:
Efficient Diffusion Model for Image Restoration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#residual-denoising-diffusion-models"><span class="toc-number">7.</span> <span class="toc-text">25.Residual Denoising
Diffusion Models</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="hqz" src="/assets/avatar.jpg"/><p class="name" itemprop="name">hqz</p><div class="description" itemprop="description">欢迎来到刀刀的笔记空间(^_^)</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">9</span><span class="name">文章</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/name" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;name"><i class="ic i-github"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/05/06/%E5%8A%9B%E6%89%A3/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/05/05/%E9%9F%B3%E8%A7%86%E9%A2%91/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/05/linux/linux-md/">linux.md</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/06/%E5%8A%9B%E6%89%A3/">力扣</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/07/test/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/">我的第一篇博客文章</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/05/22/papper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">未命名</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/05/mysql/">mysql</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/07/test/hello-world/">Hello World</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/05/%E9%9F%B3%E8%A7%86%E9%A2%91/">音视频</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/08/papper/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-1/">论文阅读</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2024</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">hqz @ testName</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">121k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">1:50</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `2024/05/06/论文阅读/`,
        favicon: {
        show: `（●´3｀●）やれやれだぜ`,
        hide: `(´Д｀)大変だ！`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-6-M/pace/1.2.4/pace.min.js" async></script><script src="/js/siteInit.js?v=0.4.2" type="module" fetchpriority="high" defer></script></body></html>