### 如何读文献

#### 每一篇文献自带了一份通往同一领域相关论文的地图，它叫做“参考文献”

- 背景介绍部分的参考文献，告诉你这个领域为什么重要，这个研究问题为什么有意义。
- 对于研究问题及相关研究部分的参考文献，告诉你之前都有谁做了什么相似/相关的工作
- 实验设计、实验方法部分的参考文献，告诉你这种设计/方法最早来自何处
- 数据讨论部分的参考文献，告诉你什么样已知的理论研究支持了这些假设和结论
- 总结展望部分的参考文献，告诉你最新的综述有哪些、待解決的问题有哪些、最最近的研究有哪些
- 实验部分的参考文献，告诉你这些实验操作最原始的出处在哪里 

#### 读完文献，至少应该能够回答的四个问题

- 这篇文章，到底在解决什么问题？（摘要、前言部分）
- 这个问题为什么在这个领域重要？（前言部分）
- 这些作者是怎么解决这个问题的？（具体的实验设计、优化部分）
- 这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）

注意把握核心文献，那些被多次提及以及多次引用的文献。

还有一点是几乎很多人的读文献视频都没讲过的，但是似乎很多人都知道的事情：

对于重要的研究性文献，你应该也看看它们的非正文部分，包括注释、脚注、以及supporting

information（就是包含实验部分的附属文件），因为这些部分往往会包含一些作者不太想告诉你但是又不得不告诉你的关于这篇文章的事实，比如说这篇文章的局限性、缺点、尚且解决不了的一部分问题等等。

- ﻿除了读新文献之外，还有一种文献非常值得读，那就是那些可能与你的研究没有非常直接的关联，但叉有一定关联，一定不算做你研究领域“核心文献〞，而且通常发在那些你看不起的“低影响因子小期
   刊〞，通常都是文字很多图很少，你甚至没有听过作者名字的，老文献
- ﻿“现在的科研工作者，很多都是挖尸体的人〞 大约等于
   “你不站在巨人的肩膀上，如何能够得着夜空中的明月”。
- ﻿提供一个有用的思路：你可以通过一篇文章中引用的一篇老文献找到通讯作者，然后看看这个“你没听过名字的〞教授一辈子学术生涯都做了什么，你往往会有意想不到的收获

### 文献列表

[1] 吴英杰．隐私保护数据发布：模型与算法[M]．北京：清华大学出版社，2016

[2] [周志华](https://www.zhihu.com/search?q=周志华&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2382359427})．机器学习 [ M]．北京：清华大学出版社 2016

[3] 刘俊旭 , [孟小峰](https://www.zhihu.com/search?q=孟小峰&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2382359427}) . 机器学习的隐私保护研究综述 [J]. [计算机研究与发展](https://www.zhihu.com/search?q=计算机研究与发展&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2382359427}) , 2020, 57( 2): 346

[4] 熊平 , [朱天清](https://www.zhihu.com/search?q=朱天清&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2382359427}) , 王晓峰 . 差分隐私保护及其应用 [J]. [计算机学报](https://www.zhihu.com/search?q=计算机学报&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2382359427}) ,2014, 37(1): 101 122

#### DP theory

* Gaussian differential privacy [[paper]](https://arxiv.org/pdf/1905.02383) by Jinshuo Dong,  Aaron Roth, Weijie J. Su.  2019
* Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation[[paper]](https://arxiv.org/pdf/1906.02830.pdf) by Mark Bun and Thomas Steinke. 2019
* New Differentially Private Algorithms for Learning Mixtures of Well-Separated Gaussians[[paper]](https://arxiv.org/pdf/1909.03951.pdf) by Gautam Kamath, Or Sheffet, Vikrant Singhal, Jonathan Ullman. 2019 
* Private Hypothesis Selection [[paper]](https://arxiv.org/pdf/1905.13229.pdf) by Mark Bun, Gautam Kamath, Thomas Steinke,Steven Wu. 2019
* Privacy Amplification by Iteration [[paper]](https://arxiv.org/abs/1808.06651) by Vitaly Feldman, Ilya Mironov, Kunal Talwar, Abhradeep Thakurta. 2018
* pMSE Mechanism: Differentially Private Synthetic Data with Maximal Distributional Similarity [[paper]](https://arxiv.org/pdf/1805.09392.pdf) by Joshua Snoke and Aleksandra Slavkovic. 2018
* Differentially Private Continual Learning [[paper]](https://arxiv.org/pdf/1902.06497.pdf) by S.Farquhar and Yarin Gal. 2018
* Individual Fairness Under Composition [[paper]](http://www.fatml.org/media/documents/individual_fairness_under_composition.pdf) by Cynthia Dwork and Christina Ilvento. 2018  
* Differentially Private Fair Learning [[paper]](https://arxiv.org/abs/1812.02696) by Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, Jonathan Ullman. 2018  
* Differentially Private False Discovery Rate Control [[paper]](https://arxiv.org/abs/1807.04209) by Cynthia Dwork, Weijie J. Su, Li Zhang. 2018  
* Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM [[paper]](https://arxiv.org/abs/1705.10829) [[code]](https://github.com/steven7woo/Accuracy-First-Differential-Privacy) by Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, Z. Steven Wu. 2017  
* Penalizing Unfairness in Binary Classification [[paper]](https://arxiv.org/abs/1707.00044) by Yahav Bechavod, Katrina Ligett. 2017  
* Concentrated Differential Privacy [[paper]](https://arxiv.org/abs/1603.01887) by Cynthia Dwork, Guy N. Rothblum. 2016  
* Protecting Privacy when Disclosing Information: k-Anonymity and Its Enforcement through Generalization and Suppression [[paper]](https://epic.org/privacy/reidentification/Samarati_Sweeney_paper.pdf) by Pierangela Samarati and Latanya Sweeney  

#### Local Differential Privacy  

* Answering multi-dimensional analytical queries under local differential privacy [[paper]](https://par.nsf.gov/servlets/purl/10194803) by  Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang, Ninghui Li, Somesh Jha. 2019
* Locally Private Gaussian Estimation [[paper]](https://arxiv.org/abs/1811.08382) by Matthew Joseph, Janardhan Kulkarni, Jieming Mao, Zhiwei Steven Wu. 2019  
* Local Differential Privacy for Evolving Data [[paper]](https://arxiv.org/abs/1802.07128) by Matthew Joseph, Aaron Roth, Jonathan Ullman, Bo Waggoner. 2018  
* Privacy at Scale: Local Differential Privacy in Practice [[paper]](http://dimacs.rutgers.edu/~graham/pubs/papers/ldptutorial.pdf) by Graham Cormode, Somesh Jha, Tejas kulkarni, Ninghui Li, Divesh Srivastava, Tianhao Wang. 2018  
* Locally Private Gaussian Estimation [[paper]](https://arxiv.org/abs/1811.08382) by Matthew Joseph, Janardhan Kulkarni, Jieming Mao, Zhiwei Steven Wu. 2018  
* Locally differentially private protocols for frequency estimation [[paper]](https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-wang-tianhao.pdf) by  Tianhao Wang, Jeremiah Blocki, Ninghui Li, Somesh Jha. 2017

#### Image Privacy

* Learning to Anonymize Faces for Privacy Preserving Action Detection [[paper]](https://web.cs.ucdavis.edu/~yjlee/projects/eccv2018-privacy.pdf) by Zhongzheng Ren, Yong Jae Lee and Michael S.Ryoo. 2019
* Image Privacy Prediction Using Deep Neural Networks [[paper]](https://arxiv.org/pdf/1903.03695.pdf) by Ashwini Tonge, Cornelia Caragea. 2019

#### Adversarial Examples and Robustness    

* A unified view on differential privacy and robustness to adversarial examples [[paper]](https://arxiv.org/abs/1906.07982) by Rafael Pinot, et al. 2019   
* Certified Robustness to Adversarial Examples with Differential Privacy [[paper]](https://arxiv.org/abs/1802.03471) by Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana. 2018  

#### Privacy and Generative Model

* Generalization in Generative Adversarial Networks:A Novel Perspective from Privacy Protection [[paper]](https://arxiv.org/pdf/1908.07882.pdf) by Bingzhe Wu etc. 2019
* DP-CGAN : Differentially Private Synthetic Data and Label Generation [[paper]](http://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Torkzadehmahani_DP-CGAN_Differentially_Private_Synthetic_Data_and_Label_Generation_CVPRW_2019_paper.pdf) by Peter Kairouz etc. 2019
* Siamese Generative Adversarial Privatizer for Biometric Data [[paper]](https://arxiv.org/pdf/1804.08757.pdf) [[code]](https://github.com/WUT-ML/privacy) by WUT and peter kairouz. 2018
* Generative Adversarial Models for Learning Private and Fair Representations(GAPF) [[paper]](https://arxiv.org/abs/1807.05306) [[code]](https://github.com/cabreraalex/private-fair-GAN) by Chong Huang, Peter Kairouz, Lalitha Sankar. 2018
* Context-Aware Generative Adversarial Privacy(GAP) [[paper]](https://arxiv.org/abs/1710.09549) by Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal. 2017  
* Differentially Private Generative Adversarial Networks for Time Series, Continuous, and Discrete Open Data
  [[paper]](https://arxiv.org/abs/1901.02477) by Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, Patrick Duverger. 2018   
* Generative Adversarial Nets [[paper]](https://arxiv.org/abs/1406.2661) by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 2014  

#### Privacy in Federated Learning  

* Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning [[paper]](https://arxiv.org/pdf/1812.00535.pdf) by Zhibo Wang, etc. 2019
* Exploiting Unintended Feature Leakage in Collaborative Learning [[paper]](https://arxiv.org/pdf/1805.04049.pdf) [[code]](https://github.com/csong27/property-inference-collaborative-ml) by Vitaly's group. 2019
* How To Backdoor Federated Learning [[paper]](https://arxiv.org/abs/1807.00459) [[code]](https://github.com/ebagdasa/backdoor_federated_learning) by Vitaly's group. 2018
* Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning [[paper]](https://arxiv.org/abs/1702.07464) by Briland Hitaj, Giuseppe Ateniese, Fernando Perez-Cruz. 2017  

#### Private ML  

* Towards practical differentially private convex optimization [[paper]](http://www.omthakkar.com/papers/TPDPCO.pdf) by  Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, Lun Wang. 2019
* Bolt-on Differential Privacy for Scalable Stochastic Gradient Descent-based Analytics [[paper]](https://dl.acm.org/doi/pdf/10.1145/3035918.3064047) by  Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, Jeffrey F Naughton. 2017
* Deep learning with differential privacy [[paper]](https://arxiv.org/pdf/1607.00133.pdf%20) by  Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang. 2016
* Learning with differential privacy: stability, learnability and the sufficiency and necessity of ERM principle [[paper]](https://dl.acm.org/citation.cfm?id=3053465) by Yu-Xiang Wang, Jing Lei, Stephen E.Fienberg. 2016  
* Privacy-Preserving Deep Learning [[paper]](https://www.cs.cornell.edu/~shmat/shmat_ccs15.pdf) by Reza Shokri and Vitaly Shmatikov.2015  
* Differential Privacy and Machine Learning: a Survey and Review [[paper]](https://arxiv.org/abs/1412.7584) by Zhanglong Ji, Zachary C. Lipton, Charles Elkan. 2014  

#### Privacy in MAB  

* Privacy-Preserving Contextual Bandits [[paper]](https://arxiv.org/pdf/1910.05299.pdf) by Facebook AI Research. 2019
* Differentially Private Contextual Linear Bandits [[paper]](https://arxiv.org/pdf/1810.00068.pdf) by Roshan Shariff and Or Sheffet. 2018  
* Achieving Privacy in the Adversarial Multi-Armed Bandit [[paper]](https://arxiv.org/abs/1701.04222) by Aristide C. Y. Tossou, Christos Dimitrakakis. 2017  
* Differentially Private Policy Evaluation [[paper]](https://arxiv.org/abs/1603.02010) by Borja Balle, Maziar Gomrokchi, Doina Precup. 2016  
* Algorithms for Differentially Private Multi-Armed Bandits [[paper]](https://arxiv.org/abs/1511.08681) by Aristide Tossou, Christos Dimitrakakis. 2015  
* MAB problems [[paper]](http://web.eecs.umich.edu/faculty/teneketzis/papers/MAB-Survey.pdf) by Aditya Mahajan and D.teneketzis  
* (Nearly) Optimal Differentially Private Stochastic Multi-Arm Bandits [[paper]](http://auai.org/uai2015/proceedings/papers/58.pdf) by Nikita Mishra and Abhradeep Thakurta  
* Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits [[paper]](http://proceedings.mlr.press/v32/agarwalb14.pdf) by A.A, D.H, S.K, J.L, L.L, R.E.S  

#### Privacy-preserving Encrypted Neural Network

* SHE: A Fast and Accurate Deep Neural Network for Encrypted Data [[paper]](https://arxiv.org/abs/1906.00148) [[code]](https://github.com/safednn/SHE) by Qian Lou, Lei Jiang. 2019
* 2P-DNN : Privacy-Preserving Deep Neural Networks Based on Homomorphic Cryptosystem [[paper]](https://arxiv.org/abs/1807.08459) [[code]](https://github.com/zhustrong/pigstrong/tree/master/pigstrong) by Qiang Zhu, Xixiang Lv. 2018
* ABY3 A Mixed Protocol Framework for Machine Learning [[paper]](https://eprint.iacr.org/2018/403.pdf) by  Payman Mohassel, Peter Rindal . 2018
* Secureml: A system for scalable privacy-preserving machine learning [[paper]](http://web.eecs.umich.edu/~mosharaf/Readings/SecureML.pdf) by  Payman Mohassel, Yupeng Zhang. 2017


#### Differential Privacy Tutorial

* For dummies [[link]](https://robertovitillo.com/2016/07/29/differential-privacy-for-dummies/)
* Emory University CS 573 Data Privacy and Security, Fall 2018 [[course website]](http://www.cs.emory.edu/~lxiong/cs573/)
* KDD 2018 Privacy tutorial [[link]](https://sites.google.com/view/kdd2018privacytutorial)
* KDD 2018 Privacy at scale: Local Differential Privacy in Practice [[link]](

### 2023年6月份文献阅读

#### Differential privacy protection on weighted graph in wireless networks

无线网络中加权图差分隐私保护

差分隐私（differential privacy）是一种隐私保护框架，可以在涉及个人数据的情况下，对敏感信息进行保护。差分隐私的基本思想是，对于输入数据中的任何一条记录，其存在或不存在对最终输出结果的影响应该很小。因此，在加噪声时使用随机化技术来保护数据隐私，同时通过引入随机扰动等方法来减少攻击者获取敏感信息的概率。差分隐私已被广泛应用于数据挖掘、机器学习、社交网络和隐私保护等领域。差分隐私是一种隐私保护的概念和技术，旨在对个体的敏感数据进行隐私保护，同时保持对整体数据的有意义的统计分析。差分隐私的核心思想是通过向个体数据添加噪声来保护隐私，确保即使在具有所有其他数据的情况下，个体的隐私仍然得到保护。具体而言，差分隐私通过引入随机性和噪声来模糊个体数据，使得任何单个个体的贡献都无法被准确地确定。这样，即使攻击者具有访问和分析整体数据集的能力，也无法推断出个体的准确信息。通过在计算或数据发布过程中应用差分隐私，可以确保数据的隐私性，防止个人身份的泄露以及敏感信息的暴露。差分隐私的实现方法通常涉及添加噪声、扰动数据或限制查询等技术手段。它可以应用于各种场景，包括数据挖掘、机器学习、统计分析等领域，以保护个人隐私并促进数据共享和研究。同时，差分隐私也涉及一些数学理论和算法，用于量化隐私保护的强度以及在保护和分析之间取得平衡的方法。

网络图数据包含着大量的用户信息。（如何包含？）

网络图数据包含着大量的用户信息，主要是因为网络图记录了用户之间的连接和交互关系。这些关系可以包括社交网络中的朋友关系、通信网络中的通话记录、互联网上的浏览历史等。通过分析网络图数据，可以获得用户的社交圈子、兴趣爱好、交流频率、行为模式等信息。例如，通过分析社交网络图，可以了解一个用户的朋友数量、朋友之间的联系强度，甚至可以推断出用户的兴趣爱好和社交影响力。在通信网络中，通过分析通话图可以了解用户之间的通话频率、通话时长、通话时间段等，这些信息可以用于用户行为分析、社交推荐、广告定向等应用。此外，网络图数据还可以包含用户的位置信息、IP地址、设备信息等。这些附加信息可以与网络图中的连接关系结合起来，进一步揭示用户的行为模式、偏好和用户群体特征。因此，网络图数据对于了解用户行为、社交关系和个人特征具有重要意义，同时也带来了隐私保护的挑战，需要采取适当的隐私保护措施来保护用户的个人隐私。

只是简单的删除或者替换图中的节点ID，这种简单的隐藏节点信息仍然会有隐私泄漏的风险，因为结构数据仍然能够推导出用户的隐私。更多的在加权网络图中，权值也有

hierarchical random graph：层次随机图（hierarchical random graph）是一种具有层级结构的随机图模型，它可以用于描述复杂系统中的层次结构特征。在这个模型中，每个节点都被划分为不同的层级，并且每个层级中的节点之间存在不同的概率连接。这种模型在网络科学、社会学、生物学等领域有广泛的应用。

Markov Monte Carlo：马尔可夫蒙特卡罗（Markov Monte Carlo，简称MCMC）是一种常用的随机采样方法，它可以用于从复杂的概率分布中采样得到样本。它基于马尔可夫链的性质，通过在状态空间中进行转移来达到采样的目的。MCMC可以应用于很多领域，比如贝叶斯统计、机器学习等。

k-anonymity method：k-匿名（k-anonymity）是一种隐私保护方法，它可以在传输数据时保证数据中的每个个体都至少有k个相似的邻居，从而实现个体身份的隐私保护。具体来说，在k-匿名中，通过对原始数据进行通用化、泛化等操作，将具有相同属性值的记录合并为一个组，从而达到隐藏个体身份的目的。该方法在数据挖掘、隐私保护等领域得到了广泛的应用。在使用k-匿名保护方法时，由于需要对数据进行通用化、泛化等处理，因此会存在一定的背景知识假设。如果攻击者能够获取到更多的背景知识，则可能会推断出原始数据中的一些敏感信息。因此，k-匿名方法的相对有效性可能不是非常强，需要针对具体场景采用不同的隐私保护方法。

本文通过结合边权重值和图结构设计了一种隐私保护算法。

*𝜀*- differential privacy：𝜀-差分隐私 (epsilon-differential privacy) 是一种差分隐私的度量方式之一，用于衡量随机化算法的隐私保护强度。表示在相同的输入数据集中，可能输出不同的结果（比如添加了随机噪声），但每个结果的隐私保护程度都保持一致。𝜀-差分隐私是指对于任意两个互相独立的数据集 S 和 S'，它们只有微小的区别（比如只有一个记录不同），随机化算法保证以 𝜀 的概率输出的结果具有相似的隐私保护程度。𝜀 越小则表示隐私保护程度越高，但可能会降低数据实用性和准确性。

本文主要贡献：

1. 根据边的频率，在图形生成过程中使用差分隐私的拉普拉斯噪声扰动，并设计合理的图形生成规则。
2. 在获取扰动图形集之后，设计了边权重保护算法，包括合理的隐私预算分配策略。
3. 然后，将扰动边权重集成到图形的编码过程中，并挖掘图谱的频繁子图。在挖掘过程中，使用差分隐私的拉普拉斯机制和指数机制来保护图形结构，从而提高数据效用。

**Definition 2.1**给定一个随机算法𝑀，𝑅𝑎𝑛𝑔𝑒(𝑀)表示算法𝑀的所有可能输出结果的集合。对于任意两个相邻的数据集𝐷和𝐷′以及任意子集𝑆⊆𝑅𝑎𝑛𝑔𝑒(𝑀)，如果满足𝑃𝑟(𝑀(𝐷)∈𝑆)≤𝑃𝑟(𝑀(𝐷′)∈𝑆)×exp(𝜀)，则算法𝑀就满足𝜀-差分隐私。此外，差分隐私有两种常见的机制，拉普拉斯机制和指数机制 [25]。拉普拉斯机制用于数值隐私保护，而指数机制用于非数值隐私保护。两种机制定义如下。噪声机制的选择决定了查询的准确性。

**Definition 2.2** (*Laplace Mechanism*)给定数据集𝐷，函数𝑓∶𝐷$→r_d$ 满足灵敏度，记为△𝑓，那么随机算法𝑀(𝐷)=𝐹(𝐷)+𝐿𝑎𝑝(△𝑓/𝜀)满足𝜀-差分隐私。其中，𝐿𝑎𝑝(△𝑓/𝜀)是随机噪声，是拉普拉斯分布，其比例因子为△𝑓/𝜀。噪声的大小直接与△𝑓成正比，与𝜀成反比。

**Definition 2.3** (*Global Sensitivity*)给定函数𝑓∶𝐷→ $r_d$，输入为数据集𝐷，输出为一个𝑑维实数向量。对于任意相邻的数据集𝐷和𝐷′，△𝑓 = $max_{D,D'}$‖𝑓(𝐷)−𝐹(𝐷′)‖是𝑓的全局灵敏度。其中，𝑅代表映射的实数空间，𝐿1距离表示𝑓(𝐷)和𝑓(𝐷′)之间的距离。

**Definition 2.4** (*Exponential Mechanism*)

差分隐私还具有序列组合和并行组合两个属性[10]。序列组合强调隐私预算可以在方法的不同步骤中分配，而并行组合确保算法在其数据集的非重叠子集中满足差分隐私的隐私性。

在本文中，我们提出了一种隐私保护算法，以保护物联网中的加权图，主要采用差分隐私保护模型来保护边权重和图结构。首先，我们扰乱整个图集并在图生成过程中添加噪声；其次，我们为扰乱的图集设计了边权重保护算法，然后对图进行编码并将扰乱的边权重整合到其中。然后，我们挖掘和保护图集中的频繁图结构，在挖掘过程中使用差分隐私。最后，我们在真实数据集上进行实验证明，我们的方法是可行和有效的。

#### A Survey of Dummy-Based Location Privacy Protection Techniques for Location-Based Services

《基于虚假数据的位置隐私保护技术在基于位置的服务中的调查》是一篇研究论文或文章，提供了对基于位置的服务（LBS）中用于保护位置隐私的各种技术的概述和分析。

在这项调查中，重点关注基于虚假数据的技术，这些技术涉及生成和利用虚假或伪造的位置信息，以在保护用户隐私的同时允许提供LBS。这些技术旨在防止或减轻可能危及使用LBS应用程序的个人隐私的位置跟踪和推断攻击。

调查可能涵盖一系列基于虚假数据的技术，例如：

1. 虚假注入：将伪造或虚假的位置更新与用户的真实位置一起引入，以困惑对手并使其难以准确追踪用户。
2. 虚假选择：从预定义的集合中选择合适的虚假位置，或基于特定标准选择虚假位置，以确保隐私保护同时保持LBS的效用。
3. 虚假移动：通过生成逼真的虚假位置更新来模拟用户的移动模式或轨迹，从而混淆用户的实际移动。
4. 虚假更新策略：确定虚假位置更新的频率、时间和特征，以增强隐私保护并尽量减少对LBS功能的影响。

调查可能讨论不同基于虚假数据技术的优点、局限性和权衡，考虑隐私保护、LBS的效用、计算开销和通信成本等因素。

总的来说，这项调查是了解基于虚假数据的位置隐私保护技术在基于位置的服务中的现状的全面资源。它可以提供对这一研究领域的挑战、进展和未来方向的洞察。

Dummy-based location privacy protection：基于虚假位置的隐私保护

基于虚假数据的位置隐私保护是一种技术方法，旨在保护个人在使用位置相关服务时的位置隐私。该方法通过引入虚假或伪造的位置信息，使得攻击者难以确定用户的真实位置，从而保护用户的隐私。

在基于虚假数据的位置隐私保护中，主要的思想是在用户的真实位置信息中添加一些虚假的位置数据，使得攻击者无法准确追踪用户的位置。这些虚假数据可以包括虚假位置坐标、虚假位置更新时间或者虚假移动轨迹。

使用虚假数据的好处是可以提供一定的隐私保护，同时仍然可以享受位置相关服务的便利性。通过引入虚假数据，用户的真实位置更难以被追踪或推断出来，从而保护用户的隐私。

虽然基于虚假数据的位置隐私保护可以提供一定程度的隐私保护，但也存在一些限制和挑战。其中一项挑战是如何选择合适的虚假数据以平衡隐私保护和服务的实用性。虚假数据的选择需要考虑用户的行为模式、位置需求以及攻击者的推断能力。另外，虚假数据的生成和管理也需要考虑计算和通信开销等方面的因素。

因此，基于虚假数据的位置隐私保护是一种重要的技术手段，用于在位置相关服务中保护用户的隐私。它通过引入虚假数据来混淆用户的真实位置，从而提高用户的位置隐私保护水平。

Level of privacy (LoP):"Level of privacy"（隐私水平）指的是个人或组织在其个人信息或数据处理过程中所期望的或实现的隐私保护程度。

"Quality of Service"（服务质量）通常缩写为 QoS，指的是在计算机网络和通信领域中，用于描述网络或通信系统所提供的服务的质量水平。



- 这篇文章，到底在解决什么问题？（摘要、前言部分）

  

- 这个问题为什么在这个领域重要？（前言部分）

- 这些作者是怎么解决这个问题的？（具体的实验设计、优化部分）

- 这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）

#### DP-LTOD: Differential Privacy Latent Trajectory Community Discovering Services over Location-Based Social Networks

- 这篇文章，到底在解决什么问题？（摘要、前言部分）

  这篇论文解决了两个问题：

  1、根据用户的轨迹，将用户分类

  2、设计了一个新的模糊轨迹的方法

  首先，由于大规模无线通信网络的应用，基于位置的社交网络也大规模出现并应用。但是很多应用在使用用户的位置权限时。会存在泄漏用户位置隐私的可能。这些位置信息包含用户的位置轨迹。

  所以本篇文章提出了一种差分隐私潜在轨迹团发现方案（DP- LTOD，Differential Privacy Latent Trajectory Community Discovering ）。通过将原始轨迹序列模糊化为符合差分隐私的轨迹序列来保护轨迹隐私。

  还开发了一种轨迹聚类算法，根据语义距离和地理距离将轨迹分类到不同类型的聚类中。

- 这个问题为什么在这个领域重要？（前言部分）

  1、历史轨迹反应了用户的随时间的位置变化。通过分析收集大量用户的历史轨迹，可以把有相似爱好、兴趣、行为的用户聚类在一起。

  2、轨迹信息可以为许多应用软件提供支持。

  那么问题就来了，有些不被信任的第三方泄漏用户信息来做一些恶意事件。或者攻击者得到数据后分析用户数据做一些恶意事件。

- 这些作者是怎么解决这个问题的？（具体的实验设计、优化部分）

  对于用户轨迹分类问题：

  该论文考虑了两方面：语义分类，地理位置分类。综合这两种方法，可以有效的将用户轨迹分类。

  对于用户轨迹信息的保护，该论文设计了一个挑选模糊位置的方法。这个方法可以挑选出最接近真实位置的模糊位置。之后设计了两种攻击方式。贝叶斯、马尔可夫。并针对这两种攻击分别进行了拉普拉斯噪声、指数噪声的添加。使得之后的扰乱矩阵既能保护隐私又能兼顾数据的使用。

- 这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）

在本文中，我们研究了在严格的差分隐私模型下的潜在轨迹社区发现问题。首先，我们介绍了考虑用户兴趣和偏好的潜在轨迹社区发现（LTOD）方法。然后，我们探讨了设计差分隐私潜在轨迹社区发现（DP-LTOD）方案的可能性，以确保隐私和数据效用的良好性能。我们发现，在DP-LTOD中，模糊化轨迹序列的特征对于提高LTOD的准确性至关重要。如果我们能够有效地选择最优的模糊化轨迹，可以显著改善效用和隐私的权衡。为此，我们制定了一个轨迹模糊化问题，选择与原始轨迹差异最小的最优轨迹。我们证明了这个问题是NP难问题，并提出了一种启发式的轨迹模糊化算法来解决该问题。在我们的DP-LTOD方案中，另一个核心是在轨迹模糊化阶段添加差分隐私所需的噪声。为了防止贝叶斯攻击和马尔可夫攻击，我们在位置模糊化矩阵生成和轨迹序列函数生成阶段分别添加基于拉普拉斯分布和指数分布的噪声。通过形式化的隐私分析，我们证明了DP-LTOD方案满足-差分隐私。通过实验证明，我们的DP-LTOD方案能够以高准确性私密地发现潜在轨迹社区。

对于未来的工作，我们将进一步完善攻击模型，考虑社交连接图或内容文本等因素。将利用深度学习技术训练位置模糊化矩阵，以智能感知用户的实际位置并选择模糊化位置。此外，我们将考虑更多的维度（例如时间或速度等）来发现潜在轨迹社区。将发现的潜在轨迹社区应用于LBSNs中为用户推荐个性化服务将是一个有趣的研究方向。



#### Privacy attitudes and privacy behaviour: A review of current research on the privacy paradox phenomenon

隐私态度和隐私行为：对隐私悖论现象的当前研究进行综述。综述论文。



### 1.JUNE 5, 2023(WEEK 1)

#### Decision tree

| Title                                                        | Publication                                                  | Year | Authors                    | Area                                                         | Problem                                                      | Main Method                                                  | Datasets                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---- | -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **LSTM-TrajGAN: A Deep Learning Approach to Trajectory Privacy Protection** | ACM Subject Classification  Security and privacy             | 2020 | Jinmen Rao <br />Song Gao  | Deep Learning Trajectory Privacy Protection                  | 如何生成与原始数据相似度大的伪造轨迹，从而保护原始轨迹隐私并使得原始数据的使用影响降到最小。 | 使用一种深度学习模型来训练生成伪造轨迹                       | Foursquare weekly trajectory dataset in New York City        |
| **trajGANs: Using generative adversarial networks for geo-privacy protection of trajectory data (Vision paper)** |                                                              |      |                            |                                                              |                                                              |                                                              |                                                              |
| **Differentially Private Triangle and 4-Cycle Counting in the Shuffle Model** | CCS                                                          | 2022 | Jacob Imola Takao Murakami | [Computer and Communications Security](https://dl.acm.org/doi/proceedings/10.1145/3548606) |                                                              |                                                              | https://github.com/Triangle4CycleShuffle/Triangle4CycleShuffle |
| **Personalized Differential Privacy Preservation Method for Trajectory Based on Regional Density Analysis** | 2023 2nd International Conference on Big Data, Information and Computer Network (BDICN) | 2023 | Weicheng zhi               | 差分隐私  轨迹隐私                                           | 如何平衡虚假轨迹的轨迹隐私保护和轨迹的服务利用率             | 通过将轨迹中比较集中的点用一个点来代替，生成一个新的轨迹，之后再对这个点添加噪声 | Geolife<br />T-drive                                         |

##### **LSTM-TrajGAN: A Deep Learning Approach to Trajectory Privacy Protection**

> 这篇论文在解决什么问题？（摘要、前言）

随着位置服务的兴起，如何保护用户的位置、轨迹隐私？

> 这个问题为什么重要？

现如今通用的方法是从用户轨迹数据中移除一些敏感数据（用户名字、ID）。但是这些数据可能能通过空间、时间、地理位置的语义信息被推断出。而另一种把用户数据整合进地理位置的方法不仅不能保护用户隐私，反而也造成空间分析的低效。因此，为了平衡用户隐私保护和用户数据在应用中的使用，作者提出了**LSTM- TrajGAN**模型。

> 作者是如何解决这个问题？（具体的实验设计、优化部分）

**大体思路：**

<img src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2023-06-07 22.41.13.png" alt="截屏2023-06-07 22.41.13" style="zoom:50%;" />

将轨迹信息（空间、时间、语义信息）编码，之后送入LSTM- TrajGAN 模型，之后利用伪造轨迹数据进行应用。

**轨迹编码：**

轨迹的编码主要分为location、time、user id 、trajectory id、other optional attributes。

location：标准化经纬度（具体的方法是，得到数据集中所有位置点的质心点，然后所有的位置的经纬度跟这个质心的偏差来作为位置经纬度的标准化）这样的方法可以使得模型更好的学习到两个不同的轨迹点的空间偏差。

time：使用one-hot encoders 去编码。具体编码为周、小时这两种。

other optional attributes：编码为10维的二进制向量

不对用户ID和轨迹ID编码。因为它们仅用于指示点所属的用户和轨迹。

<img src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2023-06-07 22.55.30.png" alt="截屏2023-06-07 22.55.30" style="zoom:50%;" />

**轨迹填充：**

填0

**LSTM-TrajGAN Model:**

<img src="/Users/huangqiuzhao/Library/Application Support/typora-user-images/截屏2023-06-07 22.57.27.png" alt="截屏2023-06-07 22.57.27" style="zoom:50%;" />

主要是两部分，轨迹生成器和轨迹分辨器。在轨迹生成器中，首先是将噪声和经过多层感知机的真实的轨迹数据组合在一起（这里起到添加噪声的作用），然后经过全连接层再经过LSTM，之后经过全连结层和3个softmax生成了伪造轨迹。之后将真实轨迹和伪造数据一起送入轨迹分辨器中，进行分辨。在进行对抗训练中，本文的一大亮点，重新设计了损失函数（**TrajLoss for Measuring Trajectory Similarity Losses**）
$$
TrajLoss(y^r,y^p,t^r,t^s)=\alpha L_{BCE}(y^r,y^p)+\beta L_{s}(t^r,t^s)+\gamma L_{t}(t^r,t^s)+cL_{c}(t^r,t^s)
$$
其中 $y^r$ 和 $y^p$ 表示真实的标签和通过分类器预测的结果。 $t^r$ 和 $t^s$ 则表示真实轨迹和伪造轨迹。



> 这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）





##### **Personalized Differential Privacy Preservation Method for Trajectory Based on Regional Density Analysis**

> 将基于区域密度分析的个性化差分隐私保护方法应用于轨迹数据

- 这篇文章，到底在解决什么问题？（摘要、前言部分）

> 目前大部分方法都对轨迹中的所有点进行噪声的添加，同时对轨迹中的点分配同样的隐私预算。这样不能对轨迹中的用户的特征进行个性化的轨迹隐私保护。

- 这个问题为什么在这个领域重要？（前言部分）
- 这些作者是怎么解决这个问题的？（具体的实验设计、优化部分）

> 针对这个问题，提出了一种基于区域密度分析的轨迹保护方法。分析每个用户的停留区域，计算停留点，并根据时间和距离阈值重构轨迹集。使用基于局部密度峰值的最小生成树聚类算法获取用户轨迹的隐私敏感位置点和活动热点区域。根据设计的隐私重要程度表达式，计算每个敏感位置点的隐私得分，并为其分配适当的隐私预算值。

- 这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）

> 主要的亮点在于，使用经纬度和时间，找到了一个密度最高区域。并将其重构，然后根据设计的隐私重要程度表达式，计算每个敏感位置的隐私得分，并为其分配适当的隐私预算值。这其实算是将轨迹中的重要的点找出来（毕竟轨迹中的的点有很多，每一个点其实所包含的信息是不一样的。比如在街道上的一些点和在餐厅的点，它们包含的信息当然不同），为其分配不同的隐私预算值。其实这个的隐私预算值可以视为权重值。含有信息不同的轨迹的点自然需要不同的权重值。然后根据隐私预算为这个重构的轨迹图添加噪声。
>
> 我觉得这个问题的解决还有一个点可以考虑进去，首先作者只是考虑了经纬度、时间。一些点所处位置的语义信息并没有被考虑进去。是否可以将这个语义信息考虑进去？不同的点他们所处的位置可能是咖啡店、书店等等。这个信息其实也有较大的影响。



这篇文章，到底在解决什么问题？（摘要、前言部分）

这个问题为什么在这个领域重要？（前言部分）

这些作者是怎么解决这个问题的？（具体的实验设计、优化部分）

这个问题的解决有什么亮点、局限，有什么应用？（数据分析、应用展示、结论、展望部分）