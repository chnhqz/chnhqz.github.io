---
title: 音视频
date: 2024-05-05 13:07:50
tags:
---



<img src="截屏2024-05-05 23.17.47.png" alt="截屏2024-05-05 23.17.47" style="zoom:50%;" />



#### 将MP4转化为FLV格式

- `avformat_alloc_output_context2()` 和 `avformat_free_context()` 是 FFmpeg 中用于操作输出格式上下文（output format context）的函数。
  - `avformat_alloc_output_context2()` 用于分配并初始化一个输出格式上下文。
  - `avformat_free_context()` 用于释放一个输出格式上下文及其相关资源。
- `avformat_new_stream()` 用于创建新的流（stream）。这个函数允许你在一个封装器（如 AVFormatContext）中添加一个新的音频或视频流。
- `avcodec_parameters_copy()` 是一个函数，用于复制编解码器参数（codec parameters）。它可以将源编解码器参数复制到目标编解码器参数，确保目标参数与源参数具有相同的属性和配置。
- `avformat_write_header()` 是一个函数，用于向输出容器（output container）写入容器头（container header）。在使用 FFmpeg 编码或封装媒体文件时，通常需要在写入数据流之前调用此函数。
- `av_write_frame()` 和 `av_interleaved_write_frame()` 是用于将音视频帧写入输出文件的函数。

  - `av_write_frame()` 用于将音视频帧写入输出文件，但不进行交错写入（interleaved writing）。
  - `av_interleaved_write_frame()` 则是进行交错写入的版本，确保音频和视频帧以交错的方式写入输出文件，以便播放器能够按照正确的顺序解码和播放。
- `av_write_trailer()` 是一个函数，用于写入封装器（muxer）的尾部（trailer），完成媒体文件的封装过程。在使用 FFmpeg 编码或封装媒体文件时，通常需要在写入所有数据流并关闭文件之前调用此函数。

> 错误记录：
>
> `error C2065: “INTMAX_MAX”: 未声明的标识符 (编译源文件 src\main.cpp)`
>
> 打开项目属性-> C/C++ -> 预处理器-> 预处理器定义
>
> 添加： **__STDC_LIMIT_MACROS**  
>
> 遇到 `不能将 const AVOutputformat * 类型的值分配到 AVOutputformat * 类型的实体 ` 错误
>
> 这个错误可能是因为试图将一个 `const AVOutputFormat` 类型的值分配给一个 `AVOutputFormat` 类型的变量，而这两个类型不完全相同。
>
> 通常情况下，可以通过使用类型转换来解决这个问题。例如：
>
> ```c
> const AVOutputFormat *const_format_ptr = ...; // 指向常量 AVOutputFormat 的指针
> AVOutputFormat *format_ptr = (AVOutputFormat *)const_format_ptr; // 转换为普通 AVOutputFormat 指针
> ```
>
> 但是，确保应用程序逻辑和安全性不受影响，因为在将常量指针转换为非常量指针时可能会引入潜在的错误。

```c++
static void log_packet(const AVFormatContext* fmt_ctx, const AVPacket* pkt, const char* tag)
{
	AVRational* time_base = &fmt_ctx->streams[pkt->stream_index]->time_base;
}

int mp42flv() {
	AVOutputFormat* ofmt = NULL;
	AVFormatContext* ifmt_ctx = NULL, * ofmt_ctx = NULL;
	AVPacket pkt;
	char src[] = "input.mp4";
	char dst[] = "output.flv";
	int ret, i;
	int stream_index = 0;
	int *stream_mapping = NULL;
	int stream_mapping_size = 0;

	if ((ret = avformat_open_input(&ifmt_ctx, src, 0, 0)) < 0) {
		// 打开输入的多媒体文件，生成多媒体上下文
		fprintf(stderr, "Could not open input file '%s'.\n", src);
		goto end;
	}

	if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
		fprintf(stderr, "Failed to retrieve input stream information.\n");
		goto end;
	}

	av_dump_format(ifmt_ctx, 0, src, 0);

	avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, dst);	// 输出的上下文

	if (!ofmt_ctx) {
		fprintf(stderr, "Could not create output context.\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}

	stream_mapping_size = ifmt_ctx->nb_streams;
	stream_mapping = (int*)av_malloc_array(stream_mapping_size, sizeof(*stream_mapping));
	if (!stream_mapping) {
		ret = AVERROR(ENOMEM);
		printf("-1.\n");
		system("pause");
		goto end;
	}

	ofmt = (AVOutputFormat*)ofmt_ctx->oformat;

	for (i = 0; i < ifmt_ctx->nb_streams; i++) {
		AVStream* out_stream;
		AVStream* in_stream = ifmt_ctx->streams[1];
		AVCodecParameters* in_codecpar = in_stream->codecpar;

		if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO &&
			in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO &&
			in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) {
			stream_mapping[i] = -1;
			continue;
		}

		stream_mapping[i] = stream_index++;
		out_stream = avformat_new_stream(ofmt_ctx, NULL);

		if (!out_stream) {
			fprintf(stderr, "Failed allocating output stream.\n");
			ret = AVERROR_UNKNOWN;
			goto end;
		}

		ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar);
		if (ret < 0) {
			fprintf(stderr, "Failed to copy codec parameters.\n");
			goto end;
		}
		out_stream->codecpar->codec_tag = 0;
	}
		
	av_dump_format(ofmt_ctx, 0, dst, 1);

	system("pause");
	if (!(ofmt->flags & AVFMT_NOFILE)) {
		ret = avio_open(&ofmt_ctx->pb, dst, AVIO_FLAG_WRITE);
		if (ret < 0) {
			fprintf(stderr, "Could not open file '%s'.\n", dst);
			goto end;
		}
	}

	ret = avformat_write_header(ofmt_ctx, NULL);
	if (ret < 0) {
		fprintf(stderr, "Error occurred when opening output file.\n");
		goto end;
	}

	while (1) {
		AVStream* in_stream, * out_stream;
		ret = av_read_frame(ifmt_ctx, &pkt);
		if (ret < 0) {
			break;
		}
		in_stream = ifmt_ctx->streams[pkt.stream_index];
		if (pkt.stream_index >= stream_mapping_size || stream_mapping[pkt.stream_index] < 0) {
			av_packet_unref(&pkt);
			continue;
		}
		pkt.stream_index = stream_mapping[pkt.stream_index];
		out_stream = ofmt_ctx->streams[pkt.stream_index];
		log_packet(ifmt_ctx, &pkt, "in");

		/* copy packet*/
		pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base,
			AV_ROUND_PASS_MINMAX);
		pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base,
			AV_ROUND_PASS_MINMAX);
		pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
		pkt.pos = -1;
		log_packet(ofmt_ctx, &pkt, "out");
		ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
		if (ret < 0) {
			fprintf(stderr, "Error muxing packet.\n");
			break;
		}
		av_packet_unref(&pkt);

	}
	av_write_trailer(ofmt_ctx);

end:
	avformat_close_input(&ifmt_ctx);

	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE)) {
		avio_closep(&ofmt_ctx->pb);
	}
	avformat_free_context(ofmt_ctx);
	av_freep(&stream_mapping);
	return -1;

}

```

#### FFmpeg文件与目录操作

文件的删除与重命名

- `avpriv_io_delete()` 是 FFmpeg 中的一个私有函数，用于删除指定的文件或目录。它可以删除本地文件系统上的文件或目录，并提供了一些选项来控制删除操作的行为。由于 `avpriv_io_delete()` 是一个私有函数，它并不是 FFmpeg 公共 API 的一部分，因此在正式的应用程序中使用它可能会有一些风险，因为它的行为和接口可能随时发生变化。通常情况下，您应该尽量避免使用私有函数，而是使用 FFmpeg 提供的公共 API。
- `avpriv_io_move()` 是 FFmpeg 中的一个私有函数，用于移动文件或目录。它可以将文件或目录从一个位置移动到另一个位置，并提供了一些选项来控制移动操作的行为。

#### 实现一个简单的小咖秀

实现步骤：

1. 从两个媒体文件中分别抽取音频与视频轨
2. 将音频与视频轨合并成一个新文件
3. 对音频与视频轨进行裁剪

#### libavcodec/avcodec.h

- `AVCodec` 编码器结构体

- `AVCodecContext` 编码器上下文

- `AVFrame` 解码后的帧

- `av_frame_alloc()` 和 `av_frame_free()` 是 FFmpeg 中用于操作帧（frame）的函数。

  - `av_frame_alloc()` 用于分配一个新的帧，并返回一个指向该帧的指针。您可以使用此函数来创建一个空的帧，然后填充数据。

  - `av_frame_free()` 用于释放先前分配的帧。当您完成对帧的使用后，应该调用此函数来释放相关的内存，并避免内存泄漏。

  这两个函数通常在编解码过程中用于分配和释放帧内存。

- `avcodec_alloc_context3()` 分配一个`AVCodecContext`结构体的实例，用于存储编解码器的参数和状态。

- `avcodec_free_context()`  释放上下文

**解码步骤**

1. 查找解码器 `avcodec_find_decoder`
2. 打开解码器 `avcodec_open2`
3. 解码 `avcodec_decode_video2`

**编码步骤**

1. 查找编码器 `avcodec_find_encoder_by_name/avcodec_find_encoder` 在 FFmpeg 中，`avcodec_find_encoder_by_name()` 和 `avcodec_find_encoder()` 都是用于查找编码器的函数。
   - `avcodec_find_encoder_by_name()` 根据编码器的名称查找编码器。您需要传递编码器的名称作为参数，函数将返回一个指向 `AVCodec` 结构体的指针，该结构体包含了编码器的信息。

   - `avcodec_find_encoder()` 则是根据编码器的 ID（编码器的枚举值）来查找编码器。您需要传递编码器的 ID 作为参数，函数将返回一个指向 `AVCodec` 结构体的指针，该结构体包含了编码器的信息。

2. 设置编码参数（分辨率、帧率、...），并打开编码器 `avcodec_open2`
3. 编码 `avcodec_encode_video2` 

 **注意：音频，视频的编解码大概类似，大体都分为三步：找到编解码器、（设置参数）打开编解码器、进行编解码。**

#### 从MP4截取一段视频

- `av_seek_frame` 函数用于在媒体文件中定位到指定的帧。

  参数解释：
  - `AVFormatContext *s`：表示输入或输出媒体文件的格式上下文。
  - `int stream_index`：表示要寻找的媒体流的索引。
  - `int64_t timestamp`：表示要寻找的时间戳，以媒体流的基本时间单位为单位（通常是微秒）。
  - `int flags`：表示定位的标志位，可以是 `AVSEEK_FLAG_BACKWARD`（向后定位）、`AVSEEK_FLAG_BYTE`（以字节为单位定位）等。
    

  该函数将在媒体文件中定位到距离给定时间戳最近的关键帧，并更新 `AVFormatContext` 中的信息以反映新的位置。

> avformat_new_stream 的用法：
>
> AVStream 即是流通道。例如我们将 H264 和 AAC 码流存储为MP4文件的时候，就需要在 MP4文件中增加两个流通道，一个存储Video：H264，一个存储Audio：AAC。（假设H264和AAC只包含单个流通道）。
>
> AVStream包含很多参数，用于记录通道信息，其中最重要的是 :
>
> AVCodecParameters * codecpar ：用于记录编码后的流信息，即通道中存储的流的编码信息。
>
> AVRational time_base ：AVStream通道的时间基，时间基是个相当重要的概念。（可参考之后的关于ffmpeg时间的文章）
>
> 需要注意的是：现在的 ffmpeg 3.1.4版本已经使用AVCodecParameters * codecpar替换了原先的CodecContext* codec !
>
> avformat_new_stream 在 AVFormatContext中创建 Stream 通道
>
> ```c++
> //AVFormatContext:
> unsigned int nb_streams; // 记录stream通道数目
> AVStream **streams;      // 存储stream通道
> 
> // AVStream
> int index;							// 在AVFormatContext 中所处的通道索引
> ```
>
> ![截屏2024-05-07 16.22.19](/Users/huangqiuzhao/blog/source/_posts/音视频/截屏2024-05-07 16.22.19.png)
>
> `avformat_new_stream`之后便在`AVFormatContext`里增加了`AVStream`增加了`AVStream`通道（相关的`index`已经被设置了）。之后我们就可以自行设置`AVStream`的一些信息。例如 `codec_id,format,bit_rate,width,heoght.....`
>
> avformat_new_stream(AVFormatContext * s, const AVCodec * c)
>
> AVCodec ：s需要通信的视频对应的编码方式。
> 在已知codec_id的情况下可以通过 codec=avcodec_find_decoder(codec_id)；的方式得到。
>
> 原文：https://blog.csdn.net/u014162133/article/details/82258488
>
> https://blog.csdn.net/Kami_Jiang/article/details/106784580
>
> 







#### SDL事件基本原理

1. SDL将所有事件都存放在一个队列中
2. 所有对事件的操作，其实就是对队列的操作

**SDL事件种类**

1. `SDL_WindowEvent`：窗口事件
2. `SDL_KeyboardEvent` ：键盘事件
3. `SDL_MouseMotionEvent` ：鼠标事件
4. 自定义事件

`SDL_PollEvent` DL 库中的一个函数，用于检查是否有任何事件发生。它会检查事件队列，并返回队列中的下一个事件（如果有）。轮询操作，要等待一段时间。如果不等待时间，CPU过载

`SDL_WaitEvent` 事件触发机制，用于等待事件的发生。如果当前事件队列为空，则该函数将会一直等待，直到有事件发生。

**纹理渲染**

![截屏2024-05-08 14.22.58](/Users/huangqiuzhao/blog/source/_posts/音视频/截屏2024-05-08 14.22.58.png)

- `SDL_CreateTexture()` 
  - `format:YUV,RGB`
  - `access:Texture类型,Target,Stream`
- `SDL_DestroyTexture()`
- `SDL_SetRenderTarget()`用于设置渲染目标，即指定在哪个渲染器上进行绘制。你可以将渲染目标设置为一个纹理或者窗口。
- `SDL_RenderClear()`用于清除当前渲染目标上的所有内容，将其填充为指定的颜色。
- `SDL_RenderCopy()`用于将纹理复制到当前渲染目标上。
- `SDL_RenderPresent`用于更新窗口显示，将之前所有的渲染操作绘制到窗口上。



#### YUV播放器

**创建线程**

- `SDL_CreateThread()`函数用于创建一个新的线程。它接受两个参数：一个指向函数的指针，这个函数将作为新线程的入口点；以及一个可选的参数，将传递给新线程的入口函数。函数返回一个指向新线程的 `SDL_Thread` 结构体的指针。
  - `fn` 线程执行函数
  - `name` 线程名
  - `data` 执行函数参数

**更新纹理**

- `SDL_UpdateTexture()` 函数用于更新纹理的像素数据。它接受纹理、矩形区域和像素数据作为参数，以更新纹理的一部分或全部像素。
- `SDL_UpdateYUVTexture` 函数用于更新YUV格式的纹理像素数据。它接受纹理、矩形区域、Y、U和V平面的像素数据作为参数，以更新纹理的一部分或全部像素。



#### SDL音频API

- `SDL_OpenAudio/SDL_CloseAudio` `SDL_OpenAudio()`函数用于初始化音频子系统，并打开音频设备以进行音频播放或录制。而`SDL_CloseAudio()`函数用于关闭音频设备并释放音频子系统的资源。
- `SDL_PauseAudio` `SDL_PauseAudio` 用于暂停或继续音频回放。
- `SDL_MixAudio` 用于将音频数据混合到音频缓冲区中。



#### PCM 音频播放器

```c++
#pragma once
#define SDL_MAIN_HANDLED
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
extern "C" {
#include "SDL2/SDL.h"
#include "libavcodec/avcodec.h"
}

#define BLOCK_SIZE 4096000
static Uint8* audio_buf = NULL;
static size_t buffer_len = 0;
static Uint8* audio_pos = NULL;

void read_audio_data(void* udata, Uint8* stream, int len) {
	if (buffer_len == 0) {
		return;
	}

	SDL_memset(stream, 0, len);
	len = (len < buffer_len) ? len : buffer_len;
	SDL_MixAudio(stream, audio_pos, len, SDL_MIX_MAXVOLUME);

	audio_pos += len;
	buffer_len -= len;
}

class PCM_play {
public:
	int pcm_player(){
		// SDL 初始化
		int ret = -1;
		char path[] = "NULL";
		FILE* audio_fd = NULL;


		if (SDL_Init(SDL_INIT_AUDIO)) {
			SDL_Log("Failed to initial.\n");
			return ret;
		}

		audio_fd = fopen(path, "r");

		if (!audio_fd) {
			SDL_Log("Failed to open pcm file!\n");
			goto __FAIL;
		}

		// 分配内存空间
		audio_buf = (Uint8*)malloc(BLOCK_SIZE);
		if (!audio_buf) {
			SDL_Log("Failed to alloc memory!\n");
			goto __FAIL;
		}

		// spec 音频参数 采样率
		SDL_AudioSpec spec;
		spec.freq = 44100;
		spec.channels = 2;
		spec.format = AUDIO_S16SYS;
		spec.silence = 0;
		spec.callback = read_audio_data;
		spec.userdata = NULL;

		if (SDL_OpenAudio(&spec, NULL)) {
			SDL_Log("Failed to open audio device!\n");\
			goto __FAIL;
		}

		SDL_PauseAudio(0);

		do {
			buffer_len = fread(audio_buf, 1, BLOCK_SIZE, audio_fd);
			audio_pos = audio_buf;
			while (audio_pos < (audio_buf + buffer_len)) {
				SDL_Delay(1);
			}
		} while (buffer_len != 0);

		SDL_CloseAudio();

		ret = 0;

	__FAIL:
		if (audio_buf) {
			free(audio_buf);
		}
		if (audio_fd) {
			fclose(audio_fd);
		}
		SDL_Quit();
		return 0;
	}
};
```

#### 简单视频播放器

利用`FFmpeg` 将`h264`等数据解码成`YUV`数据，通过`SDL`渲染。

```c++
sws_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height,
	pCodecCtx->pix_fmt, pCodecCtx->width, pCodecCtx->height,
	AV_PIX_FMT_YUV420P, SWS_BILINEAR,
	NULL, NULL, NULL);
```

> 这段代码使用了 libswscale 库中的 `sws_getContext` 函数来创建一个图像转换上下文（sws_ctx）。这个上下文用于将一个像素格式的图像转换成另一种像素格式的图像，以便在播放或处理视频时进行必要的格式转换。
>
> 具体来说，这段代码的作用是将视频帧从输入编解码器上下文（pCodecCtx）的像素格式（pCodecCtx->pix_fmt）转换为 YUV420P 格式。在大多数情况下，YUV420P 是视频处理中常用的一种格式，因为它包含了亮度（Y）和色度（U、V）信息，而且是压缩视频格式的一种常见格式。
>
> 下面是这段代码的参数解释：
> - `pCodecCtx->width` 和 `pCodecCtx->height`: 输入视频帧的宽度和高度。
> - `pCodecCtx->pix_fmt`: 输入视频帧的像素格式。
> - `pCodecCtx->width` 和 `pCodecCtx->height`: 输出视频帧的宽度和高度，通常与输入视频帧的尺寸相同。
> - `AV_PIX_FMT_YUV420P`: 输出视频帧的像素格式，这里选择了 YUV420P 格式。
> - `SWS_BILINEAR`: 像素转换的算法，这里选择了双线性插值算法，用于平滑图像。
>
> 最后三个参数是用于设置特殊选项的，通常情况下可以设为 NULL。
>
> 总之，这段代码的目的是创建一个图像转换上下文，以便在播放视频时将输入的像素格式转换为 YUV420P 格式，以便后续处理或显示。

```c++
pict = (AVPicture*)malloc(sizeof(AVPicture));
avpicture_alloc(pict, AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height);
```

> 这段代码分配了一个大小合适的内存块来存储 YUV420P 格式的图像数据，并使用 `avpicture_alloc` 函数来初始化 `pict` 指针所指向的内存块，使其可以存储 YUV420P 格式的图像数据。
>
> 具体来说，`avpicture_alloc` 函数会根据指定的像素格式（AV_PIX_FMT_YUV420P）、宽度（pCodecCtx->width）和高度（pCodecCtx->height），为 `pict` 指向的内存块分配足够的空间，并根据像素格式的要求对内存块进行初始化，以便后续存储 YUV420P 格式的图像数据。
>
> 这段代码的目的是为后续的视频解码和处理准备一个用于存储图像数据的缓冲区，以便于在播放或处理视频时使用。

```c++
sws_scale(sws_ctx, (uint8_t const* const*)pFrame->data, pFrame->linesize, 0, pCodecCtx->height, pict->data, pict->linesize);
```

> 这段代码使用 `sws_scale` 函数将解码后的视频帧数据转换为 YUV420P 格式，并将结果存储在 `pict` 所指向的内存块中。
>
> 具体来说，`sws_scale` 函数会将输入的视频帧数据（`pFrame->data`）按照指定的转换参数（`sws_ctx`）进行转换，转换后的数据存储在输出缓冲区（`pict->data`）中。参数 `pFrame->linesize` 表示输入数据每行的字节数，而 `pict->linesize` 表示输出数据每行的字节数。函数还会根据输入和输出的图像大小进行缩放和裁剪操作，以确保输出数据的正确性和一致性。
>
> 这段代码的作用是将解码后的视频帧数据转换为 YUV420P 格式，并存储在指定的内存块中，以便后续的视频处理和显示。



```c++
int player() {
	int ret = -1;
	char filepath[] = "input.mp4";
	// 多媒体文件上下文
	AVFormatContext* pFormatCtx = NULL;			
	int i, videoStream;
	// 编解码上下文
	AVCodecContext* pCodecCtxOrig = NULL;
	AVCodecContext* pCodecCtx = NULL;
	// 图像裁剪上下文
	struct SwsContext* sws_ctx = NULL;
	// 编解码器
	AVCodec* pCodec = NULL;
	// 解码后的数据帧
	AVFrame* pFrame = NULL;
	// 解码前的数据包
	AVPacket packet;

	int frameFinished;
	float aspect_ratio;

	// 解码出来的YUV数据存放在
	AVPicture* pict = NULL;
	SDL_Rect rect;
	Uint32 pixformat;

	// render
	SDL_Window* win = NULL;
	SDL_Renderer* renderer = NULL;
	SDL_Texture* texture = NULL;

	// 设置默认的窗口大小
	int w_width = 640;
	int w_height = 480;
	if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Could not initialize SDL - %s.\n", SDL_GetError());
		goto __FAIL;
	}
	av_register_all();

	if (avformat_open_input(&pFormatCtx, filepath, NULL, NULL) != 0) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Could not open video file\n");
		goto __FAIL;
	}

	if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to find stream information.\n");
		goto __FAIL;
	}
	av_dump_format(pFormatCtx, 0, filepath, 0);

	// 找到第一个视频流
	videoStream = -1;
	for (i = 0; i < pFormatCtx->nb_streams; i++) {
		if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) {
			videoStream = i;
			printf("视频流编号：%d\n", i);
			break;
		}
	}

	if (videoStream == -1) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Din't find a video stream.\n");
		goto __FAIL;
	}

	/* 这行代码的目的是获取视频流的编解码上下文（`AVCodecContext`）。
	`pFormatCtx` 是输入格式上下文，`streams[videoStream]` 表示视频流，
	然后通过 `codec` 成员获取该流的编解码上下文。*/
	pCodecCtxOrig = pFormatCtx->streams[videoStream]->codec;

	// 找到视频流的解码器
	pCodec = avcodec_find_decoder(pCodecCtxOrig->codec_id);

	if (pCodec == NULL) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Unsupported codec.\n");
		goto __FAIL;
	}

	// 复制 上下文 为了不破坏原来的流
	pCodecCtx = avcodec_alloc_context3(pCodec);
	if (avcodec_copy_context(pCodecCtx, pCodecCtxOrig) != 0) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Couldn't copy codec context.\n");
		goto __FAIL;
	}

	// 打开解码器
	if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to open decoder!\n");
		goto __FAIL;
	}

	// 分配视频帧
	pFrame = av_frame_alloc();

	w_width = pCodecCtx->width;
	w_height = pCodecCtx->height;

	win = SDL_CreateWindow("Media Player",
		SDL_WINDOWPOS_UNDEFINED,
		SDL_WINDOWPOS_UNDEFINED,
		w_width, w_height,
		SDL_WINDOW_OPENGL | SDL_WINDOW_RESIZABLE);

	if (!win) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to create window by SDL");
		goto __FAIL;
	}

	renderer = SDL_CreateRenderer(win, -1, 0);

	if (!renderer) {
		SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to create Renderer by SDL");
		goto __FAIL;
	}

	pixformat = SDL_PIXELFORMAT_IYUV;
	texture = SDL_CreateTexture(renderer, pixformat,
		SDL_TEXTUREACCESS_STREAMING, w_width, w_height);

	sws_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height,
		pCodecCtx->pix_fmt, pCodecCtx->width, pCodecCtx->height,
		AV_PIX_FMT_YUV420P, SWS_BILINEAR,
		NULL, NULL, NULL);

	pict = (AVPicture*)malloc(sizeof(AVPicture));
	avpicture_alloc(pict, AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height);

	while (av_read_frame(pFormatCtx, &packet) >= 0) {
		if (packet.stream_index == videoStream) {
			// 解码视频帧
			avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);
			// 得到解码后的视频帧
			if (frameFinished) {
				// 将图片转化为 SDL 使用的 YUV 格式
				sws_scale(sws_ctx, (uint8_t const* const*)pFrame->data,
					pFrame->linesize, 0, pCodecCtx->height, pict->data, pict->linesize);
					
				SDL_UpdateYUVTexture(texture, NULL,
					pict->data[0], pict->linesize[0],
					pict->data[1], pict->linesize[1],
					pict->data[2], pict->linesize[2]
					);

				rect.x = 0;
				rect.y = 0;
				rect.w = pCodecCtx->width;
				rect.h = pCodecCtx->height;

				SDL_RenderClear(renderer);
				SDL_RenderCopy(renderer, texture, NULL, &rect);
				SDL_RenderPresent(renderer);

			}
		}
		av_free_packet(&packet);
	}





__FAIL:
	// Free the YUV frame
	if (pFrame) {
		av_frame_free(&pFrame);
	}

	// Close the codec
	if (pCodecCtx) {
		avcodec_close(pCodecCtx);
	}

	if (pCodecCtxOrig) {
		avcodec_close(pCodecCtxOrig);
	}

	// Close the video file
	if (pFormatCtx) {
		avformat_close_input(&pFormatCtx);
	}

	if (pict) {
		avpicture_free(pict);
		free(pict);
	}

	if (win) {
		SDL_DestroyWindow(win);
	}

	if (renderer) {
		SDL_DestroyRenderer(renderer);
	}

	if (texture) {
		SDL_DestroyTexture(texture);
	}

	SDL_Quit();
	return 0;
}
```

#### 使用队列存放音频包-播放器

```c++
#pragma once
#define SDL_MAIN_HANDLED
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
extern "C" {
#include "SDL2/SDL.h"
#include "libavcodec/avcodec.h"
#include <libavutil/log.h>
#include <libavformat/avformat.h>
#include <libavutil/avutil.h>
#include <libavcodec/version.h>
#include <libavutil/mem.h>
#include <libswscale/swscale.h>
#include <libswresample/swresample.h>
#include <assert.h>

}

#define BLOCK_SIZE 4096000
static Uint8* audio_buf = NULL;
static size_t buffer_len = 0;
static Uint8* audio_pos = NULL;


#if LIBAVCODEC_VERSION_INT < AV_VERSION_INT(55, 28, 1)
#define av_frame_alloc avcodec_alloc_frame
#define av_frame_free avcodec_free_frame
#endif

#define SDL_AUDIO_BUFFER_SIZE 1024
#define MAX_AUDIO_FRAME_SIZE 192000
int quit = 0;
static struct SwrContext* audio_convert_ctx = NULL;

typedef struct PacketQueue {
	AVPacketList* first_pkt, * last_pkt;
	int nb_packets;
	int size;
	SDL_mutex* mutex;
	SDL_cond* cond;
} PacketQueue;

PacketQueue audioq;

void packet_queue_init(PacketQueue* q) {
	memset(q, 0, sizeof(PacketQueue));
	q->mutex = SDL_CreateMutex();
	q->cond = SDL_CreateCond();
}

int packet_queue_put(PacketQueue* q, AVPacket* pkt) {

	AVPacketList* pkt1;
	if (av_dup_packet(pkt) < 0) {
		return -1;
	}
	pkt1 = (AVPacketList*)av_malloc(sizeof(AVPacketList));
	if (!pkt1)
		return -1;
	pkt1->pkt = *pkt;
	pkt1->next = NULL;

	SDL_LockMutex(q->mutex);

	if (!q->last_pkt) {
		q->first_pkt = pkt1;
	}
	else {
		q->last_pkt->next = pkt1;
	}

	q->last_pkt = pkt1;
	q->nb_packets++;
	q->size += pkt1->pkt.size;
	SDL_CondSignal(q->cond);

	SDL_UnlockMutex(q->mutex);
	return 0;
}

int packet_queue_get(PacketQueue* q, AVPacket* pkt, int block)
{
	AVPacketList* pkt1;
	int ret;

	SDL_LockMutex(q->mutex);

	for (;;) {

		if (quit) {
			ret = -1;
			break;
		}

		pkt1 = q->first_pkt;
		if (pkt1) {
			q->first_pkt = pkt1->next;
			if (!q->first_pkt)
				q->last_pkt = NULL;
			q->nb_packets--;
			q->size -= pkt1->pkt.size;
			*pkt = pkt1->pkt;
			av_free(pkt1);
			ret = 1;
			break;
		}
		else if (!block) {
			ret = 0;
			break;
		}
		else {
			SDL_CondWait(q->cond, q->mutex);
		}
	}
	SDL_UnlockMutex(q->mutex);
	return ret;
}

int audio_decode_frame(AVCodecContext* aCodecCtx, uint8_t* audio_buf, int buf_size) {

	static AVPacket pkt;
	static uint8_t* audio_pkt_data = NULL;
	static int audio_pkt_size = 0;
	static AVFrame frame;

	int len1, data_size = 0;

	for (;;) {
		while (audio_pkt_size > 0) {
			int got_frame = 0;
			len1 = avcodec_decode_audio4(aCodecCtx, &frame, &got_frame, &pkt);
			if (len1 < 0) {
				audio_pkt_size = 0;
				break;
			}
			audio_pkt_data += len1;
			audio_pkt_size -= len1;
			data_size = 0;
			if (got_frame) {
				data_size = 2 * 2 * frame.nb_samples;
				assert(data_size <= buf_size);
				swr_convert(audio_convert_ctx,
					&audio_buf,
					MAX_AUDIO_FRAME_SIZE * 3 / 2,
					(const uint8_t**)frame.data,
					frame.nb_samples);
			}
			if (data_size <= 0) {
				continue;
			}
			return data_size;
		}
		if (pkt.data)
			av_free_packet(&pkt);

		if (quit) {
			return -1;
		}

		if (packet_queue_get(&audioq, &pkt, 1) < 0) {
			return -1;
		}
		audio_pkt_data = pkt.data;
		audio_pkt_size = pkt.size;
	}
}

void audio_callback(void* userdata, Uint8* stream, int len) {

	AVCodecContext* aCodecCtx = (AVCodecContext*)userdata;
	int len1, audio_size;

	static uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE * 3) / 2];
	static unsigned int audio_buf_size = 0;
	static unsigned int audio_buf_index = 0;

	while (len > 0) {
		if (audio_buf_index >= audio_buf_size) {
			/* We have already sent all our data; get more*/
			audio_size = audio_decode_frame(aCodecCtx, audio_buf, sizeof(audio_buf));
			if (audio_size < 0) {
				audio_buf_size = 1024;
				memset(audio_buf, 0, audio_buf_size);
			}
			else {
				audio_buf_size = audio_size;
			}
			audio_buf_index = 0;
		}
		len1 = audio_buf_size - audio_buf_index;
		if (len1 > len) len1 = len;
		memcpy(stream, (uint8_t*)audio_buf + audio_buf_index, len1);
		len -= len1;
		stream += len1;
		audio_buf_index += len1;
	}

}

class PCM_play {
public:
	int player() {
		int ret = -1;
		char filepath[] = "input.mp4";
		// 多媒体文件上下文
		AVFormatContext* pFormatCtx = NULL;			
		int i, videoStream, audioStream;
		// 编解码上下文 视频流
		AVCodecContext* pCodecCtxOrig = NULL;
		AVCodecContext* pCodecCtx = NULL;

		// 图像裁剪上下文
		struct SwsContext* sws_ctx = NULL;

		// 音频流
		AVCodecContext* aCodecCtxtOrig = NULL;
		AVCodecContext* aCodecCtx = NULL;
		AVCodec* aCodec = NULL;
		
		
		int64_t in_channel_layout;
		int64_t out_channel_layout;

		// 编解码器
		AVCodec* pCodec = NULL;
		// 解码后的数据帧
		AVFrame* pFrame = NULL;
		// 解码前的数据包
		AVPacket packet;

		int frameFinished;
		float aspect_ratio;

		// 解码出来的YUV数据存放在
		AVPicture* pict = NULL;
		SDL_Rect rect;
		Uint32 pixformat;

		// render
		SDL_Window* win = NULL;
		SDL_Renderer* renderer = NULL;
		SDL_Texture* texture = NULL;

		// 设置默认的窗口大小
		int w_width = 640;
		int w_height = 480;
		
		// 事件
		SDL_Event event;

		// 音频 指定音频播放的参数
		SDL_AudioSpec wanted_spec, spec;

		if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Could not initialize SDL - %s.\n", SDL_GetError());
			goto __FAIL;
		}
		av_register_all();

		if (avformat_open_input(&pFormatCtx, filepath, NULL, NULL) != 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Could not open video file\n");
			goto __FAIL;
		}

		if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to find stream information.\n");
			goto __FAIL;
		}
		av_dump_format(pFormatCtx, 0, filepath, 0);

		// 找到 视频流 音频流
		videoStream = -1;
		audioStream = -1;
		for (i = 0; i < pFormatCtx->nb_streams; i++) {
			if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) {
				videoStream = i;
				printf("视频流编号：%d\n", i);
			}
			if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO) {
				audioStream = i;
				printf("音频流编号：%d\n", i);
			}
		}

		if (videoStream == -1) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Din't find a video stream.\n");
			goto __FAIL;
		}
		if (audioStream == -1) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Din't find a audio stream.\n");
			goto __FAIL;
		}

		// 音频
		aCodecCtxtOrig = pFormatCtx->streams[audioStream]->codec;
		aCodec = avcodec_find_decoder(aCodecCtxtOrig->codec_id);

		if (!aCodec) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Unsupported codec.\n");
			goto __FAIL;
		}

		aCodecCtx = avcodec_alloc_context3(aCodec);
		if (avcodec_copy_context(aCodecCtx, aCodecCtxtOrig) != 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Couldn't copy codec context.\n");
			goto __FAIL;
		}

		// 设置音频参数
		wanted_spec.freq = aCodecCtx->sample_rate;
		wanted_spec.format = AUDIO_S16SYS;
		wanted_spec.channels = aCodecCtx->channels;
		wanted_spec.silence = 0;
		wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE;
		wanted_spec.callback = audio_callback;
		wanted_spec.userdata = aCodecCtx;

		if (SDL_OpenAudio(&wanted_spec, &spec) < 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to open audio device -%s.\n", SDL_GetError());
			goto __FAIL;
		}
		// 打开音频解码器
		avcodec_open2(aCodecCtx, aCodec, NULL);

		packet_queue_init(&audioq);

		in_channel_layout = av_get_default_channel_layout(aCodecCtx->channels);
		out_channel_layout = in_channel_layout;
		fprintf(stderr, "in layout:%lld, out layout:%lld \n", in_channel_layout, out_channel_layout);

		audio_convert_ctx = swr_alloc();

		if (audio_convert_ctx) {
			swr_alloc_set_opts(audio_convert_ctx,
				out_channel_layout,
				AV_SAMPLE_FMT_S16,
				aCodecCtx->sample_rate,
				in_channel_layout,
				aCodecCtx->sample_fmt,
				aCodecCtx->sample_rate,
				0,
				NULL
				);
		}

		swr_init(audio_convert_ctx);
		SDL_PauseAudio(0);



		/* 这行代码的目的是获取视频流的编解码上下文（`AVCodecContext`）。
		`pFormatCtx` 是输入格式上下文，`streams[videoStream]` 表示视频流，
		然后通过 `codec` 成员获取该流的编解码上下文。*/
		pCodecCtxOrig = pFormatCtx->streams[videoStream]->codec;

		// 找到视频流的解码器
		pCodec = avcodec_find_decoder(pCodecCtxOrig->codec_id);

		if (pCodec == NULL) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Unsupported codec.\n");
			goto __FAIL;
		}

		// 复制 上下文 为了不破坏原来的流
		pCodecCtx = avcodec_alloc_context3(pCodec);
		if (avcodec_copy_context(pCodecCtx, pCodecCtxOrig) != 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Couldn't copy codec context.\n");
			goto __FAIL;
		}

		// 打开解码器
		if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to open decoder!\n");
			goto __FAIL;
		}

		// 分配视频帧
		pFrame = av_frame_alloc();

		w_width = pCodecCtx->width;
		w_height = pCodecCtx->height;

		win = SDL_CreateWindow("Media Player",
			SDL_WINDOWPOS_UNDEFINED,
			SDL_WINDOWPOS_UNDEFINED,
			500, 500,
			SDL_WINDOW_OPENGL | SDL_WINDOW_RESIZABLE);

		if (!win) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION, "Failed to create window by SDL");
			goto __FAIL;
		}

		renderer = SDL_CreateRenderer(win, -1, 0);

		if (!renderer) {
			SDL_LogError(SDL_LOG_CATEGORY_APPLICATION , "Failed to create Renderer by SDL");
			goto __FAIL;
		}

		pixformat = SDL_PIXELFORMAT_IYUV;
		texture = SDL_CreateTexture(renderer, pixformat,
			SDL_TEXTUREACCESS_STREAMING, w_width, w_height);

		sws_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height,
			pCodecCtx->pix_fmt, pCodecCtx->width, pCodecCtx->height,
			AV_PIX_FMT_YUV420P, SWS_BILINEAR,
			NULL, NULL, NULL);

		pict = (AVPicture*)malloc(sizeof(AVPicture));
		avpicture_alloc(pict, AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height);

		while (av_read_frame(pFormatCtx, &packet) >= 0) {
			if (packet.stream_index == videoStream) {
				// 解码视频帧
				avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);
				// 得到解码后的视频帧
				if (frameFinished) {
					// 将图片转化为 SDL 使用的 YUV 格式
					sws_scale(sws_ctx, (uint8_t const* const*)pFrame->data,
						pFrame->linesize, 0, pCodecCtx->height, pict->data, pict->linesize);
					
					SDL_UpdateYUVTexture(texture, NULL,
						pict->data[0], pict->linesize[0],
						pict->data[1], pict->linesize[1],
						pict->data[2], pict->linesize[2]
						);

					rect.x = 0;
					rect.y = 0;
					rect.w = pCodecCtx->width;
					rect.h = pCodecCtx->height;

					SDL_RenderClear(renderer);
					SDL_RenderCopy(renderer, texture, NULL, &rect);
					SDL_RenderPresent(renderer);
					SDL_Delay(40);
					av_free_packet(&packet);
				}
			}
			else if (packet.stream_index == audioStream) {
				packet_queue_put(&audioq, &packet);
			}
			else {
				av_free_packet(&packet);
			}
			
			SDL_PollEvent(&event);
			switch (event.type) {
			case SDL_QUIT:
				quit = 1;
				goto __QUIT;
				break;
			default:
				SDL_Log("event type is %d", event.type);
				break;
			}
		}

	__QUIT:
		ret = 0;
	__FAIL:
		// Free the YUV frame
		if (pFrame) {
			av_frame_free(&pFrame);
		}

		// Close the codec
		if (pCodecCtx) {
			avcodec_close(pCodecCtx);
		}

		if (pCodecCtxOrig) {
			avcodec_close(pCodecCtxOrig);
		}

		// Close the video file
		if (pFormatCtx) {
			avformat_close_input(&pFormatCtx);
		}

		if (pict) {
			avpicture_free(pict);
			free(pict);
		}

		if (win) {
			SDL_DestroyWindow(win);
		}

		if (renderer) {
			SDL_DestroyRenderer(renderer);
		}

		if (texture) {
			SDL_DestroyTexture(texture);
		}

		SDL_Quit();
		return 0;
	}
};
```

![黄秋钊](/Users/huangqiuzhao/blog/source/_posts/音视频/黄秋钊.jpg)

#### 多线程与锁（音视频的同步）

实现音视频同步的本质就是多线程之间的同步。

- `SDL_CreateThread` 用于创建一个新的线程。它接受一个函数指针和一个指向函数参数的指针作为参数，然后创建一个新的线程来执行指定的函数，并将指定的参数传递给该函数。
- `SDL_WaitThread`函数用于等待一个指定线程的结束。它接受一个指向线程的指针作为参数，并在该线程结束后返回。
- `SDL_CteateMutex/SDL_DestroyMutex` `SDL_CreateMutex`函数用于创建一个互斥锁（mutex），用于线程间的同步。而`SDL_DestroyMutex`函数用于销毁互斥锁。
- `SDL_LockMutex/SDL_UnlockMutex` `SDL_LockMutex`函数用于尝试锁定互斥锁，如果互斥锁已经被锁定，则该函数会阻塞直到锁可用。`SDL_UnlockMutex`函数用于释放已经锁定的互斥锁。
- `SDL_CreateCond/SDL_DestroyCond` `SDL_CreateCond`函数用于创建条件变量，而`SDL_DestroyCond`函数用于销毁条件变量。条件变量通常与互斥锁一起使用，用于在线程之间等待某个条件的发生或通知其他线程。
- `SDL_CondWait/SDL_CondSignal` `SDL_CondWait`函数用于等待条件变量的信号。当线程调用该函数时，它会阻塞并等待条件变量的信号。在等待期间，该函数会释放传入的互斥锁，以允许其他线程修改共享数据。一旦条件变量被另一个线程发送信号，当前线程将重新获取互斥锁并继续执行。`SDL_CondSignal`函数用于向等待某个条件的线程发送信号，通知它们条件已经满足，可以继续执行。这个函数通常与`SDL_CondWait`配合使用，用于唤醒一个等待该条件的线程。

#### 播放器线程模型

![截屏2024-05-11 16.50.12](/Users/huangqiuzhao/blog/source/_posts/音视频/截屏2024-05-11 16.50.12.png)

一共有四个线程：

1. 主线程，主要用于一些参数的检查，事件的处理，视频的渲染，主线程会定时从视频解码队列中取出视频帧。
2. 第二个线程，解复用线程，在这个线程中会创建一个视频解码线程，紧接着对多媒体文件进行解复用，将视频包存放在视频流队列，将音频包存放在音频流队列中
3.  第三个线程是视频解码线程，首先去视频流队列中取出一个个视频包，进行解码，解码后的视频帧，存放在解码视频队列。
4. 最后一个线程是`SDL` 在打开音频设备时创建的，在这个线程中，它会调用我们的回调函数，最终时从音频队列中取出一个个音频包，进行解码，解码后就交给声卡。

> 为什么要进行重采样？
>
> ```c++
> struct SwrContext *audio_swr_ctx;
> ```
>
> 这是因为我们的音频设备的音频参数是固定的（采样率、通道数、采样大小等等）。这些都是我们已经固定的。也就是我们一旦初始化音频设备以后，这些参数就不能改变了，而在多媒体文件中，他就存在各种各样的音频格式，比如采样大小，有的是32位的，有的是16位的，有的是浮点的，有的是非浮点的。最终放到我们的音频设备中，统一的播放出来，我们应该对音频进行重采样。将我们见到的所有的音频格式统一到一个格式中，也就是我们打开音频设备时定义的。



#### 音视频同步

**时间戳**

- `PTS (Presentation timestamp)` ：表示在解码视频时应该在特定时间显示帧的时间戳。PTS告诉解码器何时将帧呈现给用户，以确保视频按照正确的顺序和时间显示。PTS通常以时间基为单位（例如，以毫秒为单位）进行表示。

- `DTS (Decoding timestamp)`：指解码器开始解码帧的时间戳。它指示视频帧何时应该开始解码，以确保在正确的时间呈现帧。DTS通常以时间基为单位（例如，以毫秒为单位）进行表示。 DTS和PTS之间的差异可以表示解码器需要多长时间来解码视频帧。

- `I (intra) / B (bidirectional) / P (predicted) 帧` ：视频编码中常见的帧类型。它们用于表示视频序列中的不同类型的帧。

  1. I帧（Intra Frame）：I帧是视频序列中的关键帧或帧间隔。每个I帧都是独立的，不依赖于其他帧。它包含完整的图像信息，可以作为其他帧的参考点。通常，视频序列的开始和切换点（例如场景变化）会包含I帧。

  2. P帧（Predicted Frame）：P帧是通过对前向参考帧（通常是前一个I帧或P帧）进行运动补偿来编码的。它只包含自身与前一帧之间的差异信息。P帧依赖于之前的帧进行解码，并且可以用来预测未来帧的内容。

  3. B帧（Bidirectional Frame）：B帧是通过对前后两个参考帧（通常是前一个和后一个I帧或P帧）进行运动补偿来编码的。它包含自身与前后两个参考帧之间的差异信息。B帧通常具有最高的压缩率，因为它可以利用未来和过去帧之间的关联来减少信息冗余。

  这些帧类型通常在视频编码标准（如H.264 / AVC或H.265 / HEVC）中使用，并且在压缩视频时起着重要作用。

**时间戳顺序**

实际帧顺序： `I B B P` 

存放帧顺序： `I P B B` 

解码时间戳： `1 4 2 3` `DTS`

展示时间戳： `1 2 3 4` `PTS`

**从哪里获得PTS**

- `AVPacket PTS`
- `AVFrame PTS`
- `av_frame_get_best_effort_timestamp()` 当 `PTS` 无效时，我们可以调用推算出合适的 `PTS`

**时间基**

- `tbr` : 帧率 指在视频中每秒显示的帧数。帧速率决定了视频的流畅度，更高的帧速率通常会产生更加流畅的视频效果。通常，帧速率以每秒帧数（fps）的形式表示。
- `tbn (time base of stream)`：指流的时间基准（time base of stream），它表示流中时间单位的基准。在视频处理中，时间单位通常以分数形式表示，例如1/1000秒。这个时间基准用于确定时间戳的单位，以及在视频流中的时间度量。
- `tbc (time base of codec)`：编解码器的时间基准（time base of codec），它是指编解码器内部使用的时间单位的基准。与流的时间基准不同，编解码器的时间基准可能与流的时间基准不同，尤其是在处理不同类型的视频或音频流时。编解码器的时间基准用于确定编码器内部时间单位的度量，通常以分数形式表示，例如1/1000秒。

**计算当前帧的 PTS**

```c++
PTS = PTS * av_q2d(video_stream->time_base);
av_q2d(AVRotional a) {
  return a.num / (double)a.den;
}
```

**计算下一帧的 PTS**

`video_clock`：预测的下一帧视频的 `PTS` 上一帧的 `PTS` 加上 `frame_delay`

`frame_delay`：`1 / tbr`

`audio_clock`：音频当前播放的时间戳

**音视频同步方式**

1. 视频同步到音频：
2. 音频同步到视频：

3. 音频和视频都同步到系统时钟：

**视频播放的基本思路**

一般的做法，展示第一帧视频帧后，获得要显示的下一个视频帧的 `PTS` ，然后设置一个定时器，当定时器超时后，刷新新的视频帧，如此反复操作。

#### **player**

```c++
int packet_queue_put(PacketQueue *q, AVPacket *pkt) {

  AVPacketList *pkt1;
  if(av_dup_packet(pkt) < 0) {
    return -1;
  }
  pkt1 = av_malloc(sizeof(AVPacketList));
  if (!pkt1)
    return -1;
  pkt1->pkt = *pkt;
  pkt1->next = NULL;
  
  SDL_LockMutex(q->mutex);

  if (!q->last_pkt)
    q->first_pkt = pkt1;
  else
    q->last_pkt->next = pkt1;
  q->last_pkt = pkt1;
  q->nb_packets++;
  q->size += pkt1->pkt.size;
  SDL_CondSignal(q->cond);
  
  SDL_UnlockMutex(q->mutex);
  return 0;
}
```

> `packet_queue_put` 函数用于将一个数据包放入 `PacketQueue` 队列中。以下是该函数的详细解释：
>
> ### 函数原型
> ```c
> int packet_queue_put(PacketQueue *q, AVPacket *pkt);
> ```
>
> ### 参数
> - `PacketQueue *q`: 指向需要添加数据包的队列的指针。
> - `AVPacket *pkt`: 需要添加到队列中的数据包指针。
>
> ### 函数流程
> 1. **复制数据包**：
>    
>    ```c
>    if(av_dup_packet(pkt) < 0) {
>      return -1;
>    }
>    ```
> 使用 `av_dup_packet` 函数复制一个数据包。如果复制失败，返回 -1。
>    
> 2. **分配新节点**：
>    ```c
>    AVPacketList *pkt1;
>    pkt1 = av_malloc(sizeof(AVPacketList));
>    if (!pkt1)
>      return -1;
>    ```
>    为新的数据包节点分配内存。如果分配失败，返回 -1。
>
> 3. **初始化新节点**：
>    ```c
>    pkt1->pkt = *pkt;
>    pkt1->next = NULL;
>    ```
>    将传入的数据包内容复制到新分配的节点中，并将节点的 `next` 指针设为 `NULL`，表示这是队列的末尾节点。
>
> 4. **锁定互斥锁**：
>    ```c
>    SDL_LockMutex(q->mutex);
>    ```
>
> 5. **将节点添加到队列**：
>    ```c
>    if (!q->last_pkt)
>      q->first_pkt = pkt1;
>    else
>      q->last_pkt->next = pkt1;
>    q->last_pkt = pkt1;
>    q->nb_packets++;
>    q->size += pkt1->pkt.size;
>    ```
>    - 如果队列为空（`last_pkt` 为 `NULL`），将 `first_pkt` 指向新节点。
>    - 否则，将当前队列的最后一个节点的 `next` 指针指向新节点。
>    - 更新 `last_pkt` 指向新节点。
>    - 增加队列中数据包的计数 `nb_packets`。
>    - 增加队列的大小 `size`。
>
> 6. **发出条件信号**：
>    ```c
>    SDL_CondSignal(q->cond);
>    ```
>    通知等待该条件的线程，队列中有新的数据包可用。
>
> 7. **解锁互斥锁**：
>    ```c
>    SDL_UnlockMutex(q->mutex);
>    ```
>
> 8. **返回成功**：
>    ```c
>    return 0;
>    ```
>
> ### 作用
> `packet_queue_put` 函数将一个数据包添加到 `PacketQueue` 队列中，同时使用互斥锁和条件变量确保线程安全和同步。该函数在多线程音视频处理应用中非常重要，确保多个线程可以安全地访问和修改数据包队列。

```c
int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block)
{
  AVPacketList *pkt1;
  int ret;

  SDL_LockMutex(q->mutex);
  
  for(;;) {
    
    if(global_video_state->quit) {
      ret = -1;
      break;
    }

    pkt1 = q->first_pkt;
    if (pkt1) {
      q->first_pkt = pkt1->next;
      if (!q->first_pkt)
				q->last_pkt = NULL;
      q->nb_packets--;
      q->size -= pkt1->pkt.size;
      *pkt = pkt1->pkt;
      av_free(pkt1);
      ret = 1;
      break;
    } else if (!block) {
      ret = 0;
      break;
    } else {
      SDL_CondWait(q->cond, q->mutex);
    }
  }
  SDL_UnlockMutex(q->mutex);
  return ret;
}
```

> `packet_queue_get` 函数用于从 `PacketQueue` 队列中获取一个数据包。以下是该函数的详细解释：
>
> ### 函数原型
> ```c
> int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block);
> ```
>
> ### 参数
> - `PacketQueue *q`: 指向要从中获取数据包的队列的指针。
> - `AVPacket *pkt`: 指向接收数据包的指针。
> - `int block`: 指示函数是否应该阻塞等待数据包的标志。非零值表示阻塞，零值表示非阻塞。
>
> ### 函数流程
> 1. **定义局部变量**：
>    ```c
>    AVPacketList *pkt1;
>    int ret;
>    ```
>
> 2. **锁定互斥锁**：
>    ```c
>    SDL_LockMutex(q->mutex);
>    ```
>
> 3. **进入循环**：
>    ```c
>    for(;;) {
>    ```
>
> 4. **检查全局退出标志**：
>    ```c
>    if(global_video_state->quit) {
>      ret = -1;
>      break;
>    }
>    ```
>    如果全局状态 `global_video_state` 的 `quit` 标志被设置，则退出循环并返回 -1。
>
> 5. **获取队列中的第一个数据包**：
>    ```c
>    pkt1 = q->first_pkt;
>    if (pkt1) {
>    ```
>    如果队列不为空，获取队列的第一个数据包。
>
> 6. **更新队列状态**：
>    ```c
>    q->first_pkt = pkt1->next;
>    if (!q->first_pkt)
>      q->last_pkt = NULL;
>    q->nb_packets--;
>    q->size -= pkt1->pkt.size;
>    *pkt = pkt1->pkt;
>    av_free(pkt1);
>    ret = 1;
>    break;
>    ```
>    - 将队列的 `first_pkt` 指针更新为下一个节点。
>    - 如果队列现在为空（`first_pkt` 为 `NULL`），更新 `last_pkt` 为 `NULL`。
>    - 更新队列中数据包的计数 `nb_packets` 和总大小 `size`。
>    - 将获取的数据包内容复制到参数 `pkt` 指向的结构体中。
>    - 释放节点 `pkt1` 的内存。
>    - 设置返回值为 1 表示成功获取数据包，并退出循环。
>
> 7. **处理队列为空的情况**：
>    ```c
>    } else if (!block) {
>      ret = 0;
>      break;
>    } else {
>      SDL_CondWait(q->cond, q->mutex);
>    }
>    ```
>    - 如果队列为空且 `block` 标志为 0（非阻塞），设置返回值为 0 并退出循环。
>    - 如果 `block` 标志为非零（阻塞），等待条件变量 `q->cond`，在有新数据包添加到队列时被唤醒。
>
> 8. **解锁互斥锁**：
>    ```c
>    SDL_UnlockMutex(q->mutex);
>    ```
>
> 9. **返回结果**：
>    ```c
>    return ret;
>    ```
>
> ### 作用
> `packet_queue_get` 函数用于从 `PacketQueue` 队列中取出一个数据包。它使用互斥锁来确保线程安全，并根据 `block` 参数决定是否阻塞等待新数据包。该函数在多线程音视频处理应用中非常重要，确保多个线程可以安全地从数据包队列中获取数据包。

```c
double get_audio_clock(VideoState* is) {
	double pts;
	int hw_buf_size, bytes_per_sec, n;

	pts = is->audio_clock;
	hw_buf_size = is->audio_buf_size - is->audio_buf_index;
	bytes_per_sec = 0;
	n = is->audio_ctx->channels * 2;
	if (is->audio_st) {
		bytes_per_sec = is->audio_ctx->sample_rate * n;
	}
	if (bytes_per_sec) {
		pts -= (double)hw_buf_size / bytes_per_sec;
	}
	return pts;
}
```

> `get_audio_clock` 函数用于计算和返回当前音频时钟的值。音频时钟是音视频同步中的一个关键因素，用于确保音频和视频的播放保持一致。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> double get_audio_clock(VideoState* is);
> ```
>
> ### 参数
> - `VideoState* is`: 指向管理视频播放状态的结构体 `VideoState` 的指针。
>
> ### 函数流程
> 1. **定义局部变量**：
>    ```c
>    double pts;
>    int hw_buf_size, bytes_per_sec, n;
>    ```
>
> 2. **获取当前音频时钟**：
>    ```c
>    pts = is->audio_clock;
>    ```
>    将当前的音频时钟值赋给 `pts` 变量。`is->audio_clock` 存储的是解码器更新的音频时钟。
>
> 3. **计算音频硬件缓冲区的大小**：
>    ```c
>    hw_buf_size = is->audio_buf_size - is->audio_buf_index;
>    ```
>    `hw_buf_size` 计算的是音频硬件缓冲区中尚未播放的音频数据量。`is->audio_buf_size` 是音频缓冲区的总大小，`is->audio_buf_index` 是当前缓冲区中已播放的位置。
>
> 4. **初始化每秒字节数**：
>    ```c
>    bytes_per_sec = 0;
>    ```
>    初始化 `bytes_per_sec` 为 0。
>
> 5. **计算每个音频样本的字节数**：
>    ```c
>    n = is->audio_ctx->channels * 2;
>    ```
>    每个音频样本的字节数 `n` 由音频通道数乘以每个样本的字节数（假设每个样本是16位，即2字节）。
>
> 6. **计算每秒的字节数**：
>    ```c
>    if (is->audio_st) {
>        bytes_per_sec = is->audio_ctx->sample_rate * n;
>    }
>    ```
>    如果音频流存在（`is->audio_st` 非空），则计算每秒的字节数 `bytes_per_sec`。这是由音频采样率乘以每个音频样本的字节数得到的。
>
> 7. **调整音频时钟**：
>    ```c
>    if (bytes_per_sec) {
>        pts -= (double)hw_buf_size / bytes_per_sec;
>    }
>    ```
>    如果 `bytes_per_sec` 非零，则调整音频时钟 `pts`。调整量是未播放的音频数据量（`hw_buf_size`）除以每秒的字节数（`bytes_per_sec`），这样可以得到未播放音频数据对应的时间，并从当前音频时钟中减去这个时间。
>
> 8. **返回调整后的音频时钟**：
>    ```c
>    return pts;
>    ```
>
> ### 作用
> `get_audio_clock` 函数用于返回当前准确的音频时钟值，通过考虑音频硬件缓冲区中尚未播放的数据量来调整音频时钟。这样可以确保在播放过程中音频和视频的同步。这在多媒体应用中非常重要，尤其是需要保持音视频同步的场景下。

```c
int audio_decode_frame(VideoState* is, uint8_t* audio_buf, int buf_size, double* pts_ptr) {
	int len1, data_size = 0;
	AVPacket* pkt = &is->audio_pkt;
	double pts;
	int n;

	for (;;) {
		while (is->audio_pkt_size > 0) {
			int got_frame = 0;
			len1 = avcodec_decode_audio4(is->audio_ctx, &is->audio_frame, &got_frame, pkt);
			if (len1 < 0) {
				is->audio_pkt_size = 0;
				break;
			}
			data_size = 0;

			if (got_frame) {
				data_size = 2 * is->audio_frame.nb_samples * 2;
				assert(data_size <= buf_size);
				swr_convert(is->audio_swr_ctx,
					&audio_buf,
					MAX_AUDIO_FRAME_SIZE * 3 / 2,
					(const uint8_t **)is->audio_frame.data,
					is->audio_frame.nb_samples);
			}
			is->audio_pkt_data += len1;
			is->audio_pkt_size -= len1;
			if (data_size <= 0) {
				continue;
			}

			pts = is->audio_clock;
			*pts_ptr = pts;
			n = 2 * is->audio_ctx->channels;
			is->audio_clock += (double)data_size / (double)(n * is->audio_ctx->sample_rate);
			return data_size;
		}
		if (pkt->data) {
			av_free_packet(pkt);
		}
		if (is->quit) {
			return -1;
		}

		is->audio_pkt_data = pkt->data;
		is->audio_pkt_size = pkt->size;

		if (pkt->pts != AV_NOPTS_VALUE) {
			is->audio_clock = av_q2d(is->audio_st->time_base) * pkt->pts;
		}
	}
}
```

> `audio_decode_frame` 函数用于从 `VideoState` 中解码音频数据，并将解码后的音频数据存储在 `audio_buf` 中，同时返回解码的音频帧大小，并更新音频时钟。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> int audio_decode_frame(VideoState* is, uint8_t* audio_buf, int buf_size, double* pts_ptr);
> ```
>
> ### 参数
> - `VideoState* is`: 指向管理视频播放状态的结构体 `VideoState` 的指针。
> - `uint8_t* audio_buf`: 指向存储解码音频数据的缓冲区。
> - `int buf_size`: 缓冲区 `audio_buf` 的大小。
> - `double* pts_ptr`: 指向存储音频帧时间戳的指针。
>
> ### 函数流程
> 1. **定义局部变量**：
>    ```c
>    int len1, data_size = 0;
>    AVPacket* pkt = &is->audio_pkt;
>    double pts;
>    int n;
>    ```
>
> 2. **进入循环**：
>    ```c
>    for (;;) {
>    ```
>
> 3. **处理音频数据包**：
>    ```c
>    while (is->audio_pkt_size > 0) {
>    ```
>    当音频数据包中仍有未解码数据时，继续解码。
>
> 4. **解码音频帧**：
>    ```c
>    int got_frame = 0;
>    len1 = avcodec_decode_audio4(is->audio_ctx, &is->audio_frame, &got_frame, pkt);
>    if (len1 < 0) {
>        is->audio_pkt_size = 0;
>        break;
>    }
>    data_size = 0;
>    ```
>    - 调用 `avcodec_decode_audio4` 解码音频数据包。如果解码失败，设置 `audio_pkt_size` 为 0 并退出循环。
>    - 如果解码成功，`len1` 是消耗的字节数，`got_frame` 表示是否成功解码出一个完整的音频帧。
>
> 5. **处理解码后的音频帧**：
>    ```c
>    if (got_frame) {
>        data_size = 2 * is->audio_frame.nb_samples * 2;
>        assert(data_size <= buf_size);
>        swr_convert(is->audio_swr_ctx,
>            &audio_buf,
>            MAX_AUDIO_FRAME_SIZE * 3 / 2,
>            (const uint8_t **)is->audio_frame.data,
>            is->audio_frame.nb_samples);
>    }
>    ```
>    - 如果成功解码出一个音频帧，计算 `data_size`。
>    - 使用 `swr_convert` 函数将音频帧数据转换并存储到 `audio_buf` 中。
>
> 6. **更新数据包指针**：
>    ```c
>    is->audio_pkt_data += len1;
>    is->audio_pkt_size -= len1;
>    if (data_size <= 0) {
>        continue;
>    }
>    ```
>
> 7. **更新音频时钟**：
>    ```c
>    pts = is->audio_clock;
>    *pts_ptr = pts;
>    n = 2 * is->audio_ctx->channels;
>    is->audio_clock += (double)data_size / (double)(n * is->audio_ctx->sample_rate);
>    return data_size;
>    ```
>
> 8. **释放数据包**：
>    ```c
>    if (pkt->data) {
>        av_free_packet(pkt);
>    }
>    if (is->quit) {
>        return -1;
>    }
>    ```
>
> 9. **读取下一个数据包**：
>    ```c
>    is->audio_pkt_data = pkt->data;
>    is->audio_pkt_size = pkt->size;
>    if (pkt->pts != AV_NOPTS_VALUE) {
>        is->audio_clock = av_q2d(is->audio_st->time_base) * pkt->pts;
>    }
>    ```
>
> ### 作用
> `audio_decode_frame` 函数用于从 `VideoState` 的音频包队列中解码音频数据并将其存储到 `audio_buf` 中。该函数在解码过程中更新音频时钟，并返回解码后的音频帧大小。该函数确保音频数据的解码和处理，包括对音频时钟的更新，这对于音视频同步非常重要。

```c
void audio_callback(void *userdata, Uint8 *stream, int len) {
	VideoState* is = (VideoState*)userdata;
	int len1, audio_size;
	double pts;

	SDL_memset(stream, 0, len);

	while (len > 0) {
		if (is->audio_buf_index >= is->audio_buf_size) {
			audio_size = audio_decode_frame(is, is->audio_buf, sizeof(is->audio_buf), &pts);
			if (audio_size < 0) {
				is->audio_buf_size = 1024 * 2 * 2;
				memset(is->audio_buf, 0, is->audio_buf_size);
			}
			else {
				is->audio_buf_size = audio_size;
			}
			is->audio_buf_index = 0;
		}
		len1 = is->audio_buf_size - is->audio_buf_index;
		if (len1 > len) {
			len1 = len;
		}
		SDL_MixAudio(stream, (uint8_t*)is->audio_buf + is->audio_buf_index, len1, SDL_MIX_MAXVOLUME);
		len -= len1;
		stream += len1;
		is->audio_buf_index += len1;
	}
}
```

> `audio_callback` 函数是音频回调函数，通常在多媒体播放中使用，用于将解码后的音频数据填充到音频输出缓冲区中。在使用 SDL 库进行音频播放时，会指定这个回调函数来处理音频数据。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> void audio_callback(void *userdata, Uint8 *stream, int len);
> ```
>
> ### 参数
> - `void *userdata`: 指向用户数据的指针，在此上下文中，它是一个 `VideoState` 结构体的指针。
> - `Uint8 *stream`: 指向 SDL 音频输出缓冲区的指针，音频数据将被写入这个缓冲区。
> - `int len`: 要填充的音频数据的长度，以字节为单位。
>
> ### 函数流程
>
> 1. **获取 `VideoState` 结构体的指针**：
>    ```c
>    VideoState* is = (VideoState*)userdata;
>    ```
>
> 2. **定义局部变量**：
>    ```c
>    int len1, audio_size;
>    double pts;
>    ```
>
> 3. **清空输出缓冲区**：
>    ```c
>    SDL_memset(stream, 0, len);
>    ```
>    使用 `SDL_memset` 函数将输出缓冲区 `stream` 清零，确保缓冲区内没有旧的音频数据。
>
> 4. **循环填充音频数据**：
>    ```c
>    while (len > 0) {
>    ```
>
> 5. **检查并解码新的音频帧**：
>    ```c
>    if (is->audio_buf_index >= is->audio_buf_size) {
>        audio_size = audio_decode_frame(is, is->audio_buf, sizeof(is->audio_buf), &pts);
>        if (audio_size < 0) {
>            is->audio_buf_size = 1024 * 2 * 2;
>            memset(is->audio_buf, 0, is->audio_buf_size);
>        } else {
>            is->audio_buf_size = audio_size;
>        }
>        is->audio_buf_index = 0;
>    }
>    ```
>    - 如果音频缓冲区中没有未播放的数据（`audio_buf_index` 超过 `audio_buf_size`），则调用 `audio_decode_frame` 函数解码新的音频帧。
>    - 如果解码失败，设定一个默认缓冲区大小并清零缓冲区。
>    - 如果解码成功，更新缓冲区大小。
>    - 重置 `audio_buf_index` 为 0。
>
> 6. **计算需要拷贝的数据长度**：
>    ```c
>    len1 = is->audio_buf_size - is->audio_buf_index;
>    if (len1 > len) {
>        len1 = len;
>    }
>    ```
>    - 计算当前缓冲区中未播放的数据长度 `len1`。
>    - 如果 `len1` 大于剩余要填充的长度 `len`，则只填充 `len` 的长度。
>
> 7. **将音频数据拷贝到输出缓冲区**：
>    ```c
>    SDL_MixAudio(stream, (uint8_t*)is->audio_buf + is->audio_buf_index, len1, SDL_MIX_MAXVOLUME);
>    ```
>    使用 `SDL_MixAudio` 函数将音频数据从 `audio_buf` 拷贝到 `stream`，并混音。
>
> 8. **更新剩余长度和指针**：
>    ```c
>    len -= len1;
>    stream += len1;
>    is->audio_buf_index += len1;
>    ```
>
> ### 作用
> `audio_callback` 函数在音频设备需要更多数据时被调用。它从 `VideoState` 结构体中获取音频数据，将其解码并填充到 SDL 提供的音频缓冲区中。这保证了音频流的连续性和正确性，确保音频播放平滑且无间断。

```c
static Uint32 sdl_refresh_timer_cb(Uint32 interval, void* opaque) {
	SDL_Event event;
	event.type = FF_REFRESH_EVENT;
	event.user.data1 = opaque;
	SDL_PushEvent(&event);
	return 0;
}
```

> 这个函数 `sdl_refresh_timer_cb` 是一个 SDL 定时器回调函数，用于向 SDL 事件队列中推送一个自定义事件，从而触发后续的事件处理。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> static Uint32 sdl_refresh_timer_cb(Uint32 interval, void* opaque);
> ```
>
> ### 参数
> - `Uint32 interval`: 定时器的间隔时间，单位是毫秒。
> - `void* opaque`: 一个指向用户数据的指针，可以是任何类型的数据。在这个上下文中，它通常用于传递状态信息或上下文。
>
> ### 函数流程
>
> 1. **定义一个 SDL 事件**：
>
>    ```c
>    SDL_Event event;
>    ```
> ```
> 
> 2. **设置事件类型**：
>    ```c
>    event.type = FF_REFRESH_EVENT;
> ```
>    - `FF_REFRESH_EVENT` 是一个自定义的 SDL 事件类型，通常是通过 `#define` 语句定义的。
>    - 例如：
>      ```c
>      #define FF_REFRESH_EVENT (SDL_USEREVENT + 1)
>      ```
>
> 3. **设置事件数据**：
>
>    ```c
>    event.user.data1 = opaque;
>    ```
>    - `event.user.data1` 是一个 `void*` 类型的数据指针，可以存储传入的用户数据 `opaque`。
> - 这允许在事件处理函数中访问这个用户数据。
>
> 4. **将事件推送到 SDL 事件队列**：
>    ```c
>    SDL_PushEvent(&event);
>    ```
>    - 使用 `SDL_PushEvent` 函数将定义好的事件推送到 SDL 事件队列中。
>    - 这会触发事件处理机制，从而在事件循环中处理这个自定义事件。
>
> 5. **返回值**：
>    ```c
>    return 0;
>    ```
>    - 定时器回调函数返回 0 表示一次性定时器，即不再重复触发。如果希望定时器重复触发，可以返回 `interval` 的值。
>
> ### 作用
> `sdl_refresh_timer_cb` 函数的主要作用是创建并推送一个自定义的刷新事件到 SDL 事件队列中，以触发后续的图像刷新或其他需要处理的任务。这个机制常用于视频播放或其他需要定时刷新界面的应用中。
>
> ### 使用场景
> 假设在视频播放应用中，我们希望定期刷新视频帧。我们可以设置一个 SDL 定时器，当定时器超时时，调用 `sdl_refresh_timer_cb` 推送一个刷新事件，然后在事件循环中处理这个事件，更新视频显示。这样可以确保视频播放的平滑性和同步性。
>
> ### 示例
> 以下是一个可能的使用示例：
>
> ```c
> // 定义自定义事件类型
> #define FF_REFRESH_EVENT (SDL_USEREVENT + 1)
> 
> // 设置定时器
> SDL_AddTimer(40, sdl_refresh_timer_cb, NULL);
> 
> // 在事件循环中处理刷新事件
> SDL_Event event;
> while (SDL_WaitEvent(&event)) {
>     if (event.type == FF_REFRESH_EVENT) {
>         // 调用刷新函数，例如刷新视频帧
>         refresh_video_frame();
>     } else if (event.type == SDL_QUIT) {
>         break;
>     }
> }
> ```
>
> 在这个示例中，每 40 毫秒触发一次 `sdl_refresh_timer_cb`，该函数向事件队列推送 `FF_REFRESH_EVENT` 事件，从而定期刷新视频帧。

```c
void video_display(VideoState *is) {

  SDL_Rect rect;
  VideoPicture *vp;
  float aspect_ratio;
  int w, h, x, y;
  int i;

  vp = &is->pictq[is->pictq_rindex];
  if(vp->bmp) {

    SDL_UpdateYUVTexture( texture, NULL,
                          vp->bmp->data[0], vp->bmp->linesize[0],
                          vp->bmp->data[1], vp->bmp->linesize[1],
                          vp->bmp->data[2], vp->bmp->linesize[2]);

    rect.x = 0;
    rect.y = 0;
    rect.w = is->video_ctx->width;
    rect.h = is->video_ctx->height;
    SDL_LockMutex(text_mutex);
    SDL_RenderClear( renderer );
    SDL_RenderCopy( renderer, texture, NULL, &rect);
    SDL_RenderPresent( renderer );
    SDL_UnlockMutex(text_mutex);

  }
}
```

> `video_display` 函数用于在 SDL 窗口中显示视频帧。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> void video_display(VideoState *is);
> ```
>
> ### 参数
> - `VideoState *is`: 指向视频状态的指针，包含了视频播放所需的所有信息和状态。
>
> ### 函数流程
>
> 1. **定义局部变量**：
>    ```c
>    SDL_Rect rect;
>    VideoPicture *vp;
>    float aspect_ratio;
>    int w, h, x, y;
>    int i;
>    ```
>
> 2. **获取当前显示的图片帧**：
>    ```c
>    vp = &is->pictq[is->pictq_rindex];
>    ```
>    - 从 `VideoState` 的 `pictq` 队列中获取当前要显示的视频帧。
>    - `is->pictq_rindex` 是当前要显示的图片帧在队列中的索引。
>
> 3. **检查视频帧是否有数据**：
>    ```c
>    if(vp->bmp) {
>    ```
>    - `vp->bmp` 是一个指向 `AVFrame` 的指针，包含了 YUV 格式的视频数据。
>    - 如果 `vp->bmp` 不为空，说明有视频帧需要显示。
>
> 4. **更新 YUV 纹理**：
>    ```c
>    SDL_UpdateYUVTexture(texture, NULL,
>                         vp->bmp->data[0], vp->bmp->linesize[0],
>                         vp->bmp->data[1], vp->bmp->linesize[1],
>                         vp->bmp->data[2], vp->bmp->linesize[2]);
>    ```
>    - 使用 `SDL_UpdateYUVTexture` 函数将 `AVFrame` 中的 YUV 数据更新到 SDL 纹理 `texture` 中。
>
> 5. **设置显示矩形**：
>    ```c
>    rect.x = 0;
>    rect.y = 0;
>    rect.w = is->video_ctx->width;
>    rect.h = is->video_ctx->height;
>    ```
>    - 设置显示区域 `rect`，使其大小与视频的宽高一致。
>
> 6. **绘制视频帧**：
>    ```c
>    SDL_LockMutex(text_mutex);
>    SDL_RenderClear(renderer);
>    SDL_RenderCopy(renderer, texture, NULL, &rect);
>    SDL_RenderPresent(renderer);
>    SDL_UnlockMutex(text_mutex);
>    ```
>    - 锁定互斥锁 `text_mutex`，以确保线程安全。
>    - 清除渲染器 `renderer`。
>    - 使用 `SDL_RenderCopy` 函数将更新后的纹理 `texture` 复制到渲染器 `renderer`，并按 `rect` 定义的位置和大小显示。
>    - 使用 `SDL_RenderPresent` 函数将渲染器内容显示到窗口中。
>    - 解锁互斥锁 `text_mutex`。
>
> ### 作用
> `video_display` 函数的主要作用是将解码后的视频帧显示在 SDL 窗口中。通过从 `VideoState` 的图片队列中获取当前帧，然后使用 SDL 提供的函数将 YUV 数据更新到纹理中，再通过渲染器显示到窗口上，从而实现视频播放功能。
>
> ### 使用场景
> `video_display` 函数通常在视频播放应用中使用，它被定期调用以刷新视频显示。可以在事件循环中或定时器回调中调用这个函数，以确保视频帧按正确的时间间隔显示。
>
> ### 示例
> 在实际使用中，`video_display` 函数可能在一个事件处理或定时器回调中调用，例如：
>
> ```c
> void on_video_refresh(VideoState* is) {
>     video_display(is);
> }
> 
> // 在事件循环中处理自定义刷新事件
> SDL_Event event;
> while (SDL_WaitEvent(&event)) {
>     if (event.type == FF_REFRESH_EVENT) {
>         on_video_refresh((VideoState*)event.user.data1);
>     } else if (event.type == SDL_QUIT) {
>         break;
>     }
> }
> ```
>
> 通过这种方式，可以实现视频帧的定时刷新和显示，确保视频播放的平滑和同步。

```c
void video_refresh_timer(void *userdata) {

  VideoState *is = (VideoState *)userdata;
  VideoPicture *vp;
  double actual_delay, delay, sync_threshold, ref_clock, diff;
  
  if(is->video_st) {
    if(is->pictq_size == 0) {
      schedule_refresh(is, 1);
    } else {
      vp = &is->pictq[is->pictq_rindex];

      delay = vp->pts - is->frame_last_pts; /* the pts from last time */
      if(delay <= 0 || delay >= 1.0) {
	/* if incorrect delay, use previous one */
	delay = is->frame_last_delay;
      }
      /* save for next time */
      is->frame_last_delay = delay;
      is->frame_last_pts = vp->pts;

      /* update delay to sync to audio */
      ref_clock = get_audio_clock(is);
      diff = vp->pts - ref_clock;

      /* Skip or repeat the frame. Take delay into account
	 FFPlay still doesn't "know if this is the best guess." */
      sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;
      if(fabs(diff) < AV_NOSYNC_THRESHOLD) {
	if(diff <= -sync_threshold) {
	  delay = 0;
	} else if(diff >= sync_threshold) {
	  delay = 2 * delay;
	}
      }
      is->frame_timer += delay;
      /* computer the REAL delay */
      actual_delay = is->frame_timer - (av_gettime() / 1000000.0);
      if(actual_delay < 0.010) {
	/* Really it should skip the picture instead */
	actual_delay = 0.010;
      }
      schedule_refresh(is, (int)(actual_delay * 1000 + 0.5));
      
      /* show the picture! */
      video_display(is);
      
      /* update queue for next picture! */
      if(++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {
	is->pictq_rindex = 0;
      }
      SDL_LockMutex(is->pictq_mutex);
      is->pictq_size--;
      SDL_CondSignal(is->pictq_cond);
      SDL_UnlockMutex(is->pictq_mutex);
    }
  } else {
    schedule_refresh(is, 100);
  }
}
```

> `video_refresh_timer` 函数负责定期刷新视频帧，并确保视频帧与音频同步显示。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> void video_refresh_timer(void *userdata);
> ```
>
> ### 参数
> - `void *userdata`: 指向 `VideoState` 结构的指针，包含了视频播放所需的所有信息和状态。
>
> ### 函数流程
>
> 1. **定义局部变量**：
>    ```c
>    VideoState *is = (VideoState *)userdata;
>    VideoPicture *vp;
>    double actual_delay, delay, sync_threshold, ref_clock, diff;
>    ```
>
> 2. **检查是否有视频流**：
>    ```c
>    if(is->video_st) {
>    ```
>    - 判断 `is` 中是否有视频流。
>
> 3. **检查图片队列是否为空**：
>    ```c
>    if(is->pictq_size == 0) {
>      schedule_refresh(is, 1);
>    } else {
>    ```
>    - 如果图片队列为空，则调度下次刷新（1 毫秒后），并返回。
>
> 4. **获取当前要显示的视频帧**：
>    ```c
>    vp = &is->pictq[is->pictq_rindex];
>    ```
>
> 5. **计算当前帧的显示延迟**：
>    ```c
>    delay = vp->pts - is->frame_last_pts; /* the pts from last time */
>    if(delay <= 0 || delay >= 1.0) {
>      delay = is->frame_last_delay;
>    }
>    is->frame_last_delay = delay;
>    is->frame_last_pts = vp->pts;
>    ```
>
> 6. **同步音频和视频**：
>    ```c
>    ref_clock = get_audio_clock(is);
>    diff = vp->pts - ref_clock;
>    
>    sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;
>    if(fabs(diff) < AV_NOSYNC_THRESHOLD) {
>      if(diff <= -sync_threshold) {
>        delay = 0;
>      } else if(diff >= sync_threshold) {
>        delay = 2 * delay;
>      }
>    }
>    is->frame_timer += delay;
>    ```
>
> 7. **计算实际延迟**：
>    ```c
>    actual_delay = is->frame_timer - (av_gettime() / 1000000.0);
>    if(actual_delay < 0.010) {
>      actual_delay = 0.010;
>    }
>    schedule_refresh(is, (int)(actual_delay * 1000 + 0.5));
>    ```
>
> 8. **显示视频帧**：
>    ```c
>    video_display(is);
>    ```
>
> 9. **更新图片队列**：
>    ```c
>    if(++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {
>      is->pictq_rindex = 0;
>    }
>    SDL_LockMutex(is->pictq_mutex);
>    is->pictq_size--;
>    SDL_CondSignal(is->pictq_cond);
>    SDL_UnlockMutex(is->pictq_mutex);
>    ```
>
> 10. **处理没有视频流的情况**：
>     ```c
>     } else {
>       schedule_refresh(is, 100);
>     }
>     ```
>
> ### 作用
> `video_refresh_timer` 函数的主要作用是定期刷新视频帧，同时确保视频帧与音频的同步。通过计算帧的显示延迟，并根据音频时钟调整帧的显示时间，从而实现音视频同步播放。
>
> ### 使用场景
> `video_refresh_timer` 函数通常在视频播放应用中使用，它通过定时器或事件循环定期调用，以确保视频帧按正确的时间间隔显示，同时与音频保持同步。
>
> ### 示例
> 在实际使用中，`video_refresh_timer` 函数可能通过定时器或事件回调机制调用，例如：
>
> ```c
> static Uint32 sdl_refresh_timer_cb(Uint32 interval, void* opaque) {
>     SDL_Event event;
>     event.type = FF_REFRESH_EVENT;
>     event.user.data1 = opaque;
>     SDL_PushEvent(&event);
>     return 0;
> }
> 
> // 在事件循环中处理自定义刷新事件
> SDL_Event event;
> while (SDL_WaitEvent(&event)) {
>     if (event.type == FF_REFRESH_EVENT) {
>         video_refresh_timer(event.user.data1);
>     } else if (event.type == SDL_QUIT) {
>         break;
>     }
> }
> ```
>
> 通过这种方式，可以实现视频帧的定时刷新和显示，确保视频播放的平滑和同步。

```c
void alloc_picture(void *userdata) {

  int ret = -1;

  VideoState *is = (VideoState *)userdata;
  VideoPicture *vp;

  vp = &is->pictq[is->pictq_windex];
  if(vp->bmp) {

    // we already have one make another, bigger/smaller
    avpicture_free(vp->bmp);
    free(vp->bmp);

    vp->bmp = NULL;
  }

  // Allocate a place to put our YUV image on that screen
  SDL_LockMutex(text_mutex);
  vp->bmp = (AVPicture*)malloc(sizeof(AVPicture));
  ret = avpicture_alloc(vp->bmp, AV_PIX_FMT_YUV420P, is->video_ctx->width, is->video_ctx->height);
  if (ret < 0) {
      fprintf(stderr, "Could not allocate temporary picture: %s\n", av_err2str(ret));
  }

  SDL_UnlockMutex(text_mutex);

  vp->width = is->video_ctx->width;
  vp->height = is->video_ctx->height;
  vp->allocated = 1;

}
```

> `alloc_picture` 函数负责分配视频帧存储空间，并处理与视频帧队列相关的内存管理。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> void alloc_picture(void *userdata);
> ```
>
> ### 参数
> - `void *userdata`: 指向 `VideoState` 结构的指针，包含了视频播放所需的所有信息和状态。
>
> ### 函数流程
>
> 1. **定义局部变量**：
>    ```c
>    int ret = -1;
>    VideoState *is = (VideoState *)userdata;
>    VideoPicture *vp;
>    ```
>
> 2. **获取当前写入队列中的视频帧**：
>    ```c
>    vp = &is->pictq[is->pictq_windex];
>    ```
>
> 3. **检查当前视频帧是否已经分配过内存**：
>    ```c
>    if(vp->bmp) {
>      // we already have one make another, bigger/smaller
>      avpicture_free(vp->bmp);
>      free(vp->bmp);
>      vp->bmp = NULL;
>    }
>    ```
>
> 4. **分配内存**：
>    - **锁定互斥量以避免多线程冲突**：
>      ```c
>      SDL_LockMutex(text_mutex);
>      ```
>    - **分配 `AVPicture` 结构体**：
>      ```c
>      vp->bmp = (AVPicture*)malloc(sizeof(AVPicture));
>      ```
>    - **为 `AVPicture` 结构体分配内存，用于存储 YUV 图像**：
>      ```c
>      ret = avpicture_alloc(vp->bmp, AV_PIX_FMT_YUV420P, is->video_ctx->width, is->video_ctx->height);
>      if (ret < 0) {
>          fprintf(stderr, "Could not allocate temporary picture: %s\n", av_err2str(ret));
>      }
>      ```
>
> 5. **解锁互斥量**：
>    ```c
>    SDL_UnlockMutex(text_mutex);
>    ```
>
> 6. **更新 `VideoPicture` 结构体中的相关字段**：
>    ```c
>    vp->width = is->video_ctx->width;
>    vp->height = is->video_ctx->height;
>    vp->allocated = 1;
>    ```
>
> ### 作用
> `alloc_picture` 函数的主要作用是为 `VideoPicture` 分配内存，以便存储解码后的视频帧。它会根据当前视频的宽度和高度分配合适的内存，并确保多线程环境下的内存安全。
>
> ### 使用场景
> `alloc_picture` 函数通常在视频播放的初始化阶段调用，或者在需要调整视频帧的存储空间时调用，例如视频尺寸发生变化时。
>
> ### 示例
> 在视频解码和播放过程中，当需要为新的视频帧分配内存时，可以调用 `alloc_picture` 函数：
>
> ```c
> // 在初始化视频播放时，调用 alloc_picture 为每个 VideoPicture 分配内存
> for (int i = 0; i < VIDEO_PICTURE_QUEUE_SIZE; i++) {
>     alloc_picture(video_state);
> }
> 
> // 在视频尺寸发生变化时，重新分配内存
> if (new_width != video_state->video_ctx->width || new_height != video_state->video_ctx->height) {
>     alloc_picture(video_state);
> }
> ```
>
> 通过这种方式，可以确保每个 `VideoPicture` 都有足够的内存来存储解码后的视频帧，确保视频播放的顺畅性和稳定性。

```c
int queue_picture(VideoState *is, AVFrame *pFrame, double pts) {

  VideoPicture *vp;

  /* wait until we have space for a new pic */
  SDL_LockMutex(is->pictq_mutex);
  while(is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE &&
	!is->quit) {
    SDL_CondWait(is->pictq_cond, is->pictq_mutex);
  }
  SDL_UnlockMutex(is->pictq_mutex);

  if(is->quit)
    return -1;

  // windex is set to 0 initially
  vp = &is->pictq[is->pictq_windex];

  /* allocate or resize the buffer! */
  if(!vp->bmp ||
     vp->width != is->video_ctx->width ||
     vp->height != is->video_ctx->height) {

    vp->allocated = 0;
    alloc_picture(is);
    if(is->quit) {
      return -1;
    }
  }

  /* We have a place to put our picture on the queue */
  if(vp->bmp) {

    vp->pts = pts;
    
    // Convert the image into YUV format that SDL uses
    sws_scale(is->video_sws_ctx, (uint8_t const * const *)pFrame->data,
	      pFrame->linesize, 0, is->video_ctx->height,
	      vp->bmp->data, vp->bmp->linesize);
    
    /* now we inform our display thread that we have a pic ready */
    if(++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {
      is->pictq_windex = 0;
    }
    SDL_LockMutex(is->pictq_mutex);
    is->pictq_size++;
    SDL_UnlockMutex(is->pictq_mutex);
  }
  return 0;
}
```

> `queue_picture` 函数用于将解码后的视频帧添加到视频帧队列中，以便在合适的时间进行显示。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> int queue_picture(VideoState *is, AVFrame *pFrame, double pts);
> ```
>
> ### 参数
> - `VideoState *is`: 指向 `VideoState` 结构的指针，包含视频播放所需的所有信息和状态。
> - `AVFrame *pFrame`: 解码后的视频帧。
> - `double pts`: 帧的展示时间戳 (Presentation Timestamp)。
>
> ### 返回值
> - `int`: 如果队列操作成功返回 0，出现错误或用户请求退出时返回 -1。
>
> ### 函数流程
>
> 1. **定义局部变量**：
>    ```c
>    VideoPicture *vp;
>    ```
>
> 2. **等待队列有空间**：
>
>    - 锁定互斥量以确保线程安全。
>      ```c
>      SDL_LockMutex(is->pictq_mutex);
>      ```
>    - 如果队列已满且程序未退出，则等待条件变量。
>      ```c
>      while(is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {
>        SDL_CondWait(is->pictq_cond, is->pictq_mutex);
>      }
>      ```
>    - 解锁互斥量。
>      ```c
>      SDL_UnlockMutex(is->pictq_mutex);
>      ```
>   ```
> 
> 3. **检查程序是否请求退出**：
>    ```c
>    if(is->quit)
>      return -1;
>   ```
>
> 4. **获取当前写入队列中的视频帧**：
>
>    ```c
>    vp = &is->pictq[is->pictq_windex];
>    ```
> ```
> 
> 5. **分配或调整缓冲区**：
> 
>    - 如果 `VideoPicture` 没有分配内存或尺寸不匹配，则重新分配内存。
>      ```c
>      if(!vp->bmp || vp->width != is->video_ctx->width || vp->height != is->video_ctx->height) {
>        vp->allocated = 0;
>        alloc_picture(is);
>        if(is->quit) {
>          return -1;
>        }
>      }
> ```
>
> 6. **将解码后的帧转换为 YUV 格式并存储到队列**：
>
>    - 设置帧的展示时间戳。
>      ```c
>      vp->pts = pts;
>      ```
>    - 使用 `sws_scale` 将图像转换为 SDL 使用的 YUV 格式。
>      ```c
>      sws_scale(is->video_sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_ctx->height, vp->bmp->data, vp->bmp->linesize);
>      ```
>   ```
> 
> 7. **更新写入索引并通知显示线程**：
> 
>    - 更新写入索引。
>      ```c
>      if(++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {
>        is->pictq_windex = 0;
>      }
>   ```
>    - 锁定互斥量并更新队列大小，通知条件变量。
>      ```c
>      SDL_LockMutex(is->pictq_mutex);
>      is->pictq_size++;
>      SDL_UnlockMutex(is->pictq_mutex);
>      ```
>   ```
> 
> 8. **返回成功状态**：
>    ```c
>    return 0;
>   ```
>
> ### 作用
> `queue_picture` 函数的主要作用是将解码后的视频帧添加到显示队列中，并确保在多线程环境下操作的线程安全。它负责将帧转换为 SDL 可处理的格式，并在队列有空间时将帧插入队列，供显示线程进行展示。
>
> ### 使用场景
> 该函数通常在视频解码后的流程中调用，用于将解码后的帧加入到待显示的队列中。例如，在解码线程中，当解码出一个新的视频帧时，可以调用 `queue_picture` 将该帧加入显示队列：
>
> ```c
> while (!is->quit) {
>     AVFrame* frame = av_frame_alloc();
>     if (avcodec_receive_frame(is->video_ctx, frame) == 0) {
>         double pts = av_frame_get_best_effort_timestamp(frame);
>         queue_picture(is, frame, pts);
>     }
>     av_frame_free(&frame);
> }
> ```
>
> 通过这种方式，确保每个解码后的视频帧都能按照正确的时间顺序被显示，保证视频播放的连续性和同步性。

```c
double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) {

  double frame_delay;

  if(pts != 0) {
    /* if we have pts, set video clock to it */
    is->video_clock = pts;
  } else {
    /* if we aren't given a pts, set it to the clock */
    pts = is->video_clock;
  }
  /* update the video clock */
  frame_delay = av_q2d(is->video_ctx->time_base);
  /* if we are repeating a frame, adjust clock accordingly */
  frame_delay += src_frame->repeat_pict * (frame_delay * 0.5);
  is->video_clock += frame_delay;
  return pts;
}
```

> `synchronize_video` 函数用于同步视频帧的时间戳 (PTS, Presentation Timestamp) 和视频时钟，确保视频播放的同步性。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> double synchronize_video(VideoState *is, AVFrame *src_frame, double pts);
> ```
>
> ### 参数
> - `VideoState *is`: 指向 `VideoState` 结构的指针，包含视频播放所需的所有信息和状态。
> - `AVFrame *src_frame`: 当前解码的视频帧。
> - `double pts`: 视频帧的展示时间戳。如果没有有效的时间戳，则为 0。
>
> ### 返回值
> - `double`: 更新后的时间戳，用于同步视频时钟。
>
> ### 函数流程
>
> 1. **检查时间戳是否有效**：
>    - 如果提供了有效的 PTS，则将视频时钟设置为该时间戳。
>      ```c
>      if(pts != 0) {
>        is->video_clock = pts;
>      } else {
>          /* 如果没有提供有效的 PTS，则使用视频时钟的当前值 */
>        pts = is->video_clock;
>      }
>      ```
>
> 2. **更新视频时钟**：
>    - 计算当前帧的显示延迟。`frame_delay` 是根据帧率计算的基本时间间隔。
>      ```c
>      frame_delay = av_q2d(is->video_ctx->time_base);
>      ```
>    - 如果帧有重复字段，调整时间延迟。
>      ```c
>      frame_delay += src_frame->repeat_pict * (frame_delay * 0.5);
>      ```
>    - 更新视频时钟，增加帧延迟以反映下一帧的预期显示时间。
>      ```c
>      is->video_clock += frame_delay;
>      ```
>
> 3. **返回同步后的时间戳**：
>    ```c
>    return pts;
>    ```
>
> ### 作用
> `synchronize_video` 函数的主要作用是同步视频帧的时间戳和视频时钟，确保视频帧按照正确的时间顺序显示。该函数在以下情况下使用：
> - 当解码器提供了有效的 PTS 时，更新视频时钟。
> - 当解码器未提供有效的 PTS 时，使用当前视频时钟的值。
> - 根据帧率和重复字段调整视频时钟，以反映正确的显示时间。
>
> ### 使用场景
> 该函数通常在解码线程或视频显示线程中调用，用于确保每个解码后的视频帧都能按照正确的时间顺序被显示，保证视频播放的同步性和连续性。例如，在解码视频帧后，可以调用 `synchronize_video` 函数更新时间戳：
>
> ```c
> double pts = 0;
> if (frame->pts != AV_NOPTS_VALUE) {
>     pts = frame->pts * av_q2d(video_st->time_base);
> }
> pts = synchronize_video(is, frame, pts);
> queue_picture(is, frame, pts);
> ```
>
> 通过这种方式，可以确保每个视频帧都按照正确的时间顺序显示，避免视频播放时出现同步问题。

```c
int decode_video_thread(void *arg) {
  VideoState *is = (VideoState *)arg;
  AVPacket pkt1, *packet = &pkt1;
  int frameFinished;
  AVFrame *pFrame;
  double pts;

  pFrame = av_frame_alloc();

  for(;;) {
    if(packet_queue_get(&is->videoq, packet, 1) < 0) {
      // means we quit getting packets
      break;
    }
    pts = 0;

    // Decode video frame
    avcodec_decode_video2(is->video_ctx, pFrame, &frameFinished, packet);

    if((pts = av_frame_get_best_effort_timestamp(pFrame)) == AV_NOPTS_VALUE) {
      pts = 0;
    }
    pts *= av_q2d(is->video_st->time_base);

    // Did we get a video frame?
    if(frameFinished) {
      pts = synchronize_video(is, pFrame, pts);
      if(queue_picture(is, pFrame, pts) < 0) {
	break;
      }
    }
    av_free_packet(packet);
  }
  av_frame_free(&pFrame);
  return 0;
}
```

> `decode_video_thread` 函数是用于解码视频数据的线程函数。该函数从视频包队列中获取数据包，对其进行解码，并将解码后的帧加入到显示队列中。以下是该函数的详细解释：
>
> ### 函数原型
> ```c
> int decode_video_thread(void *arg);
> ```
>
> ### 参数
> - `void *arg`: 指向 `VideoState` 结构的指针，包含视频播放所需的所有信息和状态。
>
> ### 返回值
> - `int`: 返回 0 表示线程正常退出。
>
> ### 详细流程
>
> 1. **初始化变量**：
>    - `VideoState *is`: 将 `arg` 转换为 `VideoState` 类型。
>    - `AVPacket pkt1, *packet = &pkt1`: 定义并初始化一个 `AVPacket` 变量 `pkt1`，`packet` 指向 `pkt1`。
>    - `int frameFinished`: 标记是否解码出完整帧的标志。
>    - `AVFrame *pFrame`: 用于存储解码后的视频帧。
>    - `double pts`: 存储帧的显示时间戳。
>
>    ```c
>    VideoState *is = (VideoState *)arg;
>    AVPacket pkt1, *packet = &pkt1;
>    int frameFinished;
>    AVFrame *pFrame;
>    double pts;
>
>    pFrame = av_frame_alloc();
>    ```
>
> 2. **主解码循环**：
>    - 不断从视频包队列中获取数据包进行解码，直到接收到退出信号或包队列为空。
>
>    ```c
>    for(;;) {
>      if(packet_queue_get(&is->videoq, packet, 1) < 0) {
>        // means we quit getting packets
>        break;
>      }
>      pts = 0;
>    ```
>
> 3. **解码视频帧**：
>    - 使用 `avcodec_decode_video2` 解码视频包。
>    - 获取解码后的时间戳，如果没有有效的时间戳，则设置为 0。
>    - 将时间戳转换为秒。
>
>    ```c
>    avcodec_decode_video2(is->video_ctx, pFrame, &frameFinished, packet);
>
>    if((pts = av_frame_get_best_effort_timestamp(pFrame)) == AV_NOPTS_VALUE) {
>      pts = 0;
>    }
>    pts *= av_q2d(is->video_st->time_base);
>    ```
>
> 4. **处理解码后的帧**：
>    - 如果解码出完整帧，则同步时间戳，并将帧加入显示队列。
>    - 如果加入显示队列失败，则退出循环。
>
>    ```c
>    if(frameFinished) {
>      pts = synchronize_video(is, pFrame, pts);
>      if(queue_picture(is, pFrame, pts) < 0) {
>        break;
>      }
>    }
>    av_free_packet(packet);
>    ```
>
> 5. **清理资源**：
>    - 解码循环结束后，释放分配的帧资源。
>
>    ```c
>    av_frame_free(&pFrame);
>    return 0;
>    ```
>
> ### 函数用途
> `decode_video_thread` 函数的主要用途是持续从视频包队列中获取数据包并进行解码，将解码后的帧添加到显示队列中。这是一个独立的线程函数，确保视频数据能够被持续解码并准备好进行显示，以保证视频播放的流畅性和同步性。
>
> ### 示例代码
> 以下是函数在视频解码线程中的调用示例：
>
> ```c
> VideoState *is = ...; // 初始化VideoState结构
> SDL_Thread *video_thread = SDL_CreateThread(decode_video_thread, "Video Decode Thread", is);
> if (!video_thread) {
>   fprintf(stderr, "Could not create video decode thread: %s\n", SDL_GetError());
>   // 处理错误
> }
> ```
>
> 通过这种方式，可以创建并启动一个用于解码视频数据的线程，确保视频播放的正常进行。

```c
int stream_component_open(VideoState *is, int stream_index) {

  AVFormatContext *pFormatCtx = is->pFormatCtx;
  AVCodecContext *codecCtx = NULL;
  AVCodec *codec = NULL;
  SDL_AudioSpec wanted_spec, spec;

  if(stream_index < 0 || stream_index >= pFormatCtx->nb_streams) {
    return -1;
  }

  codecCtx = avcodec_alloc_context3(NULL);

  int ret = avcodec_parameters_to_context(codecCtx, pFormatCtx->streams[stream_index]->codecpar);
  if (ret < 0)
    return -1;

  codec = avcodec_find_decoder(codecCtx->codec_id);
  if(!codec) {
    fprintf(stderr, "Unsupported codec!\n");
    return -1;
  }


  if(codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {

    // Set audio settings from codec info
    wanted_spec.freq = codecCtx->sample_rate;
    wanted_spec.format = AUDIO_S16SYS;
    wanted_spec.channels = 2;//codecCtx->channels;
    wanted_spec.silence = 0;
    wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE;
    wanted_spec.callback = audio_callback;
    wanted_spec.userdata = is;
    
    if(SDL_OpenAudio(&wanted_spec, &spec) < 0) {
      fprintf(stderr, "SDL_OpenAudio: %s\n", SDL_GetError());
      return -1;
    }
    is->audio_hw_buf_size = spec.size;
  }
  if(avcodec_open2(codecCtx, codec, NULL) < 0) {
    fprintf(stderr, "Unsupported codec!\n");
    return -1;
  }

  switch(codecCtx->codec_type) {
  case AVMEDIA_TYPE_AUDIO:
    is->audioStream = stream_index;
    is->audio_st = pFormatCtx->streams[stream_index];
    is->audio_ctx = codecCtx;
    is->audio_buf_size = 0;
    is->audio_buf_index = 0;
    memset(&is->audio_pkt, 0, sizeof(is->audio_pkt));
    packet_queue_init(&is->audioq);

    //Out Audio Param
    uint64_t out_channel_layout=AV_CH_LAYOUT_STEREO;

    //AAC:1024  MP3:1152
    int out_nb_samples= is->audio_ctx->frame_size;
    //AVSampleFormat out_sample_fmt = AV_SAMPLE_FMT_S16;

    int out_sample_rate=is->audio_ctx->sample_rate;
    int out_channels=av_get_channel_layout_nb_channels(out_channel_layout);
    //Out Buffer Size
    /*
    int out_buffer_size=av_samples_get_buffer_size(NULL,
                                                   out_channels,
                                                   out_nb_samples,
                                                   AV_SAMPLE_FMT_S16,
                                                   1);
                                                   */

    //uint8_t *out_buffer=(uint8_t *)av_malloc(MAX_AUDIO_FRAME_SIZE*2);
    int64_t in_channel_layout=av_get_default_channel_layout(is->audio_ctx->channels);

    struct SwrContext *audio_convert_ctx;
    audio_convert_ctx = swr_alloc();
    swr_alloc_set_opts(audio_convert_ctx,
                       out_channel_layout,
                       AV_SAMPLE_FMT_S16,
                       out_sample_rate,
                       in_channel_layout,
                       is->audio_ctx->sample_fmt,
                       is->audio_ctx->sample_rate,
                       0,
                       NULL);
    fprintf(stderr, "swr opts: out_channel_layout:%lld, out_sample_fmt:%d, out_sample_rate:%d, in_channel_layout:%lld, in_sample_fmt:%d, in_sample_rate:%d",
            out_channel_layout, AV_SAMPLE_FMT_S16, out_sample_rate, in_channel_layout, is->audio_ctx->sample_fmt, is->audio_ctx->sample_rate);
    swr_init(audio_convert_ctx);

    is->audio_swr_ctx = audio_convert_ctx;

    SDL_PauseAudio(0);
    break;
  case AVMEDIA_TYPE_VIDEO:
    is->videoStream = stream_index;
    is->video_st = pFormatCtx->streams[stream_index];
    is->video_ctx = codecCtx;

    is->frame_timer = (double)av_gettime() / 1000000.0;
    is->frame_last_delay = 40e-3;
    
    packet_queue_init(&is->videoq);
    is->video_sws_ctx = sws_getContext(is->video_ctx->width, is->video_ctx->height,
				 is->video_ctx->pix_fmt, is->video_ctx->width,
				 is->video_ctx->height, AV_PIX_FMT_YUV420P,
				 SWS_BILINEAR, NULL, NULL, NULL
				 );
    is->video_tid = SDL_CreateThread(decode_video_thread, "decode_video_thread", is);
    break;
  default:
    break;
  }
}
```

> `stream_component_open` 函数用于打开并初始化音频或视频流，并为解码和播放做好准备。以下是该函数的详细解释：
>
> ### 函数原型
> ```c
> int stream_component_open(VideoState *is, int stream_index);
> ```
>
> ### 参数
> - `VideoState *is`: 指向包含视频播放相关状态的结构体 `VideoState`。
> - `int stream_index`: 要打开的流的索引。
>
> ### 返回值
> - `int`: 返回 0 表示成功，返回 -1 表示失败。
>
> ### 详细流程
>
> 1. **获取 `AVFormatContext`**：
>    - 从 `VideoState` 结构体中获取格式上下文 `pFormatCtx`。
>
>    ```c
>    AVFormatContext *pFormatCtx = is->pFormatCtx;
>    ```
>
> 2. **检查流索引的有效性**：
>    - 检查 `stream_index` 是否有效，如果无效则返回 -1。
>
>    ```c
>    if (stream_index < 0 || stream_index >= pFormatCtx->nb_streams) {
>      return -1;
>    }
>    ```
>
> 3. **分配和初始化解码器上下文**：
>    - 分配一个新的解码器上下文 `codecCtx`。
>    - 将流参数复制到解码器上下文中。
>
>    ```c
>    AVCodecContext *codecCtx = avcodec_alloc_context3(NULL);
>    int ret = avcodec_parameters_to_context(codecCtx, pFormatCtx->streams[stream_index]->codecpar);
>    if (ret < 0)
>      return -1;
>    ```
>
> 4. **查找解码器**：
>    - 查找流对应的解码器，如果找不到则返回 -1。
>
>    ```c
>    AVCodec *codec = avcodec_find_decoder(codecCtx->codec_id);
>    if (!codec) {
>      fprintf(stderr, "Unsupported codec!\n");
>      return -1;
>    }
>    ```
>
> 5. **初始化音频流**：
>    - 如果流是音频类型，设置音频的 SDL 音频规范并打开音频设备。
>    - 初始化音频缓冲区大小，音频包队列，以及音频重采样上下文。
>
>    ```c
>    if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {
>      wanted_spec.freq = codecCtx->sample_rate;
>      wanted_spec.format = AUDIO_S16SYS;
>      wanted_spec.channels = 2;
>      wanted_spec.silence = 0;
>      wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE;
>      wanted_spec.callback = audio_callback;
>      wanted_spec.userdata = is;
>
>      if (SDL_OpenAudio(&wanted_spec, &spec) < 0) {
>        fprintf(stderr, "SDL_OpenAudio: %s\n", SDL_GetError());
>        return -1;
>      }
>      is->audio_hw_buf_size = spec.size;
>
>      is->audioStream = stream_index;
>      is->audio_st = pFormatCtx->streams[stream_index];
>      is->audio_ctx = codecCtx;
>      is->audio_buf_size = 0;
>      is->audio_buf_index = 0;
>      memset(&is->audio_pkt, 0, sizeof(is->audio_pkt));
>      packet_queue_init(&is->audioq);
>
>      uint64_t out_channel_layout = AV_CH_LAYOUT_STEREO;
>      int out_sample_rate = is->audio_ctx->sample_rate;
>      int64_t in_channel_layout = av_get_default_channel_layout(is->audio_ctx->channels);
>
>      struct SwrContext *audio_convert_ctx = swr_alloc_set_opts(NULL,
>                         out_channel_layout, AV_SAMPLE_FMT_S16, out_sample_rate,
>                         in_channel_layout, is->audio_ctx->sample_fmt,
>                         is->audio_ctx->sample_rate, 0, NULL);
>      swr_init(audio_convert_ctx);
>
>      is->audio_swr_ctx = audio_convert_ctx;
>
>      SDL_PauseAudio(0);
>    }
>    ```
>
> 6. **初始化视频流**：
>    - 如果流是视频类型，初始化视频流相关的变量和上下文，并创建视频解码线程。
>
>    ```c
>    if (codecCtx->codec_type == AVMEDIA_TYPE_VIDEO) {
>      is->videoStream = stream_index;
>      is->video_st = pFormatCtx->streams[stream_index];
>      is->video_ctx = codecCtx;
>
>      is->frame_timer = (double)av_gettime() / 1000000.0;
>      is->frame_last_delay = 40e-3;
>
>      packet_queue_init(&is->videoq);
>      is->video_sws_ctx = sws_getContext(is->video_ctx->width, is->video_ctx->height,
>                 is->video_ctx->pix_fmt, is->video_ctx->width,
>                 is->video_ctx->height, AV_PIX_FMT_YUV420P,
>                 SWS_BILINEAR, NULL, NULL, NULL);
>      is->video_tid = SDL_CreateThread(decode_video_thread, "decode_video_thread", is);
>    }
>    ```
>
> 7. **打开解码器**：
>    - 打开解码器，如果失败则返回 -1。
>
>    ```c
>    if (avcodec_open2(codecCtx, codec, NULL) < 0) {
>      fprintf(stderr, "Unsupported codec!\n");
>      return -1;
>    }
>    ```
>
> ### 示例代码
> 以下是调用 `stream_component_open` 函数的示例代码，用于打开视频和音频流：
>
> ```c
> VideoState *is = ...; // 初始化VideoState结构
> 
> if (stream_component_open(is, is->videoStream) < 0) {
>   fprintf(stderr, "Failed to open video stream\n");
>   // 处理错误
> }
> 
> if (stream_component_open(is, is->audioStream) < 0) {
>   fprintf(stderr, "Failed to open audio stream\n");
>   // 处理错误
> }
> ```
>
> 通过这种方式，可以打开并初始化视频和音频流，为后续的解码和播放做好准备。

```c
int demux_thread(void *arg) {

  Uint32 pixformat;

  VideoState *is = (VideoState *)arg;
  AVFormatContext *pFormatCtx;
  AVPacket pkt1, *packet = &pkt1;

  int video_index = -1;
  int audio_index = -1;
  int i;

  is->videoStream=-1;
  is->audioStream=-1;

  global_video_state = is;

  // Open video file
  if(avformat_open_input(&pFormatCtx, is->filename, NULL, NULL)!=0)
    return -1; // Couldn't open file

  is->pFormatCtx = pFormatCtx;
  
  // Retrieve stream information
  if(avformat_find_stream_info(pFormatCtx, NULL)<0)
    return -1; // Couldn't find stream information
  
  // Dump information about file onto standard error
  av_dump_format(pFormatCtx, 0, is->filename, 0);
  
  // Find the first video stream
  for(i=0; i<pFormatCtx->nb_streams; i++) {
    if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO &&
       video_index < 0) {
      video_index=i;
    }
    if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_AUDIO &&
       audio_index < 0) {
      audio_index=i;
    }
  }

  if(audio_index >= 0) {
    stream_component_open(is, audio_index);
  }
  if(video_index >= 0) {
    stream_component_open(is, video_index);
  }   

  if(is->videoStream < 0 || is->audioStream < 0) {
    fprintf(stderr, "%s: could not open codecs\n", is->filename);
    goto fail;
  }

  win = SDL_CreateWindow("Media Player",
     		   SDL_WINDOWPOS_UNDEFINED,
		   SDL_WINDOWPOS_UNDEFINED,
		   is->video_ctx->width, is->video_ctx->height,
		   SDL_WINDOW_OPENGL | SDL_WINDOW_RESIZABLE);
  
  renderer = SDL_CreateRenderer(win, -1, 0);

  pixformat = SDL_PIXELFORMAT_IYUV;
  texture = SDL_CreateTexture(renderer,
			      pixformat, 
			      SDL_TEXTUREACCESS_STREAMING,
			      is->video_ctx->width,
			      is->video_ctx->height);

  // main decode loop

  for(;;) {

    if(is->quit) {
      SDL_CondSignal(is->videoq.cond);
      SDL_CondSignal(is->audioq.cond);
      break;
    }
    // seek stuff goes here
    if(is->audioq.size > MAX_AUDIOQ_SIZE ||
       is->videoq.size > MAX_VIDEOQ_SIZE) {
      SDL_Delay(10);
      continue;
    }
    if(av_read_frame(is->pFormatCtx, packet) < 0) {
      if(is->pFormatCtx->pb->error == 0) {
	SDL_Delay(100); /* no error; wait for user input */
	continue;
      } else {
	break;
      }
    }
    // Is this a packet from the video stream?
    if(packet->stream_index == is->videoStream) {
      packet_queue_put(&is->videoq, packet);
    } else if(packet->stream_index == is->audioStream) {
      packet_queue_put(&is->audioq, packet);
    } else {
      av_free_packet(packet);
    }
  }
  /* all done - wait for it */
  while(!is->quit) {
    SDL_Delay(100);
  }

 fail:
  if(1){
    SDL_Event event;
    event.type = FF_QUIT_EVENT;
    event.user.data1 = is;
    SDL_PushEvent(&event);
  }
  return 0;
}
```

> `demux_thread` 函数是负责解复用和解码视频文件的主线程。它打开文件，查找音频和视频流，并将数据包发送到相应的解码队列。以下是对该函数的详细解释：
>
> ### 函数原型
> ```c
> int demux_thread(void *arg);
> ```
>
> ### 参数
> - `void *arg`: 一个指向 `VideoState` 结构的指针，包含视频播放相关的状态信息。
>
> ### 返回值
> - `int`: 返回 0 表示成功，返回 -1 表示失败。
>
> ### 详细流程
>
> 1. **初始化变量**：
>    - 定义变量 `pixformat`、`VideoState`、`AVFormatContext` 和 `AVPacket` 等。
>    - 初始化视频和音频流索引 `video_index` 和 `audio_index` 为 -1。
>
>    ```c
>    Uint32 pixformat;
>    VideoState *is = (VideoState *)arg;
>    AVFormatContext *pFormatCtx;
>    AVPacket pkt1, *packet = &pkt1;
>    int video_index = -1;
>    int audio_index = -1;
>    int i;
>    ```
>
> 2. **设置初始流索引和全局视频状态**：
>    - 初始化 `VideoState` 结构体中的视频和音频流索引为 -1。
>    - 设置全局视频状态变量 `global_video_state`。
>
>    ```c
>    is->videoStream = -1;
>    is->audioStream = -1;
>    global_video_state = is;
>    ```
>
> 3. **打开视频文件**：
>    - 尝试打开视频文件，如果失败则返回 -1。
>
>    ```c
>    if (avformat_open_input(&pFormatCtx, is->filename, NULL, NULL) != 0)
>      return -1; // Couldn't open file
>    is->pFormatCtx = pFormatCtx;
>    ```
>
> 4. **获取流信息**：
>    - 获取视频文件的流信息，如果失败则返回 -1。
>
>    ```c
>    if (avformat_find_stream_info(pFormatCtx, NULL) < 0)
>      return -1; // Couldn't find stream information
>    av_dump_format(pFormatCtx, 0, is->filename, 0);
>    ```
>
> 5. **查找音频和视频流**：
>    - 遍历文件中的所有流，找到第一个视频流和音频流的索引。
>
>    ```c
>    for (i = 0; i < pFormatCtx->nb_streams; i++) {
>      if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO && video_index < 0) {
>        video_index = i;
>      }
>      if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO && audio_index < 0) {
>        audio_index = i;
>      }
>    }
>    ```
>
> 6. **打开音频和视频流**：
>    - 使用 `stream_component_open` 函数打开找到的音频和视频流。
>
>    ```c
>    if (audio_index >= 0) {
>      stream_component_open(is, audio_index);
>    }
>    if (video_index >= 0) {
>      stream_component_open(is, video_index);
>    }
>    if (is->videoStream < 0 || is->audioStream < 0) {
>      fprintf(stderr, "%s: could not open codecs\n", is->filename);
>      goto fail;
>    }
>    ```
>
> 7. **创建 SDL 窗口和渲染器**：
>    - 创建一个 SDL 窗口和渲染器，用于显示视频。
>
>    ```c
>    win = SDL_CreateWindow("Media Player",
>              SDL_WINDOWPOS_UNDEFINED,
>              SDL_WINDOWPOS_UNDEFINED,
>              is->video_ctx->width, is->video_ctx->height,
>              SDL_WINDOW_OPENGL | SDL_WINDOW_RESIZABLE);
>    renderer = SDL_CreateRenderer(win, -1, 0);
>    pixformat = SDL_PIXELFORMAT_IYUV;
>    texture = SDL_CreateTexture(renderer,
>                  pixformat,
>                  SDL_TEXTUREACCESS_STREAMING,
>                  is->video_ctx->width,
>                  is->video_ctx->height);
>    ```
>
> 8. **解复用主循环**：
>    - 主循环从文件中读取数据包，并将它们放入相应的队列中。
>    - 如果达到最大队列大小，则等待一段时间。
>
>    ```c
>    for (;;) {
>      if (is->quit) {
>        SDL_CondSignal(is->videoq.cond);
>        SDL_CondSignal(is->audioq.cond);
>        break;
>      }
>      if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) {
>        SDL_Delay(10);
>        continue;
>      }
>      if (av_read_frame(is->pFormatCtx, packet) < 0) {
>        if (is->pFormatCtx->pb->error == 0) {
>          SDL_Delay(100); /* no error; wait for user input */
>          continue;
>        } else {
>          break;
>        }
>      }
>      if (packet->stream_index == is->videoStream) {
>        packet_queue_put(&is->videoq, packet);
>      } else if (packet->stream_index == is->audioStream) {
>        packet_queue_put(&is->audioq, packet);
>      } else {
>        av_free_packet(packet);
>      }
>    }
>    ```
>
> 9. **等待退出**：
>    - 在退出之前等待，确保所有线程都正确退出。
>
>    ```c
>    while (!is->quit) {
>      SDL_Delay(100);
>    }
>    ```
>
> 10. **错误处理和清理**：
>     - 在出错的情况下，发送 `FF_QUIT_EVENT` 事件，确保正确清理资源。
>
>     ```c
>     fail:
>     if (1) {
>       SDL_Event event;
>       event.type = FF_QUIT_EVENT;
>       event.user.data1 = is;
>       SDL_PushEvent(&event);
>     }
>     return 0;
>     ```
>
> ### 总结
> `demux_thread` 函数是媒体播放器的核心，它处理文件的打开、解复用、解码和显示。通过结合 SDL 和 FFmpeg 库，该函数能够从视频文件中提取音频和视频流，并将它们分别送到音频和视频解码器进行处理，最终在屏幕上显示视频，并通过音频设备播放音频。

```c
int main(int argc, char *argv[]) {

  int             ret = -1;

  SDL_Event       event;

  VideoState      *is;

  is = av_mallocz(sizeof(VideoState));

  if(argc < 2) {
    fprintf(stderr, "Usage: test <file>\n");
    exit(1);
  }
  // Register all formats and codecs
  av_register_all();
  
  if(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
    fprintf(stderr, "Could not initialize SDL - %s\n", SDL_GetError());
    exit(1);
  }

  text_mutex = SDL_CreateMutex();

  av_strlcpy(is->filename, argv[1], sizeof(is->filename));

  is->pictq_mutex = SDL_CreateMutex();
  is->pictq_cond = SDL_CreateCond();

  schedule_refresh(is, 40);

  is->parse_tid = SDL_CreateThread(demux_thread, "demux_thread", is);
  if(!is->parse_tid) {
    av_free(is);
    goto __FAIL;
  }
  for(;;) {

    SDL_WaitEvent(&event);
    switch(event.type) {
    case FF_QUIT_EVENT:
    case SDL_QUIT:
      is->quit = 1;
      //SDL_Quit();
      //return 0;
      goto __QUIT;
      break;
    case FF_REFRESH_EVENT:
      video_refresh_timer(event.user.data1);
      break;
    default:
      break;
    }
  }

__QUIT:
  ret = 0;

__FAIL:

  SDL_Quit();
  /*
  if(audiofd){
    fclose(audiofd);
  }
  if(audiofd1){
    fclose(audiofd1);
  }
  */
  return ret;

}
```

> `main` 函数是媒体播放器程序的入口点。它初始化库和框架，设置必要的结构和线程，并进入事件处理循环。以下是对 `main` 函数的详细解释：
>
> ### 函数原型
> ```c
> int main(int argc, char *argv[]);
> ```
>
> ### 参数
> - `int argc`: 命令行参数的个数。
> - `char *argv[]`: 命令行参数的数组。
>
> ### 返回值
> - `int`: 返回 0 表示成功，返回其他值表示失败。
>
> ### 详细流程
>
> 1. **定义变量**：
>    - 定义返回值 `ret`，SDL 事件 `event`，和视频状态结构 `VideoState` 的指针 `is`。
>
>    ```c
>    int ret = -1;
>    SDL_Event event;
>    VideoState *is;
>    ```
>
> 2. **分配 `VideoState` 结构**：
>    - 使用 `av_mallocz` 分配并初始化 `VideoState` 结构。
>
>    ```c
>    is = av_mallocz(sizeof(VideoState));
>    ```
>
> 3. **检查命令行参数**：
>    - 确保至少有一个命令行参数（即文件名）。
>
>    ```c
>    if (argc < 2) {
>      fprintf(stderr, "Usage: test <file>\n");
>      exit(1);
>    }
>    ```
>
> 4. **注册所有格式和编解码器**：
>    - 调用 `av_register_all` 注册所有可用的文件格式和编解码器。
>
>    ```c
>    av_register_all();
>    ```
>
> 5. **初始化 SDL**：
>    - 使用 `SDL_Init` 初始化 SDL 视频、音频和定时器子系统。如果初始化失败，输出错误信息并退出。
>
>    ```c
>    if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
>      fprintf(stderr, "Could not initialize SDL - %s\n", SDL_GetError());
>      exit(1);
>    }
>    ```
>
> 6. **创建互斥锁**：
>    - 创建一个用于文本的互斥锁 `text_mutex`。
>
>    ```c
>    text_mutex = SDL_CreateMutex();
>    ```
>
> 7. **复制文件名**：
>    - 使用 `av_strlcpy` 复制命令行参数中的文件名到 `VideoState` 结构中的 `filename` 字段。
>
>    ```c
>    av_strlcpy(is->filename, argv[1], sizeof(is->filename));
>    ```
>
> 8. **初始化队列互斥锁和条件变量**：
>    - 创建用于帧队列的互斥锁 `pictq_mutex` 和条件变量 `pictq_cond`。
>
>    ```c
>    is->pictq_mutex = SDL_CreateMutex();
>    is->pictq_cond = SDL_CreateCond();
>    ```
>
> 9. **计划首次刷新**：
>    - 调用 `schedule_refresh` 函数计划首次刷新事件。
>
>    ```c
>    schedule_refresh(is, 40);
>    ```
>
> 10. **创建解复用线程**：
>     - 创建解复用线程 `parse_tid` 并传递 `VideoState` 结构指针作为参数。如果线程创建失败，释放 `VideoState` 结构并跳转到错误处理代码。
>
>     ```c
>     is->parse_tid = SDL_CreateThread(demux_thread, "demux_thread", is);
>     if (!is->parse_tid) {
>       av_free(is);
>       goto __FAIL;
>     }
>     ```
>
> 11. **事件处理循环**：
>     - 使用 `SDL_WaitEvent` 等待事件并处理不同类型的事件：
>       - `FF_QUIT_EVENT` 和 `SDL_QUIT`：设置 `quit` 标志，跳转到退出处理代码。
>       - `FF_REFRESH_EVENT`：调用 `video_refresh_timer` 函数处理刷新事件。
>       - 其他事件：忽略。
>
>     ```c
>     for (;;) {
>       SDL_WaitEvent(&event);
>       switch (event.type) {
>       case FF_QUIT_EVENT:
>       case SDL_QUIT:
>         is->quit = 1;
>         goto __QUIT;
>         break;
>       case FF_REFRESH_EVENT:
>         video_refresh_timer(event.user.data1);
>         break;
>       default:
>         break;
>       }
>     }
>     ```
>
> 12. **退出处理**：
>     - 在 `__QUIT` 标签下，设置返回值 `ret` 为 0。
>
>     ```c
>     __QUIT:
>     ret = 0;
>     ```
>
> 13. **错误处理和资源清理**：
>     - 在 `__FAIL` 标签下，调用 `SDL_Quit` 退出 SDL。如果有其他资源需要清理，可以在这里进行（此示例代码中被注释掉的部分）。
>
>     ```c
>     __FAIL:
>     SDL_Quit();
>     return ret;
>     ```
>
> ### 总结
> `main` 函数负责初始化所有必要的组件和框架，创建解复用线程，并进入事件处理循环，响应各种 SDL 事件。通过这些步骤，它启动了媒体播放器的核心功能，包括解码、同步和显示视频，以及播放音频。

![player](/Users/huangqiuzhao/blog/source/_posts/音视频/player.jpg)



------

## PCM

将音频数字化的常见技术方案是脉冲编码调制（PCM，Pulse Code Modulation），主要过程是：采样 量化 编码。

![image-20240928223612862](/Users/huangqiuzhao/blog/source/_posts/音视频/image-20240928223612862.png)

模拟信号的波形是无限光滑的，可以看成由无数个点组成，由于存储空间是相对有限的，数字编码过程中，必须要对波形的点进行采样。采样（Sampling）：每隔一段时间采集一次模拟信号的样本，是一个在时间上将模拟信号离散化（把连续信号转换成离散信号）的过程。

每秒采集的样本数量，称为采样率（采样频率，采样速率，Sampling Rate）。比如，采样率44.1kHz表示1秒钟采集44100个样本。

根据[采样定理](https://zh.wikipedia.org/wiki/采样定理)（奈奎斯特–香农采样定理，Nyquist-Shannon sampling theorem）得知：只有当采样率高于声音信号最高频率的2倍时，才能把采集的声音信号唯一地还原成原来的声音。人耳能够感觉到的最高声音频率为20000Hz，因此为了满足人耳的听觉要求，需要至少每秒进行40000次采样（40kHz采样率）。这就是为什么常见的CD的采样率为44.1kHz。电话、无线对讲机、无线麦克风等的采样率是8kHZ。

量化（Quantization）：将每一个采样点的样本值数字化。

位深度（采样精度，采样大小，Bit Depth）：使用多少个二进制位来存储一个采样点的样本值。位深度越高，表示的振幅越精确。常见的CD采用16bit的位深度，能表示65536（216）个不同的值。DVD使用24bit的位深度，大多数电话设备使用8bit的位深度。

![image-20240928223927838](/Users/huangqiuzhao/blog/source/_posts/音视频/image-20240928223927838.png)

编码：将采样和量化后的数字数据转成二进制码流。

单声道产生一组声波数据，双声道（立体声）产生两组声波数据。

采样率44.1kHZ、位深度16bit的1分钟立体声PCM数据有多大？

- 采样率 * 位深度 * 声道数 * 时间
- *44100 \* 16 \* 2 \* 60 / 8 ≈ 10.34MB*

1分钟10.34MB，这对于大部分用户来说是不能接受的。要想在不改变音频时长的前提下，降低音频数据的大小，只有2种方法：降低采样指标、压缩。降低采样指标是不可取的，会导致音频质量下降，用户体验变差，因此专家们研发了各种压缩方案。

比特率（Bit Rate），指单位时间内传输或处理的比特数量，单位是：比特每秒（bit/s或bps），还有：千比特每秒（Kbit/s或Kbps）、兆比特每秒（Mbit/s或Mbps）、吉比特每秒（Gbit/s或Gbps）、太比特每秒（Tbit/s或Tbps）。

采样率44.1kHZ、位深度16bit的立体声PCM数据的比特率是多少？

- 采样率 * 位深度 * 声道数
- *44100 \* 16 \* 2 = 1411.2Kbps*

通常，采样率、位深度越高，数字化音频的质量就越好。从比特率的计算公式可以看得出来：比特率越高，数字化音频的质量就越好。

信噪比（Signal-to-noise ratio，SNR，S/N，讯噪比），指信号与噪声的比例，用于比较所需信号的强度与背景噪声的强度，以分贝（dB）为单位。

## 音频的编码与解码

PCM数据可以理解为是：未经压缩的原始音频数据，体积比较大，为了更便于存储和传输，一般都会使用某种音频编码对它进行编码压缩，然后再存成某种音频文件格式。

压缩分为无损压缩和有损压缩。

- 无损压缩
  - 解压后可以完全还原出原始数据
  - 压缩比小，体积大
- 有损压缩
  - 解压后不能完全还原出原始数据，会丢失一部分信息
  - 压缩比大，体积小
  - 压缩比越大，丢失的信息就越多，还原后的信号失真就会越大
  - 一般是通过舍弃原始数据中对人类听觉不重要的部分，达成压缩成较小文件的目的
- 压缩比 = 未压缩大小 / 压缩后大小

当需要播放音频时，得先解码（解压缩）出PCM数据，然后再进行播放。

































































